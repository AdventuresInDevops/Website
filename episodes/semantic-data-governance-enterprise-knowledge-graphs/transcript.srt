1
00:00:00,047 --> 00:00:01,029
What's going on, Warren?

2
00:00:01,029 --> 00:00:01,784
How are you?

3
00:00:01,784 --> 00:00:04,366
You know, I was so totally unprepared for that question.

4
00:00:04,366 --> 00:00:10,106
I don't know what I thought you were going to say to start off the episode, but of all the
things you could have picked, it was not that.

5
00:00:10,641 --> 00:00:11,382
Right?

6
00:00:11,382 --> 00:00:14,423
Just what a jerk asking someone how they are.

7
00:00:14,423 --> 00:00:17,284
That's man I can't remember who it was.

8
00:00:17,284 --> 00:00:27,757
We had a um, I can't remember who the guest was but I said you know there's no like stump
the chump or surprise questions and my first question went to her was how she's doing and

9
00:00:27,757 --> 00:00:30,868
she's like I thought you said there would be no trick questions.

10
00:00:31,220 --> 00:00:34,429
That was Adriana, she was just on here.

11
00:00:34,429 --> 00:00:35,562
ah

12
00:00:35,562 --> 00:00:36,285
yeah.

13
00:00:37,931 --> 00:00:50,208
Cool, so today we're gonna be talking about AI ready data and AI governance and to help us
do that conversation, we have Ina Tokarev-Sela from Allumex.

14
00:00:50,208 --> 00:00:51,849
Ina, welcome to the show.

15
00:00:52,272 --> 00:00:53,218
Thank you so much, Will.

16
00:00:53,218 --> 00:00:54,403
Happy to be here.

17
00:00:54,945 --> 00:00:56,325
I'm excited to have you here.

18
00:00:56,325 --> 00:01:08,945
So give us a little bit about your background and what led you to creating, because you're
the CEO and founder of Illumex, so give us a little bit about how you got there.

19
00:01:10,428 --> 00:01:17,914
eh It was a long pathway, but I'm also happy about the way that my career took me.

20
00:01:17,914 --> 00:01:24,398
I started an enterprise, SAP, a huge German uh software company.

21
00:01:24,459 --> 00:01:26,138
And I think this is the...

22
00:01:26,138 --> 00:01:31,060
You know, the most hidden truth about enterprises, you can actually have quite an
adventure in them.

23
00:01:31,060 --> 00:01:32,981
You can build stuff, big stuff, right?

24
00:01:32,981 --> 00:01:34,142
And switch careers within.

25
00:01:34,142 --> 00:01:45,567
So I spent 12 years at SAP, starting as an architect and then basically evolving into a
customer facing role as a partner manager and then uh a head of P &L for video analytics

26
00:01:45,567 --> 00:01:46,177
units.

27
00:01:46,177 --> 00:01:52,500
So quite a journey uh and quite a privilege to work with the world's biggest companies.

28
00:01:52,500 --> 00:01:56,015
about the Walmart, Pacific Builder, Boeing, and so on.

29
00:01:56,015 --> 00:01:56,955
first.

30
00:01:56,955 --> 00:02:03,724
So really, watching companies onboarding to the cloud journey and the machine learning and
then, you know.

31
00:02:03,724 --> 00:02:06,045
Neural and energetic as well.

32
00:02:06,705 --> 00:02:16,860
And then uh I continued my career to Sisense as business intelligence vendor and then
understood what an underserved segment business users actually are.

33
00:02:16,860 --> 00:02:24,584
You know, after building all these analytics for all these years, you know, you're
speaking to the actual users and some of them are CEOs of the companies and they still

34
00:02:24,584 --> 00:02:28,286
cannot get their hands on the actual self-service analytics.

35
00:02:28,286 --> 00:02:34,028
So this has really moved me to build a company which creates a

36
00:02:34,139 --> 00:02:46,355
where data could be recognized and uh meaningful semantically and business-wise to
business users to enable them to self-service analytics and data access.

37
00:02:47,631 --> 00:02:48,771
Right on.

38
00:02:48,771 --> 00:03:03,051
So whenever you're thinking about self-service data, one of the challenges I've had in the
past is self-service sometimes means guiding yourself to the wrong answer.

39
00:03:03,051 --> 00:03:06,991
And like one specific example I have is I was working for a company.

40
00:03:06,991 --> 00:03:11,111
was like, how many users do we have?

41
00:03:11,399 --> 00:03:16,961
using our app each month is like, that seems like a pretty straightforward question,
right?

42
00:03:16,961 --> 00:03:30,084
But turns out it wasn't because then like, when I meant monthly active users, actually
meant people who weren't on the trial and had converted to paid, but they, you know, they

43
00:03:30,084 --> 00:03:33,915
were like using it at least three times a week, not just once a month.

44
00:03:33,915 --> 00:03:39,937
And so like a relatively simple question turned out to be

45
00:03:41,569 --> 00:03:43,126
coming with lot of constraints.

46
00:03:43,126 --> 00:03:45,384
So how do you deal with that in a self-service world?

47
00:03:45,934 --> 00:03:55,672
Yeah, it's perfect question and it goes back to I guess the title of this conversation the
IRD data and governance so for starters data has to be

48
00:03:55,972 --> 00:04:00,584
has to be in a state that you can actually create analytics on scale for many users.

49
00:04:00,584 --> 00:04:12,173
So when we speak about highly curated dashboards in the intelligence, it's no brainer
because you can massage specific data sets which go to specific reports and make sure that

50
00:04:12,173 --> 00:04:14,305
they actually have decent quality.

51
00:04:14,305 --> 00:04:20,180
For companies to have self-service and scale, they need to make sure that any potential
questions.

52
00:04:20,180 --> 00:04:24,301
could be answered with high-quality data.

53
00:04:24,522 --> 00:04:25,662
So for that, you have to

54
00:04:25,662 --> 00:04:31,242
have to a health score of the data, have to keep your monitoring and guide trails.

55
00:04:31,502 --> 00:04:41,862
I will use slightly technical terms like metadata activation, like really understanding
what's the expectation and what actually happens in the data and keeping an that

56
00:04:41,862 --> 00:04:46,842
automatically, of course, with alerts and benchmarks and all of that.

57
00:04:47,202 --> 00:04:52,732
So basically, added data is a big component of how you enable healthy self-service on
scale.

58
00:04:52,732 --> 00:04:58,972
You first need to get your data in order to be able to provide this kind of service.

59
00:04:58,972 --> 00:05:03,492
And the second one is, as you rightly mentioned, single source of truth, right?

60
00:05:03,532 --> 00:05:12,232
So definition what active user is, so how many users do we have, could be defined in
dozens of different ways in the organization.

61
00:05:12,652 --> 00:05:20,732
And even if you speak with a human analyst, if you speak with someone from product
department, they will go for the active user to actually use the product.

62
00:05:20,732 --> 00:05:22,686
And maybe if you speak to your financial department,

63
00:05:22,686 --> 00:05:28,870
and they will go to someone who actually signed a contract with you, so there are
different ways of calculating things.

64
00:05:28,930 --> 00:05:35,735
And especially for self-service, again, you're asking about things which might have
different meaning, you have to have governance.

65
00:05:35,735 --> 00:05:41,919
And this governance is not your old governance that we used to use for compliance, right?

66
00:05:41,919 --> 00:05:45,661
Those manual glossaries and dictionaries don't have you.

67
00:05:45,661 --> 00:05:53,006
It's actually understanding semantic and business definitions of business metrics and
business terms.

68
00:05:53,397 --> 00:05:58,837
like terminology, controls, analysis metrics, all of them mapped to the data assets.

69
00:05:58,837 --> 00:06:04,297
Now, of course, you have to have it automatically again because you have to have multiple
coverage, right?

70
00:06:04,297 --> 00:06:05,139
It's that you can.

71
00:06:05,139 --> 00:06:07,611
curate for every specific mission.

72
00:06:07,611 --> 00:06:09,152
And it's all about transparency.

73
00:06:09,152 --> 00:06:16,758
So when you ask about how many active users do we have, if you do have different versions
of truth in your organization, you should have this answer, you know what?

74
00:06:16,758 --> 00:06:22,912
We have this definition for, you know, based on financial records and this definition
based on the product usage.

75
00:06:23,002 --> 00:06:25,983
what definition would you like to use for your answer.

76
00:06:26,063 --> 00:06:37,137
So this may be for more advanced users like you and for really business users you might
even want to limit them to the specific department information, auto-specific dashboard

77
00:06:37,137 --> 00:06:38,558
which is already pre-built for them.

78
00:06:38,558 --> 00:06:45,150
So there are ways around that to really guide role but the first thing is governance.

79
00:06:45,350 --> 00:06:50,552
Being able to map business definitions and get transparency about any conflicts around
them.

80
00:06:50,620 --> 00:06:54,842
I feel like that's a bit of a moving target from what I've seen in my experience.

81
00:06:54,842 --> 00:06:58,073
I was never in an organization where I'm like, we figured it out.

82
00:06:58,073 --> 00:07:01,635
We managed to get a single source of truth for even what a user is.

83
00:07:01,635 --> 00:07:07,807
There's a lot of identity aspects for a user, but then there's also a lot of business
aspects to it.

84
00:07:07,807 --> 00:07:10,589
And how great of a customer are they?

85
00:07:10,589 --> 00:07:15,511
Repeat customer, how much they've spent, and then individual events related to it.

86
00:07:15,511 --> 00:07:18,869
And trying to get a single identity for that was always

87
00:07:18,869 --> 00:07:19,489
potentially a problem.

88
00:07:19,489 --> 00:07:24,269
And then every team in your organization has their own idea of what a user is.

89
00:07:24,829 --> 00:07:27,674
And each organization has their own user management

90
00:07:27,674 --> 00:07:38,637
with his own special data and I just I wonder how possible like if it like if there is a
company out there that actually has good good data or you know high marks on uh health

91
00:07:38,637 --> 00:07:47,600
scorecard for their data management like what does that look like you know is that
actually do you actually see that in practice or you know as most companies just it's this

92
00:07:47,600 --> 00:07:48,460
uh

93
00:07:48,771 --> 00:07:54,123
utopia that we reach to in theory but never actually are able to actually achieve it.

94
00:07:54,757 --> 00:08:04,197
So I would say again because we're operating in space so naturally it's possible otherwise
we wouldn't have any customers, right?

95
00:08:04,582 --> 00:08:16,700
On the other hand this is truly challenge for many organizations and I would also say that
agentic practice, so generative AI creates another silo.

96
00:08:16,700 --> 00:08:17,200
Right.

97
00:08:17,200 --> 00:08:27,120
So because usually data management departments that have their own single source of truth
may be implemented in the data pipelines, aliases, so calculated fields and so on and so

98
00:08:27,120 --> 00:08:27,500
forth.

99
00:08:27,500 --> 00:08:35,120
And then analytics departments have their own single source of truth, which is probably in
the BI tool feature store, metric store, what have you.

100
00:08:35,120 --> 00:08:35,480
Right.

101
00:08:35,480 --> 00:08:46,140
And then right now we have a genetic practice where data science departments are just
building rug or graph rug, all those black boxes tools and the feed out or feed in

102
00:08:46,140 --> 00:08:46,894
actually

103
00:08:46,894 --> 00:08:50,139
different set of definitions to the system, right?

104
00:08:50,139 --> 00:08:56,547
And it's not transparent, it's not governed, it's not connected to any compliance at all.

105
00:08:56,547 --> 00:09:04,035
And being Blakebox, you don't even know if those definitions which are fed into this
agentic practice.

106
00:09:04,476 --> 00:09:08,736
are actually adjusted by the models.

107
00:09:08,736 --> 00:09:15,096
So I mean by that, that model is trained on millions or billions of data points from
Wikipedia and social.

108
00:09:15,096 --> 00:09:23,496
And if you just feed 10 examples from your company, the model would not say, you know
what, I 20 % customize, like 50 % customize.

109
00:09:23,496 --> 00:09:25,356
It will be always a black box.

110
00:09:25,356 --> 00:09:27,176
Like it's kind of like a child, right?

111
00:09:27,176 --> 00:09:34,196
You bring more examples and then you need to ask questions on the center of your child
actually learned based on those examples.

112
00:09:34,443 --> 00:09:47,073
agentic right so so my point is you just mentioned that companies have like many versions
of truth and my point is that the gente creates another version of truth unless unless you

113
00:09:47,073 --> 00:10:00,403
create this a kind of understanding of what the organizational context and reasoning like
organizational business definitions are and they feed them into all practices together

114
00:10:00,403 --> 00:10:04,356
like simultaneously new data management analytics and agentic and that's why

115
00:10:04,356 --> 00:10:19,635
saying that governance is a cornerstone for any future AI adoption because so far it was
kind of a byproduct or a kind of insurance and to me it has to become the center otherwise

116
00:10:19,635 --> 00:10:25,556
we'll have this unhealthy practice in all data related activities.

117
00:10:25,556 --> 00:10:34,020
I mean, that's an interesting point because we already see that with the public agents
that are out there, they're trained up to, you know, if we're lucky, even six months ago,

118
00:10:34,101 --> 00:10:37,563
and especially things in a business context are rolling very quickly.

119
00:10:37,563 --> 00:10:43,286
What the principles of the organization are, or, you know, what features need to be
implemented for customers.

120
00:10:43,286 --> 00:10:45,507
Those things are iterating very quickly.

121
00:10:45,507 --> 00:10:51,430
And so, you know, I'm, I know this is a new problem that I actually hadn't considered
before.

122
00:10:51,430 --> 00:10:54,852
And it means your models are fundamentally always out of date.

123
00:10:55,166 --> 00:10:57,848
Do the, just like your documentation and our source code, right?

124
00:10:57,848 --> 00:10:59,999
It's all legacy as soon as it's made.

125
00:11:00,759 --> 00:11:11,495
Does RAG, Resource Augmented Generation help here to reduce the changing nature of those
things because you can point to potentially the production data store or is there

126
00:11:11,495 --> 00:11:22,362
something else going on where that realistically you're copying that data as RAG being
used against uh stale data sources anyway, like you're not using the production database.

127
00:11:23,332 --> 00:11:34,353
It's a fair point if a rug is a separate silo in your system and you just keep fitting it
with examples which already outdated when they're created.

128
00:11:34,353 --> 00:11:37,946
So no, rug is not uh actually solving the problem.

129
00:11:37,946 --> 00:11:42,050
But our approach is actually a little bit different.

130
00:11:42,050 --> 00:11:48,536
It's think about the organizational business logic as a business ontology, like a
knowledge graph.

131
00:11:48,669 --> 00:11:53,689
of your terminology, your business processes, your workflows, your analytics.

132
00:11:53,969 --> 00:12:02,229
So this knowledge graph is, of course, it could be divided by product lines, by
geographies for big companies and so on so forth.

133
00:12:02,229 --> 00:12:06,289
But it's still the ground truth of your organizational business.

134
00:12:06,289 --> 00:12:10,285
It changes a bit, like new SKUs are going on and so on so forth, but...

135
00:12:10,285 --> 00:12:14,625
and so on and so forth, but majority of that is stable over time.

136
00:12:14,625 --> 00:12:17,885
Like 80 % is not changing month over month, right?

137
00:12:17,945 --> 00:12:19,335
So that's about it.

138
00:12:19,335 --> 00:12:20,566
this knowledge...

139
00:12:20,858 --> 00:12:27,981
You have to basically create the semantic fabric for the starters, like create this
ontology, create this knowledge graph to start with.

140
00:12:27,981 --> 00:12:37,916
And then when organization changes, data changes, you add new data sources, you new API
synthetic data, it will update as ongoing living organism.

141
00:12:37,916 --> 00:12:45,039
And then when you do have this business ontology, this knowledge graph, you can also
represent it as semantic embeddings.

142
00:12:45,039 --> 00:12:50,782
Basically in the way that is machine readable, that LLM readable, which means

143
00:12:50,782 --> 00:12:55,082
for your agentic practice you can have the OM.

144
00:12:55,374 --> 00:12:59,056
You day-to-day representation of the changes of your organizational data.

145
00:12:59,056 --> 00:13:02,928
You remove table, you understand that specific logic does not exist anymore.

146
00:13:02,928 --> 00:13:04,528
cannot ask questions about that.

147
00:13:04,528 --> 00:13:13,413
You add new API about weather conditions and suddenly you can ask questions about the
correlation between asthma attacks and weather, right?

148
00:13:13,413 --> 00:13:15,834
So it's like ongoing and living mechanism.

149
00:13:15,834 --> 00:13:21,856
If you have static rug site, you know, it's a site, current data science, will never live
up to that.

150
00:13:21,856 --> 00:13:25,778
But if you combine metadata management, your business and health

151
00:13:26,217 --> 00:13:28,518
and use it for your agentic practice.

152
00:13:28,518 --> 00:13:32,478
This is exactly what can be up to date at every point of time.

153
00:13:32,683 --> 00:13:37,116
So the level of overhead to pull that off seems pretty significant.

154
00:13:37,116 --> 00:13:41,670
What's um the tipping point?

155
00:13:41,670 --> 00:13:55,740
Because everyone feels these pains, but at what point do you hit the scale or the quantity
of data where it actually seems like a worthwhile exercise to implement this?

156
00:13:57,807 --> 00:14:02,569
I guess it's a healthy practice for any size of organization.

157
00:14:04,130 --> 00:14:12,714
Unless, you know, you just want to rely on one developer who maintains your snowflake or
have you.

158
00:14:12,934 --> 00:14:15,095
Which is fine, okay.

159
00:14:15,095 --> 00:14:22,519
But I think like organizational knowledge and its documentation was always a challenge for
any size of organization.

160
00:14:22,699 --> 00:14:27,581
to me, it's a healthy habit to actually have it from starters, to have this.

161
00:14:29,042 --> 00:14:37,128
you know, knowledge graphs about your different data structures and different data sources
created from day one when you actually have data stores.

162
00:14:37,128 --> 00:14:47,535
But other organizations which just have one database or one warehouse, small one, would
not necessarily invest in that, despite the fact that I think it's not a good practice

163
00:14:47,535 --> 00:14:57,481
because right now we'll see flourish of agentic workflows where different agents going to
communicate with each other and they have to have shared context and reasoning.

164
00:14:57,481 --> 00:15:03,587
And the shared context and reasoning is exactly this knowledge which you should document
about your organization.

165
00:15:04,148 --> 00:15:11,796
So we become more more automated and this is also differentiation, business
differentiation between companies.

166
00:15:11,796 --> 00:15:18,922
Like if you actually advanced in keeping your knowledge and building agents around that or
you're not.

167
00:15:23,358 --> 00:15:24,996
Right on.

168
00:15:25,924 --> 00:15:32,719
Are there um out of the box agents to help with this or is it completely built custom for
everyone?

169
00:15:34,580 --> 00:15:45,145
I don't think it's feasible to really make the deployment manual for any size of
organization because data is exploding even for smaller companies.

170
00:15:45,145 --> 00:15:56,800
We got this approach of actually pre-defining ontologies for different particles and
different lines of business and then automated way to sample metadata from different data

171
00:15:56,800 --> 00:16:02,812
sources to understand what the specific logic changes for different systems.

172
00:16:03,713 --> 00:16:06,194
automatically creates this business ontology.

173
00:16:06,194 --> 00:16:14,179
But I must say to your previous points, what we discovered in this automated onboarding,
which is like few hours to few days, is that

174
00:16:14,655 --> 00:16:16,835
There are many conflicts of definitions.

175
00:16:16,835 --> 00:16:26,315
You might not be aware about that now, but you have 10 different views in your Power BI
tool where you have the same metric defined in different ways based on different data

176
00:16:26,315 --> 00:16:26,715
sources.

177
00:16:26,715 --> 00:16:28,355
And this is what we discover.

178
00:16:29,255 --> 00:16:38,215
So a part of onboarding after this automation where your knowledge graph is created is for
you to actually see, this definition is used by 80 % of our applications and users.

179
00:16:38,215 --> 00:16:39,255
Is it correct definition?

180
00:16:39,255 --> 00:16:41,395
So I might want to certify that, right?

181
00:16:41,395 --> 00:16:44,609
So just addressing the conflicts, addressing the sensitive

182
00:16:44,609 --> 00:16:46,451
part of the soundboarding.

183
00:16:46,451 --> 00:16:56,159
This is where we see, to me it's actually a good thing because this is something that
should be to me performed by domain experts and not necessarily technical users and being

184
00:16:56,159 --> 00:17:06,469
able to capture all those conflicting definitions and ask someone from the relevant
department what it should be, it's a benefit and not necessarily rely on technical teams

185
00:17:06,469 --> 00:17:08,830
who created the definitions in the first place.

186
00:17:12,152 --> 00:17:13,114
Yeah, that makes sense.

187
00:17:13,114 --> 00:17:18,983
m I can see it going horribly wrong asking your technical people about the quality of the
data.

188
00:17:22,176 --> 00:17:35,169
We are all technical people, but I guess if you want to have a business m logic, let's say
all those agents which automate customer support and so and so forth, it will be automated

189
00:17:35,169 --> 00:17:36,480
on top of your data.

190
00:17:36,480 --> 00:17:43,342
They should be aligned to some organizational knowledge which is not necessarily on
technical side, it's on operational side.

191
00:17:43,342 --> 00:17:48,403
Besides, it's coming from those departments, not from us technical people.

192
00:17:48,908 --> 00:17:59,657
Maybe to make this a little bit more concrete, do you have like some canonical examples of
what businesses are trying to often answer with the storage of their data?

193
00:18:02,368 --> 00:18:03,930
what business strategy is.

194
00:18:03,930 --> 00:18:07,794
would say again there is no business case.

195
00:18:09,099 --> 00:18:20,244
as uh solid for automation as agentic, as all those data compilers and agentic and
automation workflows, because if you do not aspire to automation, why would you sort out

196
00:18:20,244 --> 00:18:21,324
your data?

197
00:18:21,465 --> 00:18:24,026
Or why would you sort out your governance, right?

198
00:18:24,026 --> 00:18:26,927
If you don't have automation, like this is a killer use case.

199
00:18:26,927 --> 00:18:34,751
But when you have this killer use case, usually companies tend to start with more like a
knowledge center and discovery.

200
00:18:35,151 --> 00:18:37,652
basically search, right?

201
00:18:37,652 --> 00:18:38,973
oh All these customers.

202
00:18:38,973 --> 00:18:40,134
support functions.

203
00:18:40,134 --> 00:18:50,962
uh On one side, on the other side, we also have companies like Pharma, for example,
building their digital health platforms with agents.

204
00:18:50,962 --> 00:18:57,988
We have a customers and financial services industry which implemented self-service
quotations for third-party brokers.

205
00:18:57,988 --> 00:19:01,790
So basically, the use case would be always shortening the time.

206
00:19:01,790 --> 00:19:02,715
uh

207
00:19:02,715 --> 00:19:10,642
and increasing conversion rates, so there are always business-oriented metrics for
implementing those use cases in the first place.

208
00:19:10,823 --> 00:19:15,686
But internal use cases first, and then external customer facing first.

209
00:19:19,625 --> 00:19:30,860
I'm just thinking back to all the times that I was working at companies and they were
saying how valuable their data would be and collected everything ah that just immediately

210
00:19:30,860 --> 00:19:45,531
was valueless, like just waste and storage and building up internal tables of just garbage
from years and years back and realistically I'm still waiting for the time where we could.

211
00:19:45,531 --> 00:19:48,623
tools to even evaluate that effectively because

212
00:19:49,390 --> 00:19:59,852
even looking back now it's like which test failed 10 years ago in some ERP system for
validation for Q.

213
00:19:59,852 --> 00:20:01,272
assurance, for instance.

214
00:20:01,292 --> 00:20:09,352
And it just, I still don't see those being valuable mechanism, but we still see tons of
companies amassing huge data stores.

215
00:20:09,352 --> 00:20:12,412
Is this, is there something to this?

216
00:20:12,412 --> 00:20:15,703
Is there a scenario where it's like, oh no, actually this

217
00:20:15,703 --> 00:20:27,178
probably highly useless data does turn around and solve a critical need for the company
today with the advent of agents that can potentially utilize it in a much more effective

218
00:20:27,178 --> 00:20:31,733
way than humans can or shortly down the road, five, 10 years.

219
00:20:32,447 --> 00:20:38,887
Yeah, yeah, I believe so because most of organizational data is unutilized as I mentioned.

220
00:20:38,887 --> 00:20:51,387
So I see that even most advanced companies use mainly 20 to 30 % of their data on, you
know, relatively frequent cadence and the rest is, is unutilized basically.

221
00:20:51,387 --> 00:20:59,627
And for that data AI readiness also means that you actually understand few of the health
core components.

222
00:20:59,627 --> 00:21:01,807
So starters, if it's duplicated.

223
00:21:02,297 --> 00:21:07,967
if it's used and if it's used by which applications and if it's sensitive.

224
00:21:07,967 --> 00:21:11,331
So we understand also the risk factors as well.

225
00:21:11,331 --> 00:21:18,454
And then last but not least, I think it's even the most important one, is what is the
semantic meaning of that?

226
00:21:18,495 --> 00:21:22,056
What's actually hidden in this data?

227
00:21:22,056 --> 00:21:28,169
And then this is why knowledge graphs become handy because knowledge graphs create those
suggested connections.

228
00:21:28,169 --> 00:21:32,124
Say, oh, did you know that additional feature of your

229
00:21:32,124 --> 00:21:37,558
version score might come from this customer demographics parameter, like such and such.

230
00:21:37,558 --> 00:21:47,406
So it's kind of uh giving you related uh data which you might not encounter so far by your
experience, but it's in there.

231
00:21:47,406 --> 00:21:50,388
The thing is, in there right now is not covered.

232
00:21:50,388 --> 00:22:01,096
So companies usually do not index or catalog data which is not used actively for
applications, again, because cataloging was used for compliance and insurance, so to say,

233
00:22:01,096 --> 00:22:02,076
for governance.

234
00:22:02,076 --> 00:22:08,516
for governance and now we should use cataloging indexing to actually for discovery.

235
00:22:08,516 --> 00:22:12,836
Discovery, semantic mapping, risk mapping and risk management.

236
00:22:13,036 --> 00:22:20,679
of course, it's also by definition agents are better than humans to understand those
relations on scale.

237
00:22:20,679 --> 00:22:27,974
also like on scale and then we see humans as moderators.

238
00:22:27,974 --> 00:22:31,817
So agent might suggest to you that there is specific

239
00:22:31,937 --> 00:22:35,800
the Aletio correlation relationship between some data assets.

240
00:22:35,800 --> 00:22:41,966
And then from your experience, from your business experience even, he would say, you know
what, let's try that.

241
00:22:41,966 --> 00:22:44,648
Let's use it for forecast, what have you.

242
00:22:45,389 --> 00:22:52,494
And at this point, it actually brings data or agents closer to business.

243
00:22:52,605 --> 00:22:58,984
Because so far those decisions were always made on data science department, on data
management department.

244
00:22:58,984 --> 00:23:06,273
Almost never we saw business involved to some extent, you know, to be to be enabled to
understand like it's a data garbage or not.

245
00:23:06,354 --> 00:23:09,958
They're simply not called to for those decisions.

246
00:23:09,981 --> 00:23:22,210
And now we see more transparency, more communication and collaboration between all those
stakeholders because of the agents, because agentic part of recognizing, reconciling,

247
00:23:22,210 --> 00:23:25,862
semantic labeling, cataloging, indexing.

248
00:23:26,722 --> 00:23:30,393
So you mentioned like 20 to 30 % of the data is being utilized.

249
00:23:30,393 --> 00:23:33,835
So 70 % is not utilized.

250
00:23:33,835 --> 00:23:45,119
Is that because of the lack of value in it, uh lack of it being categorized effectively,
or is there another bucket that I'm missing here?

251
00:23:45,119 --> 00:23:48,962
There could be something interesting in it, but no one's taking the time to actually
understand it.

252
00:23:48,962 --> 00:23:55,463
Because I get the sense that the agents aren't going to be able to just see this
uncategorized data and

253
00:23:55,488 --> 00:23:58,288
magically pop up an answer of how it could be valuable.

254
00:23:58,288 --> 00:24:04,408
still requires a human to evaluate it and know that there is something valuable in there.

255
00:24:04,408 --> 00:24:10,841
What or what the correlations are may not be obvious, how it could be utilized still has
to be done by a human.

256
00:24:10,841 --> 00:24:11,837
that accurate?

257
00:24:12,497 --> 00:24:13,887
Yeah, it's a good point.

258
00:24:13,887 --> 00:24:26,611
in the databases which only have some analytics on them and so on and forth, the role of
the agent could be suggesting additional features to look at, to take into consideration.

259
00:24:26,611 --> 00:24:35,764
In databases where you do not have analytics at all, so for example, we have this
discussion with the company which never introduced, and this is huge company, which never

260
00:24:35,764 --> 00:24:39,395
introduced analytics for people's departments.

261
00:24:39,903 --> 00:24:41,683
There are no dashboards for people with departments.

262
00:24:41,683 --> 00:24:50,283
Right now they want to skip the stage of BI tools and go to self-service data co-pilots to
actually create this analytics to kind of skip the stage.

263
00:24:50,283 --> 00:24:50,663
Right?

264
00:24:50,663 --> 00:24:53,243
So in this case, the data is super valuable.

265
00:24:53,243 --> 00:25:00,223
Of course it does have to, you know, to go through automated labeling and reconciliation
and semantic definitions and all of that.

266
00:25:00,223 --> 00:25:03,143
you know, thankfully we have tools for that now.

267
00:25:03,143 --> 00:25:10,037
But here is the case is you had unutilized data, not for the right reasons, because the
priority was to

268
00:25:10,037 --> 00:25:13,350
to basically provide analytics to money generating departments.

269
00:25:13,350 --> 00:25:20,685
Also, human resources is very, it's very analytics prone to me.

270
00:25:20,685 --> 00:25:28,610
But here you go, have underserved departments who didn't have analytics so far and the
jumpings try to co-pilot.

271
00:25:31,352 --> 00:25:41,375
So do you find that after going through this process that companies actually have less
data storage concerns?

272
00:25:41,375 --> 00:25:51,168
we talk about a single source of truth, and a lot of times I've seen where everyone claims
theirs is the single source of truth, so they want their own copy, their own database

273
00:25:51,168 --> 00:25:55,279
servers, their own storage system, because they don't want anyone else polluting it.

274
00:25:55,279 --> 00:25:57,604
So after you go through this exercise,

275
00:25:57,604 --> 00:26:03,552
Do you find that a lot of those can be decommissioned and you actually end up with less
data storage overall?

276
00:26:04,511 --> 00:26:08,152
It's an interesting question because single source of truth has to be virtual.

277
00:26:08,231 --> 00:26:17,215
So it's kind of virtual layer which connects to your operational data source, analytics
data sources, and even applications because there's lots of business logic in application

278
00:26:17,215 --> 00:26:18,056
side.

279
00:26:18,056 --> 00:26:19,486
So it's always virtualized.

280
00:26:19,486 --> 00:26:26,879
And then the question is, do you even need aggregating layers like warehouses?

281
00:26:27,783 --> 00:26:35,286
We see this question popping up more and more and we say, okay, so there are probably
going to be stages, right?

282
00:26:35,307 --> 00:26:40,729
So some companies are going to reduce companies like in IoT or manufacturing space.

283
00:26:40,729 --> 00:26:53,575
They might want to reduce the size of the data to some warehouse to basically have more
focused use cases, More focused and scoped use cases cheaper for processing, right?

284
00:26:53,575 --> 00:26:57,697
And some companies who might have less data will just

285
00:26:57,697 --> 00:27:00,697
you know, go without any aggregation at all.

286
00:27:00,757 --> 00:27:05,297
And when I'm saying less data, it's because the storage is not expensive anymore.

287
00:27:05,341 --> 00:27:16,276
But the processing, when someone asks questions about three different data stores at once,
like if it one single prompt and which of the data stores have like one million of rows,

288
00:27:16,276 --> 00:27:17,556
it would be expensive.

289
00:27:17,556 --> 00:27:27,300
So just saying about that, that you might want to scope or pre-process some of the answers
for those use cases which are massive on one side.

290
00:27:27,300 --> 00:27:33,223
On the other side, if you don't have massive landscape, you might not want to aggregate
your data anymore in the future.

291
00:27:33,762 --> 00:27:40,916
I mean, I feel like there's a whole systems thinking problem here, which is just because
it would be better to have a single source of truth doesn't automatically make the

292
00:27:40,916 --> 00:27:43,117
organizations migrate to that.

293
00:27:43,117 --> 00:27:47,816
I do see the XKCD article on the number of sources.

294
00:27:47,816 --> 00:27:49,350
I mean, it says standards, right?

295
00:27:49,350 --> 00:27:54,383
We have three databases with user identity, user tracking metrics data in it.

296
00:27:54,383 --> 00:28:01,887
And we should have one unified answer, one perfect database that is sanitized, that is
categorized.

297
00:28:02,046 --> 00:28:02,676
correctly.

298
00:28:02,676 --> 00:28:08,168
And the result is now we have four user data.

299
00:28:08,168 --> 00:28:09,650
And, you know, someone's saying.

300
00:28:09,650 --> 00:28:19,258
those old ones and to your point of storage is still getting cheaper for us There is no
justifiable and it takes effort, you know human time and resources to actually

301
00:28:19,258 --> 00:28:28,756
decommission a database I can see that just not being encouraged to even happen What if
there's some something we missed in there that's still valuable that we could be utilizing

302
00:28:28,756 --> 00:28:30,475
to increase our

303
00:28:30,475 --> 00:28:33,426
business even by a couple of percentage points.

304
00:28:34,109 --> 00:28:43,636
So I guess this virtualized ontology, virtualized knowledge graph, which can connect to
many data sources and indicate which data source and which table or column you need to use

305
00:28:43,636 --> 00:28:55,534
for specific question, to me is something that can, with time, help you to decommission
specific data source or when migrating systems to new storages.

306
00:28:55,534 --> 00:28:57,214
And I heard this...

307
00:28:58,115 --> 00:29:06,020
I talked at Gartner last year when someone was comparing Hadoop to data lakes, data lake
houses and all of that.

308
00:29:06,020 --> 00:29:15,477
Because if you do not have like the semantic layer, the business understanding of what's
in it, um this big store of data doesn't really solve your problems.

309
00:29:15,477 --> 00:29:22,171
So basically bringing your old data, all of your data from all the databases in one data
store.

310
00:29:22,289 --> 00:29:26,783
isn't solving the problem of data discoverability and reconciliation.

311
00:29:26,783 --> 00:29:29,219
You should have have some semantics.

312
00:29:34,508 --> 00:29:37,092
What's the biggest driver for this?

313
00:29:37,092 --> 00:29:43,360
um Does it typically come to you from the business side or from the technical side?

314
00:29:43,961 --> 00:29:46,284
Who's your most passionate customer?

315
00:29:46,896 --> 00:29:57,096
Yeah, I think it's good news for the whole industry that everything about Agentech is
coming from the business side.

316
00:29:57,496 --> 00:30:03,216
So, and it's a good position to be in because this is where money is, right?

317
00:30:03,216 --> 00:30:05,256
This is where decision power is and so on and so forth.

318
00:30:05,256 --> 00:30:08,656
And you don't need to explain the technology anymore, right?

319
00:30:08,656 --> 00:30:13,495
You don't need to explain yourself anymore because, you know...

320
00:30:13,727 --> 00:30:25,127
I think it's the same that what happened with the internet in early 2000 with the dot-com
boom that the business side were like super inspired to create e-commerce use cases and

321
00:30:25,127 --> 00:30:26,627
what have you.

322
00:30:26,947 --> 00:30:28,707
And that's what's happening with agentic.

323
00:30:28,707 --> 00:30:39,667
It's business side that's already so inspired with all the capabilities of this new
technology that's actually inventing the use cases and the building business drivers and

324
00:30:39,667 --> 00:30:41,307
calculations behind that.

325
00:30:41,407 --> 00:30:43,681
This was usually the prerogative of

326
00:30:43,681 --> 00:30:45,501
of technical teams, right?

327
00:30:45,501 --> 00:30:49,701
To come up with a new technology and then find a compelling business case.

328
00:30:49,701 --> 00:30:51,761
And now it's the other way around.

329
00:30:51,841 --> 00:31:05,181
On the other hand, technical teams are struggling on the side to provide this type of
service that the business aspires to because of low data quality, because of low data

330
00:31:05,181 --> 00:31:13,901
readiness, and because of this multiple definitions where if you connect agents to them,
you know, it's a disaster.

331
00:31:14,861 --> 00:31:23,881
without any governance, any kind of layer between them or well-documented context and
reasoning.

332
00:31:25,161 --> 00:31:30,221
So yeah, there is a lot of excitement that pulls from the business side and we really
enjoy that.

333
00:31:30,821 --> 00:31:34,241
And for technical teams, we enjoy solving those problems.

334
00:31:34,241 --> 00:31:37,161
yeah, I think as an industry, we're in good place now.

335
00:31:37,392 --> 00:31:51,412
really seems like the innovation here is a fundamental paradigm shift from having business
intelligence and even data center engineers working within organizations to completely

336
00:31:51,412 --> 00:32:01,653
outsource the handling of any sort of data from your production systems because at end of
the day, they were always sort of a bottleneck for delivery.

337
00:32:01,653 --> 00:32:06,975
used to be someone's like, I need a dashboard for this or be able to answer the question
of how many we talk about monthly active users.

338
00:32:07,585 --> 00:32:08,906
Well, where is that data?

339
00:32:08,906 --> 00:32:09,646
What does it look like?

340
00:32:09,646 --> 00:32:15,942
And then figure out utilizing the tools to actually build the dashboards, having the data
in a single place, all that had to be solved.

341
00:32:15,942 --> 00:32:20,045
Whereas now those teams don't necessarily need to be working on that anymore.

342
00:32:20,045 --> 00:32:22,677
The data starts at the original application.

343
00:32:22,677 --> 00:32:24,369
You don't want a middle layer.

344
00:32:24,369 --> 00:32:33,486
You want it given to companies that understand how to sanitize it, what's relevant, where
the insights are, and providing an interface for those asking the questions to directly

345
00:32:33,486 --> 00:32:36,951
interact with the data in a understandable way rather than looking at,

346
00:32:36,951 --> 00:32:45,211
dashboards that are out of date or configure it's a ball play utilizing tools that just
don't really work that well because there's there's too many degrees of freedom too many

347
00:32:45,211 --> 00:32:51,663
variables too many columns or pieces of data that all needs to be displayed depending on
what you're actually looking for.

348
00:32:52,285 --> 00:32:58,842
Yeah, it's a good point because the majority of our decisions are ad hoc decisions on ad
hoc questions.

349
00:32:58,842 --> 00:33:02,635
I do believe that we'll still have space for KPIs and dashboards.

350
00:33:02,635 --> 00:33:07,440
Like I am starting my day with Google Analytics and Salesforce and all of that.

351
00:33:07,440 --> 00:33:10,114
So I don't want to ask those questions again and again.

352
00:33:10,114 --> 00:33:15,737
I just want to look at those dashboards and you know, I configured it this way I like and
it works for me.

353
00:33:16,895 --> 00:33:25,335
I also have like a bunch of questions which are not in those reports and is actually
changed with the data change every day, right?

354
00:33:25,335 --> 00:33:28,935
And I would like to have a tool which can help me with that.

355
00:33:29,395 --> 00:33:41,355
And for that, I think that we could be as practitioners, like as analytics practitioners,
we could be smart about it because if you actually get monitoring and agentic metadata

356
00:33:41,355 --> 00:33:46,095
analytics, you actually understand what are the interactions of users with the systems.

357
00:33:46,832 --> 00:33:49,424
dashboards which are actually useful, right?

358
00:33:49,424 --> 00:33:57,249
Because the biggest criticism from the end user that you build like bunch of dashboards
that we didn't ask for.

359
00:33:57,249 --> 00:34:03,993
now when they have this luxury of asking the questions freely, you can monitor what
they're asking about, right?

360
00:34:03,993 --> 00:34:10,359
Ask for asking for and actually create analytics they actually need and, you know, convert
into dashboards.

361
00:34:10,359 --> 00:34:16,300
Now, I'm definitely more on the anti-dashboard proponent, or guess dashboard antagonist.

362
00:34:16,300 --> 00:34:25,783
Like I find there's something very, uh like you're utilizing us at a crutch and maybe a
little bit lazy as far as not articulating what the challenges or the question you want

363
00:34:25,783 --> 00:34:26,134
answered.

364
00:34:26,134 --> 00:34:28,564
It's like, I'll just look at a dashboard of this information.

365
00:34:28,564 --> 00:34:32,376
Maybe the answer will pop up where what you really want to do is say, why am I looking at
the dashboard?

366
00:34:32,376 --> 00:34:33,846
What am I really looking for?

367
00:34:33,846 --> 00:34:35,747
and have the answer to your question.

368
00:34:35,747 --> 00:34:42,009
You don't care that the number of monthly active users that is increasing over time, but
maybe you're utilizing that to figure out, where are the biggest jumps?

369
00:34:42,009 --> 00:34:43,790
Why was there a jump here and there?

370
00:34:43,790 --> 00:34:51,914
And so the question you want to ask is, where are the biggest jumps and what happened to
my organization or to our customers or our competitors or in the global market that caused

371
00:34:51,914 --> 00:34:54,095
the biggest change in the last six months?

372
00:34:54,095 --> 00:35:02,018
And rather than looking at a graph that says that based on the underlying data, you're
getting the answer straight away and then you can actually take the next step.

373
00:35:02,338 --> 00:35:13,722
I guess one of the reasons I'm such a dashboard antagonist, I just coined that term right
now, ah is I mean, we focus a lot on high availability systems and high reliability.

374
00:35:13,902 --> 00:35:19,574
And you can't know that you're, like, you can't rely on a dashboard for telling you if
your system is up or down.

375
00:35:19,574 --> 00:35:24,115
Like, you need to know deterministically what the answer is at any single moment.

376
00:35:24,115 --> 00:35:26,906
And I think the only difference is from a business is,

377
00:35:27,020 --> 00:35:34,621
It's more long term, although I think a lot of companies delude themselves into thinking
that it can be a short term answer that I can just look at this right now and

378
00:35:34,621 --> 00:35:36,754
automatically know what my next step is that I should take.

379
00:35:36,754 --> 00:35:37,766
And it's a lot more.

380
00:35:37,766 --> 00:35:43,604
feel like deep dive into really understanding the correlations between the underlying data
stores.

381
00:35:44,491 --> 00:35:54,315
Yeah, I think Dashwords is the way that you can tell the story, like in coherent way, like
from, usually from the experience, right?

382
00:35:54,315 --> 00:36:02,939
And when you just asking questions, you know, using your Slack or Teams, it could be
random questions without the context and without continuation.

383
00:36:02,939 --> 00:36:05,200
It could be like just priority questions, right?

384
00:36:05,200 --> 00:36:10,352
So in Dashwords, usually when they build right, the best ones, right?

385
00:36:10,352 --> 00:36:11,343
Not all of them.

386
00:36:11,343 --> 00:36:13,934
You actually have a phenomena, right?

387
00:36:13,934 --> 00:36:14,494
A measure.

388
00:36:14,494 --> 00:36:19,881
and then a bunch of widgets that explain where it's coming from, like segmentation and...

389
00:36:21,078 --> 00:36:22,379
audiences and so forth.

390
00:36:22,379 --> 00:36:31,573
So basically it's a good way to visualize changes, but again this is the way that also
doesn't allow you to recognize new things.

391
00:36:31,573 --> 00:36:41,127
It might be a new factor that's affecting what's in your dashboard and you will never know
that because it's not automatically recognized and you're not popping up.

392
00:36:41,127 --> 00:36:51,032
Whereas when you use a Gentic you can know, okay, are all the factors with influencing
this spike and then you will actually see like what are the related features which can

393
00:36:51,032 --> 00:36:52,213
effect, right?

394
00:36:52,213 --> 00:36:56,476
So yeah, yeah, to your point, to me, dashboard is a good starting point.

395
00:36:56,476 --> 00:37:05,251
It's not an endpoint, but a starting point where you can actually start your exploration
going further, unless you have, you know, just a, you know, totally new question that you

396
00:37:05,251 --> 00:37:06,442
can use too.

397
00:37:06,442 --> 00:37:15,568
So to me, again, it's application free future, which we'll probably get in five years or
so that everything is going to be data driven.

398
00:37:15,568 --> 00:37:20,965
And, know, you don't have to have like myself, 50 different tabs open in your browser.

399
00:37:20,965 --> 00:37:26,168
and this might affect the upload speed of this podcast.

400
00:37:26,652 --> 00:37:28,380
Just 15.

401
00:37:28,393 --> 00:37:30,056
Those are rookie numbers.

402
00:37:30,911 --> 00:37:38,195
Yeah, so I guess we'll not have to use so many applications for everything and learning
all those applications.

403
00:37:38,195 --> 00:37:39,246
It's about experience.

404
00:37:39,246 --> 00:37:46,700
I have a question, I have a task, I want to complete that and I don't really care which
applications and data is involved.

405
00:37:49,262 --> 00:37:51,563
You don't think five years is realistic?

406
00:37:52,416 --> 00:38:00,742
I think that because of competition, are always and the segmenting of availability of data
in the market, like there's no public internet anymore.

407
00:38:00,742 --> 00:38:11,509
We've already seen the closing off of available data sources that every single one of
these applications is going to have access to just smaller pieces of data that are more

408
00:38:11,509 --> 00:38:15,694
focused and you're still going to have to go from app to app to get these questions
answered.

409
00:38:15,694 --> 00:38:22,222
And I think some companies are trying to push forward some way of still having like a
single pane of glass.

410
00:38:22,222 --> 00:38:31,822
to interact through utilizing MCP, the Model Context Protocol, or A2A, agent to agent,
thank you Google for coming up with something different.

411
00:38:32,022 --> 00:38:39,962
And even that, we can't even standardize on a single paradigm for a protocol to
communicate between agents.

412
00:38:40,642 --> 00:38:48,702
I think we failed up to this point in the year 2025 for humans to have one agreed upon
answer.

413
00:38:48,702 --> 00:38:51,054
I just don't see it happening unless.

414
00:38:51,054 --> 00:38:53,245
you fundamentally your daily driver changes.

415
00:38:53,245 --> 00:39:03,358
And I know on the software engineering side, we try to make it be the IDE of choice, but
even that still like, I don't think everyone is spending all of their time just in that

416
00:39:03,358 --> 00:39:04,118
one tool.

417
00:39:04,118 --> 00:39:08,100
You're still switching back and forth to different communication tools and whatnot.

418
00:39:08,180 --> 00:39:15,340
No, no, I'm just thinking like everyone's in favor of a single pane of glass as long as
I'm the provider of the single pane of glass.

419
00:39:15,612 --> 00:39:21,051
um So this is I think companies are going to be the owners of Singlesee.

420
00:39:21,294 --> 00:39:25,225
single source of truth as owners of their context and reasoning, right?

421
00:39:25,225 --> 00:39:29,646
Context and reasoning are not going to be part of any solution provider.

422
00:39:29,646 --> 00:39:31,726
It should be owned by company.

423
00:39:31,726 --> 00:39:39,468
And then you basically have your different agents connecting to your organizational
context and reasoning and not to each other.

424
00:39:39,468 --> 00:39:43,389
They communicate through this organizational context and reasoning.

425
00:39:43,449 --> 00:39:47,310
And to this point, it's of course is internal for external data sources.

426
00:39:47,310 --> 00:39:50,871
I would say in Europe, we actually see lots of

427
00:39:51,419 --> 00:40:00,239
Lots of openness about data sharing as far as standardization is provided and there is
lots of standardization already exposed.

428
00:40:00,239 --> 00:40:09,539
For example, for insurance and healthcare anonymized data, there is a public cloud which
is shared between a few of the countries in the European Union.

429
00:40:09,539 --> 00:40:12,847
And I think it's a big, you know...

430
00:40:12,847 --> 00:40:17,093
big leverage for any development of this kind.

431
00:40:17,093 --> 00:40:29,050
And in US we have, of course, power to share the operation between companies, again, as
far as anonymized, standardized, and could beneficial for all sites.

432
00:40:29,555 --> 00:40:34,880
I think that sort of maybe brings in a question, the raison d'etre, like of the existence
of the company.

433
00:40:34,880 --> 00:40:37,663
Like what are they doing that is fundamental?

434
00:40:37,663 --> 00:40:39,695
Like what is it that they're really trying to sell?

435
00:40:39,695 --> 00:40:43,108
And I feel like a lot of companies out there, they just copy each other.

436
00:40:43,108 --> 00:40:45,290
Like they're not creating something unique there.

437
00:40:45,290 --> 00:40:50,395
So I still see there always being an opportunity to own the data and sell it.

438
00:40:50,395 --> 00:40:55,431
And I think maybe this goes back to the question of if you're not providing something
unique,

439
00:40:55,431 --> 00:40:58,532
then other companies can spin up and still own the data.

440
00:40:58,532 --> 00:40:59,032
And why not?

441
00:40:59,032 --> 00:41:04,734
You pay some company to provide you with the answers to questions and manage all the data.

442
00:41:04,734 --> 00:41:14,827
I think this has been a model that has existed in certain areas with like user research
groups, for instance, ah think tanks, consulting companies that come and tell you how to

443
00:41:14,827 --> 00:41:18,911
just do your business exactly what the data should be and everything.

444
00:41:18,911 --> 00:41:21,159
I don't know.

445
00:41:21,411 --> 00:41:27,957
Even if in your own company you have a lot of data and you're like, we know how to utilize
the data most effectively, we can go and hire.

446
00:41:27,957 --> 00:41:31,008
engineering team to go create that single pane of glass.

447
00:41:31,008 --> 00:41:33,589
Eventually you're like, well, other companies could use that pane of glass too.

448
00:41:33,589 --> 00:41:34,469
We'll start selling it.

449
00:41:34,469 --> 00:41:37,410
And then that company becomes just the seller of a pane of glass.

450
00:41:37,410 --> 00:41:39,772
you know, that cycle is just gonna keep on going.

451
00:41:39,772 --> 00:41:47,868
But it really does bring up to the point where if another company can answer all of your
business questions for you, what is there left?

452
00:41:47,868 --> 00:41:50,288
to still be able to do uniquely?

453
00:41:51,847 --> 00:41:58,770
So every organization has their own proprietary data based on the nature of business and
the customer base.

454
00:41:58,770 --> 00:42:05,964
And even someone comes and says, okay, right now we are going to bring you into standards
about how to make business.

455
00:42:05,964 --> 00:42:08,735
You'll still customize it to what you already have.

456
00:42:08,735 --> 00:42:11,997
Like this is your advantage on one side.

457
00:42:11,997 --> 00:42:21,841
On the other side, yeah, if you have very special data and you want to sell it, you might
want to sell it in machine readable format, which is not reverse engineered.

458
00:42:21,841 --> 00:42:32,065
So you do not sell data by tables, like by kilos, but you sell your data as a semantic
embedding.

459
00:42:32,065 --> 00:42:39,788
So basically as machine-readable formats, other algorithms or agents can use it, but they
cannot decipher that.

460
00:42:41,729 --> 00:42:46,934
I think it's actually a more secure way to share your data for specific use.

461
00:42:46,934 --> 00:42:56,059
Speaking of which, what are the security concerns that you deal with whenever you have an
agent that has access to all of these different data sources?

462
00:42:56,957 --> 00:43:04,770
So in our organization, we actually choose to separate data values from data concepts from
agents.

463
00:43:04,771 --> 00:43:08,312
So agents only have access to data concepts.

464
00:43:08,312 --> 00:43:13,095
And then when the query is generated, it runs in separate environment on the data values.

465
00:43:13,095 --> 00:43:17,527
And Elumax does not ever touch data values of our customers.

466
00:43:17,527 --> 00:43:20,118
It just displays them to customers' applications.

467
00:43:20,118 --> 00:43:24,400
So we have total separation between agents and the data values themselves.

468
00:43:25,180 --> 00:43:26,854
So this approach actually allows

469
00:43:26,854 --> 00:43:32,486
you not to be concerned about data leakage or anything like that.

470
00:43:32,588 --> 00:43:38,728
I think every company will decide for themselves, this on-premise deployment is the right
choice.

471
00:43:38,728 --> 00:43:49,368
I would never actually vote for that because models investing so fast and you have limited
capability to upgrade them if you go for the on-premise deployment rather than just using

472
00:43:49,368 --> 00:43:52,348
APIs, which always go forward and so on so forth.

473
00:43:52,628 --> 00:43:58,968
But everyone will make their choices again based on sensitivity and proprietary nature of
the data.

474
00:43:58,968 --> 00:44:02,541
It's probably going to be level same way that we have cloud,

475
00:44:02,541 --> 00:44:09,263
uh storage with on-premises and different governance practices, it's going to be the same
for Agendik.

476
00:44:09,263 --> 00:44:19,605
Right now it's more more transparency about what's moving where and uh more recognition
about from the company side like what's critical and what's sensitive for them to

477
00:44:19,605 --> 00:44:24,466
basically send to third parties or what could be kept inside.

478
00:44:24,789 --> 00:44:27,000
I really like that perspective.

479
00:44:27,000 --> 00:44:37,027
If it was ever true in the past where you could be profitable with an on-prem data center
storing all your data and running all your compute, that must be less and less true every

480
00:44:37,027 --> 00:44:37,358
day.

481
00:44:37,358 --> 00:44:45,713
And you'd have to be doing something very special for you to find value in that because
technology is iterating even faster now.

482
00:44:45,933 --> 00:44:49,536
Any argument you would have had in the past is now no longer valuable.

483
00:44:49,536 --> 00:44:50,997
And so I'm totally with you.

484
00:44:50,997 --> 00:44:54,441
I don't understand even 10 years ago how people were justifying

485
00:44:54,441 --> 00:44:56,096
on-prem solutions.

486
00:44:56,220 --> 00:45:00,084
And now it's like even less of the case.

487
00:45:01,539 --> 00:45:02,860
of the case.

488
00:45:02,860 --> 00:45:22,153
oh

489
00:45:22,153 --> 00:45:27,014
transfer the, get the USB sticks on a giant truck and fly it to the data center.

490
00:45:27,014 --> 00:45:29,735
And I think that can work out.

491
00:45:29,735 --> 00:45:37,437
I mean, I think if anything now it's less about, like it must be less about the amount of
data you have and the rate of data creation.

492
00:45:37,537 --> 00:45:45,411
And I can see that with the number of, let's in a manufacturing plant or in healthcare,
like the number of sensors increasing every all the time.

493
00:45:45,411 --> 00:45:51,491
I'm wearing one here and I'm thinking about getting another one and it's just going to
increase more and more.

494
00:45:51,491 --> 00:45:55,671
And so with that increase, you need to be able to handle it much more effectively.

495
00:45:56,151 --> 00:46:01,671
I think storage costs coming down at the cloud providers is probably the next innovation
that will happen there.

496
00:46:01,671 --> 00:46:08,151
We just saw AWS's S3 one zone drastically reduced by like 85 % cost there.

497
00:46:08,151 --> 00:46:11,391
And I think we'll continue to see that as storage costs decrease over time.

498
00:46:11,391 --> 00:46:14,492
So it would just become more more feasible to put data in the cloud.

499
00:46:14,492 --> 00:46:15,510
cloud.

500
00:46:15,510 --> 00:46:16,605
to the agents.

501
00:46:17,018 --> 00:46:21,950
processing is always a bigger concern than storage for many, many years now.

502
00:46:21,950 --> 00:46:32,494
And I think this is also something that many of us in the news line, what it was like last
month, two months ago about DeepSeq.

503
00:46:32,494 --> 00:46:36,956
So how much will it cost to actually train model?

504
00:46:36,956 --> 00:46:39,777
actually have an inference running.

505
00:46:39,777 --> 00:46:45,143
So the costs of processing is shifting from training to use.

506
00:46:45,445 --> 00:46:47,936
of the models to the inference itself.

507
00:46:47,936 --> 00:46:54,399
And that's where I see that the majority of funds is going to be spent actually using a
GenTik on the data.

508
00:46:54,399 --> 00:47:04,884
And again, if it's more efficient on the cloud on data centers, uh I would say it's going
to be more efficient in the cloud because especially if you're not locked into specific

509
00:47:04,884 --> 00:47:07,885
providers, going to be more and more competition on that.

510
00:47:08,826 --> 00:47:15,282
And especially when you can recognize what data is garbage and what's not and kind of
limit the footprints.

511
00:47:15,282 --> 00:47:18,244
So everything, all the costs are going to be down.

512
00:47:18,244 --> 00:47:29,333
I think right now we spent lots of money already on the data pipelines, which are
duplicated to each other and not always feeding information that we actually use an

513
00:47:29,333 --> 00:47:36,118
application, but because companies do not monitor their metadata, they don't know what's
in use and what's not.

514
00:47:36,118 --> 00:47:36,839
Right?

515
00:47:36,839 --> 00:47:42,183
So we already have like lots of spend, which is predefined and you pay it anyhow.

516
00:47:42,183 --> 00:47:45,175
If you use your dashboards, if you use applications, if you are not,

517
00:47:45,175 --> 00:47:49,309
still paying the data pipelines that you have.

518
00:47:49,309 --> 00:48:01,118
So GenTik might replace this habit by actually invoking information and processing that
you use and not which is predefined for you by someone's assumption.

519
00:48:03,940 --> 00:48:15,255
This is probably an unpopular opinion, but I think we're going to look back decades from
now and say that making storage costs so inexpensive was the worst mistake we ever made.

520
00:48:15,255 --> 00:48:17,577
I mean, it's Jevons paradox, right?

521
00:48:17,577 --> 00:48:26,264
mean, anything that we don't want to have, we should not make more efficient because we
will eventually over utilize that thing.

522
00:48:26,264 --> 00:48:34,091
Yeah, I it happened in, I think really the industrial age in especially England with uh
coal mining.

523
00:48:34,091 --> 00:48:35,552
Yeah, I mean, for sure.

524
00:48:35,552 --> 00:48:42,358
uh People are already utilizing storage systems and it's an abusing ways cloud providers
have to...

525
00:48:42,358 --> 00:48:43,543
uh

526
00:48:43,543 --> 00:48:50,005
have a strategy for dynamically swapping out hard drives as they fail, because we haven't
improved the reliability of them.

527
00:48:50,005 --> 00:48:52,145
Just the, the side.

528
00:48:52,145 --> 00:48:52,866
Yeah, right.

529
00:48:52,866 --> 00:48:55,436
ah And, you know, that's sort of a problem.

530
00:48:55,436 --> 00:49:05,119
mean, I think it's a science fiction ideal that we figure out how to inscribe and write
and utilize data and sort of like a pure energy electromagnetic, you know, constrained

531
00:49:05,119 --> 00:49:07,030
field inside like diamonds or something.

532
00:49:07,030 --> 00:49:12,161
ah I mean, it'd be nice, honestly, Will, you're gonna start working on that?

533
00:49:12,826 --> 00:49:14,101
Yeah, probably not.

534
00:49:14,591 --> 00:49:15,538
Yeah

535
00:49:17,589 --> 00:49:18,457
you

536
00:49:18,735 --> 00:49:20,181
Where are the customers?

537
00:49:20,300 --> 00:49:21,581
Where are the customers?

538
00:49:21,581 --> 00:49:24,663
Well, my next gig is going to be in LGBT for sure.

539
00:49:24,663 --> 00:49:31,147
I think it's fascinating fields and I think we have more and more data to actually have a
breakthrough since this field.

540
00:49:31,147 --> 00:49:34,949
But yeah, yeah, I think data volumes are not necessarily a bad thing.

541
00:49:35,410 --> 00:49:38,792
But it's not about data volumes, it's about data variety really.

542
00:49:39,120 --> 00:49:45,507
I wouldn't say like actually have big data is advantage, but actually have rich data,
right?

543
00:49:45,507 --> 00:49:47,377
So this is where we should

544
00:49:47,377 --> 00:49:50,817
That's a really good point actually that I don't think anyone's brought up on on the show
before.

545
00:49:50,817 --> 00:49:59,257
I actually have a colleague that looked into the connections between networks, human
networks, but I think it applies here that as you said, it's not about the volume, the

546
00:49:59,257 --> 00:50:09,477
amount that you have, but there's some arbitrary aspect of the data that's like super
critical here, which is the say connectivity, but also the sparseness of it.

547
00:50:10,557 --> 00:50:13,197
I don't think there's a metric for that, for what that is.

548
00:50:13,197 --> 00:50:15,357
Maybe you're calling it something special.

549
00:50:15,433 --> 00:50:28,373
call it interoperability, maybe not the best word for that, but it means it's actually
like for data assets like table, you can have different types for analysis or for analysis

550
00:50:28,373 --> 00:50:31,413
you can use like different assets to fit it in.

551
00:50:31,413 --> 00:50:40,113
So interoperability is ability to match different features between different sources in a
way that is complementary.

552
00:50:43,074 --> 00:50:50,151
uh

553
00:50:50,151 --> 00:50:54,433
we know that hallucinations are coupled to utilizing a straight transformer architecture.

554
00:50:54,433 --> 00:50:57,865
uh You know, if you're using transfer architecture, you must have hallucination.

555
00:50:57,865 --> 00:51:05,339
So you must be doing something special that other companies aren't utilizing, you know,
different from what the LLMs are, are building.

556
00:51:05,359 --> 00:51:06,990
Is that something you can talk about?

557
00:51:07,089 --> 00:51:08,289
Yeah, sure.

558
00:51:08,289 --> 00:51:14,589
So our approach is to ground a single source of truth in your knowledge graph, right?

559
00:51:14,589 --> 00:51:17,109
In your business ontology, which is transparent, right?

560
00:51:17,109 --> 00:51:20,589
This business ontology is represented as a knowledge graph of semantic embeddings.

561
00:51:20,589 --> 00:51:26,729
So for starters, you only have kind of organizational agreement on what business logic is.

562
00:51:27,069 --> 00:51:33,848
And in addition to that, we actually ground your experience only to this business
ontology.

563
00:51:33,848 --> 00:51:34,248
Right.

564
00:51:34,248 --> 00:51:43,688
So we reduce the degrees of freedom of the model not to think widely about universe, but
to think about your companies and the universe.

565
00:51:43,688 --> 00:51:48,628
So when you ask questions about active users, it will not think about Wikipedia definition
of active users.

566
00:51:48,628 --> 00:51:54,368
It will think about your business metric definition of active user, maybe coming from your
BI tool.

567
00:51:54,368 --> 00:51:54,888
Right.

568
00:51:54,888 --> 00:52:00,688
So it's really grounding the experience of the users in the single source of truth of your
organization.

569
00:52:00,688 --> 00:52:03,748
In addition, because we always

570
00:52:03,872 --> 00:52:09,306
built this ontology based on the metadata, we understand the context much better.

571
00:52:09,306 --> 00:52:20,803
We do not only understand the context of user interaction within specific memory frame in
the copilot, we also understand the user interactions with any system which is connected

572
00:52:20,803 --> 00:52:21,934
to LMX.

573
00:52:22,374 --> 00:52:32,751
So it's basically previous interactions with operational systems, with analytics systems,
so our context is much wider and we can have much more personalized experience for the

574
00:52:32,751 --> 00:52:33,142
user.

575
00:52:33,142 --> 00:52:33,792
ah

576
00:52:33,792 --> 00:52:36,347
based on this metadata access.

577
00:52:36,629 --> 00:52:38,954
And the third reason is because...

578
00:52:38,954 --> 00:52:39,194
Okay.

579
00:52:39,194 --> 00:52:43,574
So the first reason was the business ontology and single source of truth grounding for
experience.

580
00:52:43,574 --> 00:52:45,854
The second one is personalization.

581
00:52:45,854 --> 00:52:53,914
And the third one, because we do have business ontologies, which are complementary, like
in their language and so on and so forth.

582
00:52:54,134 --> 00:53:06,314
Before the customization for specific company, when users use different language, which is
different from the business metrics definitions in their organization, we can pick it up

583
00:53:06,314 --> 00:53:07,994
from our...

584
00:53:08,621 --> 00:53:17,938
generic ontologies about this vertical because people switch companies, they might use
different lingo, different abbreviations, which are not necessarily implemented in this

585
00:53:17,938 --> 00:53:18,328
company.

586
00:53:18,328 --> 00:53:23,452
And we have not only user context, but we also have industry context.

587
00:53:23,452 --> 00:53:25,414
So we can pick up this language.

588
00:53:25,414 --> 00:53:32,679
So those three reasons allow us to have much reduced experience on one side.

589
00:53:32,679 --> 00:53:36,601
On the other side, it's uh very, very personalized.

590
00:53:37,280 --> 00:53:39,635
So you cannot ask ElumX about weather.

591
00:53:39,635 --> 00:53:42,905
You can only ask ElumX about your connected data.

592
00:53:42,905 --> 00:53:53,045
So you're not utilizing as uh much of a probabilistic model as uh other companies that
have built their own foundational models.

593
00:53:53,483 --> 00:54:03,047
We haven't built foundational models, but we use dozens of semantic models and two dozen
of graph models for different tasks from onboarding to the user experience and

594
00:54:03,047 --> 00:54:06,149
explainability to provide this type of experience.

595
00:54:06,149 --> 00:54:10,071
And we always keep an eye on the latest and greatest.

596
00:54:10,071 --> 00:54:15,753
we also, when the new models come out, we test them and see how we can embed it in our
ensemble.

597
00:54:15,753 --> 00:54:19,835
And it helps us to increase accuracy over time.

598
00:54:20,055 --> 00:54:23,487
But I think the biggest thing is we give the ownership

599
00:54:23,487 --> 00:54:26,358
context and reasoning for the organization that we serve.

600
00:54:26,358 --> 00:54:31,399
We automatically build it for them and from now on they're the owners of the context and
reasoning.

601
00:54:31,399 --> 00:54:37,121
And if they want to plug tomorrow NVIDIA names or AWS Bedrock, they can use this context.

602
00:54:37,121 --> 00:54:40,272
So it's kind of, you know, it's transparent and it's reusable.

603
00:54:40,272 --> 00:54:43,043
So for us, this is the biggest benefit actually.

604
00:54:44,057 --> 00:54:46,639
So there's still a chance that it will hallucinate.

605
00:54:46,639 --> 00:54:51,701
It's just very, very low and it will stay within the context of the business domain.

606
00:54:51,701 --> 00:54:54,735
no, it's not a hallucination, it's a guided spiritual journey.

607
00:54:54,735 --> 00:55:07,185
If you do have many versions of truth, for example, you just introduced a new definition
of active user in your dashboard, it makes big steps up.

608
00:55:07,185 --> 00:55:12,889
And if someone asks about active user, we might offer like, okay, there is new definition
in your BI dashboard.

609
00:55:12,889 --> 00:55:15,044
Would you like to get the answer on that?

610
00:55:15,044 --> 00:55:26,363
Well, as long as there's a probability of how you generate a solution, the answer, there's
always a chance for it to pick, it just make something up even if you have tried to

611
00:55:26,363 --> 00:55:29,175
constrain it by actual definitions.

612
00:55:29,175 --> 00:55:32,377
Otherwise, that's just a fundamental aspect of probabilities.

613
00:55:32,377 --> 00:55:40,323
So, I mean, while you can definitely reduce it and eliminate duplicate definitions,
there's a whole other part of the transformer architecture which...

614
00:55:40,468 --> 00:55:43,486
fundamentally requires the creation of hallucinations.

615
00:55:43,486 --> 00:55:46,573
I don't think you can have a transformer architecture without that.

616
00:55:48,087 --> 00:55:51,769
Again, it's a good point and we provide explainability about the answers.

617
00:55:51,769 --> 00:55:56,231
So it's not like you're a question and you have a number as an answer.

618
00:55:56,231 --> 00:55:57,802
actually provide full explainability.

619
00:55:57,802 --> 00:55:59,493
Like, this how we understand the questions?

620
00:55:59,493 --> 00:56:03,975
This is a semantic entity that we made this question to and this is logic and all of that.

621
00:56:03,975 --> 00:56:14,029
And if user would like to base their answer on different logic, they can actually choose
like this not autopilot mode and see, okay, this is the related semantic entities to a

622
00:56:14,029 --> 00:56:14,780
question.

623
00:56:14,780 --> 00:56:17,256
You know, you can pick up from them if you'd

624
00:56:17,256 --> 00:56:17,701
sense.

625
00:56:17,701 --> 00:56:23,035
have like, my husband he drives Alfa Romeo, Mito manual stick, right?

626
00:56:23,035 --> 00:56:26,357
So he will always prefer to have better control.

627
00:56:26,357 --> 00:56:27,598
We just back from Italy.

628
00:56:27,598 --> 00:56:31,530
So those are the roads we created for manual driving.

629
00:56:31,771 --> 00:56:35,363
some data is created for manual selection probably, right?

630
00:56:35,363 --> 00:56:41,177
If it's like super messy, you might want to select it manually.

631
00:56:41,177 --> 00:56:46,180
I would say like we will, of course, as an industry, we are going to be more and more
automated.

632
00:56:46,180 --> 00:56:47,228
You know, some people just

633
00:56:47,228 --> 00:56:53,684
it's like the idea of control more so than actually control.

634
00:56:53,684 --> 00:56:56,196
You don't want the manual.

635
00:56:56,196 --> 00:56:56,476
shift.

636
00:56:56,476 --> 00:57:01,663
You want to be told it's a manual stick shift, but if you mess up and do the wrong thing,
the right thing still happens.

637
00:57:02,730 --> 00:57:03,661
That's true, that's true.

638
00:57:03,661 --> 00:57:08,012
are always, you know, yeah, we have systems like EBS and all that to keep us safe.

639
00:57:08,012 --> 00:57:09,018
That's true.

640
00:57:10,503 --> 00:57:13,545
So you want to shift the gears but you don't want to dump the clutch

641
00:57:14,637 --> 00:57:16,137
Probably not.

642
00:57:16,218 --> 00:57:19,736
Not over Lake Como, like, you know, 200 meters above the water.

643
00:57:19,736 --> 00:57:21,641
No, not really.

644
00:57:22,443 --> 00:57:23,523
Not really.

645
00:57:23,614 --> 00:57:24,197
Awesome.

646
00:57:24,197 --> 00:57:27,428
So it feels like this might be a good place to roll into picks.

647
00:57:27,428 --> 00:57:28,389
What do think?

648
00:57:29,304 --> 00:57:32,744
Warren, you're never going to guess what's happening next.

649
00:57:32,744 --> 00:57:34,004
I'm going first.

650
00:57:34,285 --> 00:57:37,586
Yeah, so I got a really controversial good one here.

651
00:57:37,586 --> 00:57:41,068
ah There's Yeah, like, like I like I like it.

652
00:57:41,068 --> 00:57:44,610
ah So there's this great article that I read through.

653
00:57:44,610 --> 00:57:46,630
It's short, it's short form.

654
00:57:46,630 --> 00:57:48,461
So it should be easy for anyone to get through.

655
00:57:48,461 --> 00:57:56,895
It's basically the idea of how intuition is being used in software engineering and whether
or not LMS are capable of intuition.

656
00:57:57,346 --> 00:58:03,109
it is actually a proof that shows we can't have AGI with transform architecture.

657
00:58:03,109 --> 00:58:05,711
Our LLMs will never be able to reason.

658
00:58:05,731 --> 00:58:15,097
And it utilizes uh Google's incompleteness theorem, the non-computability of intuition,
and the computability of Turing machines.

659
00:58:15,118 --> 00:58:19,531
And just with that, we can actually prove fundamentally that we can have AGI with our
current systems.

660
00:58:19,531 --> 00:58:21,282
We haven't gotten any closer to that.

661
00:58:21,282 --> 00:58:24,780
So don't listen to the lies that people have.

662
00:58:24,780 --> 00:58:26,308
from massive

663
00:58:26,308 --> 00:58:39,268
quote unquote AI companies because the real argument here is that in order for us to have
AGI, you need to introduce intuition and that's the exact thing that's lacking in attorney

664
00:58:39,268 --> 00:58:40,248
machines.

665
00:58:40,628 --> 00:58:41,508
Okay, I have question for you.

666
00:58:41,508 --> 00:58:43,008
Are you born with intuition?

667
00:58:43,008 --> 00:58:45,848
Yeah, well there is a...

668
00:58:45,848 --> 00:58:50,072
Just answer the question, Warren.

669
00:58:50,072 --> 00:58:52,836
Yeah, it means experience really.

670
00:58:53,939 --> 00:58:54,208
Yeah.

671
00:58:54,208 --> 00:59:02,085
yeah, I mean, it's really hard to identify even what happens uh as an individual, let
alone if we can believe it on an external system.

672
00:59:02,085 --> 00:59:08,381
Luckily, uh the Turing machines we know are closed systems there.

673
00:59:08,381 --> 00:59:15,947
I likened it to this uh great quote, which will be a future pick of in a future episode, a
parrot reciting Shakespeare.

674
00:59:16,003 --> 00:59:23,130
That's that's LLMS today and you would never claim that that a parrot, know would fully
understand, you know what it's reciting there.

675
00:59:23,130 --> 00:59:27,014
ah And that's that's unfortunately the extent of our technology.

676
00:59:27,014 --> 00:59:28,165
That's my pick.

677
00:59:28,254 --> 00:59:29,378
All right, Ena, you're up.

678
00:59:29,378 --> 00:59:30,961
What did you bring for a pick?

679
00:59:32,163 --> 00:59:33,683
for pick a.

680
00:59:33,965 --> 00:59:37,288
Well, I wasn't ready for that.

681
00:59:37,288 --> 00:59:41,491
But I must say, yeah, so let's speak about AGI as well.

682
00:59:41,631 --> 00:59:55,262
I do not believe in AGI in the next five to 10 years at least, just the fact that we as
humans, we're capable of applying context from one experience to a different experience.

683
00:59:55,262 --> 01:00:03,649
So I do not call it intuition because intuition to me is just experience, but our ability
to merge contexts which are vividly.

684
01:00:04,374 --> 01:00:05,753
not connected.

685
01:00:05,753 --> 01:00:08,773
is where the human spark is.

686
01:00:08,773 --> 01:00:15,333
And to me, AGI is not going to be near that in foreseeable future, let's say 10 years.

687
01:00:15,473 --> 01:00:24,453
So think about, you can apply your knowledge from cooking to your knowledge of right now,
coding or something like that, like what's the ingredients and so on and so forth.

688
01:00:24,453 --> 01:00:34,373
So our associations work differently than machine association and this context merge from
unrelated experiences is something that machines are not good with.

689
01:00:34,565 --> 01:00:37,446
I like how you went to the philosophy side of this.

690
01:00:37,446 --> 01:00:45,441
uh know, there's this idea that the universe is deterministic and that everything is
connected through the collapse of Schrodinger's equation, the wave function.

691
01:00:45,441 --> 01:00:50,573
oh, I don't know, I was ready to go there.

692
01:00:53,010 --> 01:00:54,110
Yes.

693
01:00:55,289 --> 01:01:03,950
I do agree, know, fundamentally there is something missing from the computing systems that
we build today in order to actually achieve AGI.

694
01:01:03,950 --> 01:01:08,193
my pick's gonna take this down a whole big notch.

695
01:01:08,193 --> 01:01:15,549
Because we were talking about having the number of tabs that you have open in your browser
for the last couple months.

696
01:01:15,549 --> 01:01:26,536
I've been using the Arc browser and specific to that conversation, one of the things Arc
does is any tab that you haven't touched in the last 30 days, it just closes it for you.

697
01:01:26,746 --> 01:01:30,729
And I used to have a bunch of tabs open and I was like, okay, I'm going to try this.

698
01:01:30,729 --> 01:01:31,439
I'm going to hate it.

699
01:01:31,439 --> 01:01:34,731
I'm going to figure out how to turn that feature off or I'm going to quit using it.

700
01:01:34,831 --> 01:01:40,315
After several months, it's closed, probably hundreds of tabs for me and I've not noticed.

701
01:01:40,315 --> 01:01:43,817
So go try out the ARC browser.

702
01:01:43,817 --> 01:01:45,412
You don't need all those tabs.

703
01:01:45,412 --> 01:01:52,219
I think I'm having a little bit of uh neurological meltdown just at hearing about that
feature, Will.

704
01:01:54,559 --> 01:01:56,050
It's panic inducing.

705
01:01:56,050 --> 01:01:57,054
Yeah, for sure.

706
01:01:57,054 --> 01:01:58,501
Definitely.

707
01:01:58,501 --> 01:02:04,763
I there are tabs that I actually leave there that I know are there and I don't want them
to go away.

708
01:02:06,750 --> 01:02:16,450
Cool I have a follow-up question for you Warren though you mentioned that you wear you're
currently wearing one IOT device and you're thinking about getting another one what are

709
01:02:16,450 --> 01:02:18,030
you wearing and what are you thinking about getting?

710
01:02:18,030 --> 01:02:22,091
Yeah, so this isn't my pick, but I'm wearing the Google Pixel watch too.

711
01:02:22,091 --> 01:02:26,823
um I would definitely not recommend anyone to get a smartwatch ever.

712
01:02:26,823 --> 01:02:28,453
So that's one.

713
01:02:28,453 --> 01:02:30,564
ah So I want a replacement.

714
01:02:30,564 --> 01:02:34,606
This thing disturbs me while I'm sleeping and I would really like to get my sleep metrics.

715
01:02:34,606 --> 01:02:36,896
And so I've been looking at alternatives there.

716
01:02:37,949 --> 01:02:38,677
I...

717
01:02:56,401 --> 01:02:57,261
curious.

718
01:02:57,261 --> 01:02:59,135
I have

719
01:02:59,135 --> 01:03:05,927
For my watch, have a Garmin Phoenix, which is technically a smartwatch, but I have
everything turned off on it.

720
01:03:06,127 --> 01:03:11,538
The only thing I use it for is heart rate and metrics whenever I'm out for a run.

721
01:03:11,538 --> 01:03:20,671
And I had an aura ring for a while and same thing with you is like, I don't want to pay a
subscription for it.

722
01:03:20,811 --> 01:03:28,633
And I know a lot of people who use the whoop band, uh but it's another subscription based
service, but there are

723
01:03:28,881 --> 01:03:31,114
Like just let me, let me buy it and go on with my life.

724
01:03:31,114 --> 01:03:32,349
Is that cool with you?

725
01:03:32,349 --> 01:03:42,316
But trying to push updates out onto devices that people you don't have access to, like
that's not a fun place to be.

726
01:03:42,316 --> 01:03:50,722
Even from like my days of supporting mobile apps, just trying to get people to update was
frustrating.

727
01:03:50,820 --> 01:03:53,685
All right, you know, thank you so much for joining us today.

728
01:03:53,685 --> 01:03:55,307
This has been a lot of fun.

729
01:03:55,649 --> 01:03:57,884
Likewise, I really enjoyed the composite.

730
01:03:57,884 --> 01:03:59,644
Warren, as always, thank you.

731
01:03:59,644 --> 01:04:02,024
Appreciate you being on the show.

732
01:04:02,684 --> 01:04:09,824
And to all the listeners, thank you very, very much because you're kind of the reason that
we do this.

733
01:04:09,824 --> 01:04:11,364
So hopefully you enjoyed this.

734
01:04:11,364 --> 01:04:13,684
If not, you know how to find us and let us know.

735
01:04:13,684 --> 01:04:15,350
And we'll see you all next week.

