1
00:00:01,725 --> 00:00:03,028
And we're live.

2
00:00:03,028 --> 00:00:06,657
Welcome back everyone to another episode of Adventures in DevOps.

3
00:00:06,657 --> 00:00:11,818
Hosting today with me is Jillian and Jillian are you looking forward to today's episode?

4
00:00:23,940 --> 00:00:12,827
Hi, everybody.

5
00:00:19,669 --> 00:00:16,591
mean, when we planned this episode, I had a strong sense that this was going to be a
popular choice for you.

6
00:00:16,591 --> 00:00:27,311
And that's because today I want to invite Berzan Mozafari as our guest, MIT alum and
University of Michigan associate professor working with data-intensive systems for over 15

7
00:00:27,311 --> 00:00:27,646
years.

8
00:00:27,646 --> 00:00:28,636
Welcome.

9
00:00:28,856 --> 00:00:31,020
Thank you so much, great to be here with you.

10
00:00:31,128 --> 00:00:41,071
Yeah, you know, I have worked in a lot of engineering organizations and data has always
been this aspect of an area that no one wants to touch.

11
00:00:41,911 --> 00:00:56,755
There's stuff going on there and I just always seem like there's bigger business problems
that are at play or there's other challenges present, but I always got scared when the

12
00:00:56,755 --> 00:00:59,976
topic of interfacing with one of the data teams comes up.

13
00:01:00,921 --> 00:01:10,217
And I noticed from your history, your profile, there's a lot of different aspects of data
systems that you seem like you've experienced that.

14
00:01:10,217 --> 00:01:11,788
You want to talk a little bit about that?

15
00:01:12,440 --> 00:01:15,020
Sure, I agree with you.

16
00:01:15,020 --> 00:01:18,912
think data can be pretty intimidating depending on what people...

17
00:01:19,202 --> 00:01:21,843
make of it and what's the expectation.

18
00:01:21,843 --> 00:01:26,715
But typically it's considered like the gold of the modern digital era.

19
00:01:26,715 --> 00:01:35,909
So there's usually a lot of potential in it and like anything with a lot of potential, you
sometimes it comes with anxiety for the data teams or those who have to interface with

20
00:01:35,909 --> 00:01:36,410
them.

21
00:01:36,410 --> 00:01:45,313
But yeah, I've, throughout my career, as you pointed out, like at this point, almost the
last two decades, I've been working at the intersection of machine learning and database

22
00:01:45,313 --> 00:01:49,415
systems, essentially pursuing this idea of how we can leverage the

23
00:01:49,415 --> 00:01:59,262
or AI in general to build smarter data systems where smarter could mean faster, more
scalable, easier to use, easier to deploy, et cetera.

24
00:01:59,262 --> 00:02:09,449
Some of the work we've done is now part of open source transactional databases like MySQL
and whatnot, running on millions of servers, but that was all open source work.

25
00:02:09,449 --> 00:02:15,693
Some of the other aspects of my work has been on analytical databases, cloud data
warehousing and whatnot.

26
00:02:16,154 --> 00:02:18,976
Some of the work we did on approximate query processing, for example,

27
00:02:18,976 --> 00:02:22,285
was a good example of combining systems and

28
00:02:23,630 --> 00:02:31,214
statistical learning theory that got commercialized and eventually acquired as part of the
Snappy data product.

29
00:02:31,214 --> 00:02:38,737
But the latest spin-off is some of the work we're doing now with cloud data warehousing
and building a data learning tier.

30
00:02:38,737 --> 00:02:53,424
So we've worked at a very high level from the inside generation and root cause analysis
all the way to the almost metal of how to run queries more efficiently at the CPI.

31
00:02:53,424 --> 00:03:00,188
GPU level, the GPU level, the common theme is that it's all fun.

32
00:03:00,208 --> 00:03:04,681
Anything that has to do with data, with machine learning, I usually get excited over it.

33
00:03:04,681 --> 00:03:09,013
So I could be talking all day about those different aspects, but that's the common theme.

34
00:03:09,176 --> 00:03:10,747
How did you get into it?

35
00:03:16,460 --> 00:03:22,394
Did you think that maybe when you were a younger student that this would always be an area
that was most interesting or did it fall into your lap one day based off of experiments or

36
00:03:22,394 --> 00:03:25,761
labs that you were working on and this seemed like the most interesting thing?

37
00:03:26,625 --> 00:03:27,876
That's a very good question.

38
00:03:27,876 --> 00:03:35,480
It's like most things that you end up liking, it's hard to tell like when it actually
started.

39
00:03:35,480 --> 00:03:39,122
It's usually so subtle and...

40
00:03:40,643 --> 00:03:45,855
hard to tell that you can't really tell where it started, but no, actually started my, my
passion was in algorithms.

41
00:03:45,855 --> 00:03:55,400
I remember I was always like, you know, from early days, I was like into math and
statistics and, and figuring out like, you know, how many tries it's going to take to sort

42
00:03:55,400 --> 00:03:57,661
of get to a particular outcome with high confidence.

43
00:03:57,661 --> 00:04:06,905
I was always, to be honest with you, I was always intrigued by the power of statistics and
like, you know, seeing how you can get a lot further ahead in life.

44
00:04:06,905 --> 00:04:10,507
If you know more about statistics than, than most people, like something as simple

45
00:04:10,507 --> 00:04:13,909
is like, you know, people playing heads or tails, right, with coins.

46
00:04:13,909 --> 00:04:17,162
Like, you know, if you know what you're doing, you can sort of come up with creative ways.

47
00:04:17,162 --> 00:04:20,804
But no, I think a lot of it started when I went to grad school.

48
00:04:20,804 --> 00:04:25,817
I started my PhD program with my advisor was a legend in relational database systems.

49
00:04:25,817 --> 00:04:28,979
But then he was also seeing the potential at the time.

50
00:04:28,979 --> 00:04:30,921
was like data mining was the hot thing, right?

51
00:04:30,921 --> 00:04:33,693
And then that was the foray into statistics.

52
00:04:33,693 --> 00:04:37,595
And then like, you know, later applied machine learning, learning theory.

53
00:04:37,855 --> 00:04:38,816
It was a progression.

54
00:04:38,816 --> 00:04:40,377
And I think that

55
00:04:40,377 --> 00:04:45,057
trends that we were seeing in the industry was also helping with that.

56
00:04:45,857 --> 00:04:59,537
But to your point, there's a lot of people in academia who are just kind of content with
just coming up with cool ideas that just remain as that.

57
00:04:59,537 --> 00:05:06,617
Their ideas always remain as ideas, but that was always left a little bit.

58
00:05:07,607 --> 00:05:13,947
Disappointed that an idea that I thought hey has a lot of potential would never make it to
production systems

59
00:05:14,615 --> 00:05:22,195
So a lot of what I did earlier in my career was working with industry partners, partnering
with different companies and trying to get adoption for free.

60
00:05:22,195 --> 00:05:32,135
Some of it will be open source, got massive adoption, but some of it was like pulling
teeth to go and like, know, convince a bureaucratic enterprise like why this is in your

61
00:05:32,135 --> 00:05:33,235
best interest to adopt it.

62
00:05:33,235 --> 00:05:36,815
And by the way, we don't expect any money from you in return.

63
00:05:37,075 --> 00:05:38,935
I'm just driven by the impact, right?

64
00:05:38,935 --> 00:05:44,569
But at some point you realize, look, you're gonna put your money where your mouth is

65
00:05:44,569 --> 00:05:51,824
And at some point, lot of entrepreneurs realized, hey, an idea is nice, but it's the
execution that matters.

66
00:05:51,824 --> 00:05:53,495
And then you just spin it off.

67
00:05:53,495 --> 00:06:06,163
And that's how I started commercializing some of these ideas with the main motivation of
closing that gap between the theory and practice, something that's solid and works, but

68
00:06:06,163 --> 00:06:11,997
getting into a place that's consumable by data teams, by products, has a real world
impact.

69
00:06:11,997 --> 00:06:14,108
So I think that's where a lot of that

70
00:06:14,539 --> 00:06:16,183
interest evolved into.

71
00:06:17,153 --> 00:06:27,730
I think that's really interesting that you were able to kind of bridge the gap between
research and academia and getting to really build stuff because I think that's a tough

72
00:06:27,730 --> 00:06:27,940
one.

73
00:06:27,940 --> 00:06:35,825
That's a tough one for people who are in academia and get kind of frustrated by the
process and, you know, for the reasons that you described to figure out what to do.

74
00:06:35,825 --> 00:06:39,027
And I think most people just end up jumping ship to industry.

75
00:06:39,027 --> 00:06:42,961
So I think that's really cool that you could find a bridge there.

76
00:06:42,961 --> 00:06:45,001
Yeah, I won't lie to you.

77
00:06:45,001 --> 00:06:45,653
It's not as easy.

78
00:06:45,653 --> 00:06:47,664
A lot of people fall off that bridge.

79
00:06:48,005 --> 00:06:51,347
As they try to cross that bridge, a lot of them fall into the water.

80
00:06:53,409 --> 00:07:00,975
I think what happens is that, like in academia, we have this system which is kind of
designed.

81
00:07:04,235 --> 00:07:04,995
how should I say this?

82
00:07:04,995 --> 00:07:09,340
It's designed to kind of reward complexity, right?

83
00:07:09,340 --> 00:07:13,113
And if an idea works, it's simple, it's not as rewarded.

84
00:07:13,113 --> 00:07:21,309
I remember we came up with this algorithm that was improving the average performance of
queries by a significant margin.

85
00:07:22,763 --> 00:07:25,486
I forgot the number, but it was something like close to an order of magnitude.

86
00:07:25,486 --> 00:07:31,389
And then we submitted this paper to this extremely prestigious conference, academic
conference.

87
00:07:31,389 --> 00:07:38,098
And then the feedback, one of the reviewers, the feedback was like, hey, if this was an
important thing, someone else would have done it by now.

88
00:07:38,098 --> 00:07:42,120
I remember one of my, or actually two of my PhD students at the time, they worked

89
00:07:42,120 --> 00:07:43,941
with the open source community.

90
00:07:43,941 --> 00:07:47,353
They went there and they said, hey, you guys have this transaction scheduling algorithm.

91
00:07:47,353 --> 00:07:49,405
We have a smarter version of it.

92
00:07:49,405 --> 00:07:53,708
We've worked on it in an academic setting, but here's why it's significantly more
performant.

93
00:07:53,708 --> 00:07:56,830
We just want you to consider making this an option.

94
00:07:56,830 --> 00:08:00,372
So kudos to those open source developers in the MySQL community.

95
00:08:00,372 --> 00:08:02,063
They went and they did their own research.

96
00:08:02,063 --> 00:08:03,184
They tried our idea.

97
00:08:03,184 --> 00:08:03,764
They came back.

98
00:08:03,764 --> 00:08:06,636
They said, this is so much better than what our default is.

99
00:08:06,636 --> 00:08:08,360
We're not going to add it as an option.

100
00:08:08,360 --> 00:08:12,300
We're going to make this the new default and make the existing algorithm an option.

101
00:08:12,300 --> 00:08:21,686
So then the next time around we submitted that same paper exact same algorithm exact same
result and we said it by the way It is pretty important because now more than two million

102
00:08:21,686 --> 00:08:25,989
servers in the world are using this as their default algorithm And that's just one
example.

103
00:08:25,989 --> 00:08:35,055
There's a lot of good ideas that get killed But then again, there's a lot of important but
voting problems in the industry as well like, you

104
00:08:35,812 --> 00:08:41,112
tell my former PhD students that when you pick a problem you need to ask three questions.

105
00:08:41,112 --> 00:08:42,692
Is it important enough?

106
00:08:42,832 --> 00:08:44,672
Do you have the skills to solve it?

107
00:08:44,672 --> 00:08:47,332
And do you have what it takes to get it in the right hands?

108
00:08:47,332 --> 00:08:52,552
So I think if you sort of look at those three problems holistically, you can find your

109
00:08:52,552 --> 00:08:59,983
away from interesting, innovative, highly technical ideas and then still have a real
impact.

110
00:08:59,983 --> 00:09:02,233
I...

111
00:09:02,233 --> 00:09:05,579
feature like a bit of, you know, like just that little bit of spite and petty.

112
00:09:05,579 --> 00:09:08,151
I think you're such a, it's such a human motivator.

113
00:09:08,151 --> 00:09:08,812
It is.

114
00:09:08,812 --> 00:09:21,403
I'm surprised though because like a lot of conferences that I applied to I don't get any
advice back, but I feel like the feedback of someone would have done that already if it

115
00:09:21,403 --> 00:09:23,064
was meaningful.

116
00:09:23,385 --> 00:09:23,945
what is that?

117
00:09:23,945 --> 00:09:26,627
Like what is the purpose of saying those words?

118
00:09:27,248 --> 00:09:31,342
It's sort of like you're driving like, I'm gonna, you can only go from spite there.

119
00:09:31,342 --> 00:09:35,367
And I feel like that's not a good, necessarily good place to be driven from.

120
00:09:35,367 --> 00:09:44,446
realistically like why wouldn't they say like be specific like hey you know it'd be great
if it was being used in the industry already like if this is so ingenious if this is so

121
00:09:44,446 --> 00:09:46,553
great examples of

122
00:09:46,553 --> 00:09:56,358
I think, like, if you look at sort of how academics excel, right, the idea is you want to
find out what others have done and you just need to do something better than that.

123
00:09:56,358 --> 00:10:02,551
And it doesn't matter if that problem is actually realistic, if the assumptions are
realistic, it has to be innovative, right?

124
00:10:02,551 --> 00:10:04,073
Like if it's a simple idea.

125
00:10:04,073 --> 00:10:06,583
I mean, the example I can give you is Spark, right?

126
00:10:06,583 --> 00:10:07,293
Apache Spark.

127
00:10:07,293 --> 00:10:09,328
A lot of your audience are probably familiar with it.

128
00:10:09,328 --> 00:10:12,125
So the initial idea was pretty small, pretty simple.

129
00:10:12,125 --> 00:10:15,547
You have a working set, you have a data set that you

130
00:10:15,547 --> 00:10:19,569
want to keep doing the same computation on it.

131
00:10:19,569 --> 00:10:22,652
So back in the day, Hadoop days, right?

132
00:10:22,652 --> 00:10:32,158
For those of you audience who still remember, you had to basically take that intermediate
result, write it back to disk, and then if it was an area of computation, only to read it

133
00:10:32,158 --> 00:10:34,527
back into main memory immediately after you have written it.

134
00:10:34,527 --> 00:10:36,361
So there's a lot of redundant I.O.

135
00:10:36,361 --> 00:10:37,781
that's just wasted.

136
00:10:37,822 --> 00:10:44,366
And the authors of that spark paper were actually my lab mates when I was at UC Berkeley.

137
00:10:45,467 --> 00:10:52,625
observation was pretty simple but very meaningful that hey, if you have a piece of data
that you have to do some iterative computation on it, let's keep it in memory, pin it in

138
00:10:52,625 --> 00:10:57,840
memory so you can finish those iterations and then we can write it back to disk.

139
00:10:58,765 --> 00:11:08,589
The idea is sound, it makes perfect sense, well motivated, very practical, but they had a
very hard time publishing that paper in academia because I remember the early feedbacks on

140
00:11:08,589 --> 00:11:12,144
the paper was like this idea is not novel enough.

141
00:11:12,144 --> 00:11:15,928
The keyword they use is novel enough, which means like it's too simple.

142
00:11:16,089 --> 00:11:18,649
Like can you add a twist to it?

143
00:11:18,649 --> 00:11:27,789
you you know as if like it's a it's a it's a movie right like you want to you want to you
don't want to be able to see the end of the ending like you know from from the beginning

144
00:11:27,789 --> 00:11:39,069
so that's that's the kind of mindset that's that's there and I think there's good and bad
to it as well like that's how people become more creative people learn how to take on open

145
00:11:39,069 --> 00:11:46,283
ended problems I think academia does a lot of things really well but there's certain areas
where I think closer partnerships

146
00:11:46,283 --> 00:12:00,073
with actual customers helps save a lot of smart brains from burning their calories on
problems that no one cares about or solutions that no one will actually ever adopt.

147
00:12:00,189 --> 00:12:05,573
So maybe to Jillian's point, what's the benefit of staying in academia?

148
00:12:11,693 --> 00:12:09,370
academia has certain things that you only get in academia.

149
00:12:09,370 --> 00:12:12,150
Like you have access to extremely smart talent.

150
00:12:12,550 --> 00:12:17,670
And like, you know, as they say, when you hang out with smart people, you also keep
getting smarter too, right?

151
00:12:17,670 --> 00:12:19,030
And there's some truth to it, right?

152
00:12:19,030 --> 00:12:19,292
Like

153
00:12:19,292 --> 00:12:24,716
When you're operating on venture capital, there's a very specific timeline.

154
00:12:24,716 --> 00:12:28,789
There's a certain amount of risk that's encouraged to take.

155
00:12:28,890 --> 00:12:31,912
But let's say that you're working on curing cancer.

156
00:12:32,513 --> 00:12:34,284
People have been working on for a long time.

157
00:12:34,284 --> 00:12:37,677
Incremental ideas will only lead to incremental results.

158
00:12:37,677 --> 00:12:43,242
So at some point, you need to take some risk.

159
00:12:43,242 --> 00:12:48,478
You need to explore solutions that are so crazy, there's a good chance they're not going
to pan out.

160
00:12:48,478 --> 00:12:53,118
And for doing that you need a little bit of patience.

161
00:12:53,598 --> 00:12:55,778
That's hard to find outside academia.

162
00:12:55,778 --> 00:13:07,518
You need highly motivated, highly smart individuals at the beginning of their career with
that intellectual freedom to go and venture out, find those problems, explore those crazy

163
00:13:07,518 --> 00:13:08,258
ideas.

164
00:13:08,258 --> 00:13:14,118
And for every 10 crazy ideas we try out, one of them is going to pan out and that's a
really good outcome for academia.

165
00:13:14,278 --> 00:13:17,298
In the industry if you go to your

166
00:13:17,534 --> 00:13:27,554
Backer to your board to your boss whoever that is and tell them hey I need you to give me
ten times more time because I want to try and change ten different crazy ideas It's a high

167
00:13:27,554 --> 00:13:37,254
risk high reward thing by the time you to your you know third iteration You're gonna
you're probably gonna be template that have some difficult performance conversation So

168
00:13:37,254 --> 00:13:46,914
there's a time and place for both right there if you if you're looking for really creative
really impactful ideas To give you a very concrete example like you know at keyboard

169
00:13:47,614 --> 00:13:52,774
which is the startup that I'm reading now, we're very successful.

170
00:13:52,774 --> 00:14:02,054
One of the main things that people love about our product is that it takes 30 minutes and
then within 30 minutes of investment from your side, the AI kicks in and starts optimizing

171
00:14:02,054 --> 00:14:11,854
your cloud data warehouse, for example, your Snowflake, and within 24 hours, you're seeing
on average a 25, 30 % cut to your overall Snowflake bill, which is very meaningful.

172
00:14:11,854 --> 00:14:16,215
We have organizations or customers who are spending millions of dollars on their
Snowflake.

173
00:14:16,215 --> 00:14:27,600
I definitely want to dive into that, but the duality is really interesting that you
brought up that in academia, having ideas that really have a business impact and maybe

174
00:14:27,600 --> 00:14:32,241
more than that have a world impact are not paid attention to as much.

175
00:14:32,241 --> 00:14:36,464
Like just do a little bit better than what you're doing and experiment a lot.

176
00:14:36,845 --> 00:14:40,826
Whereas in business, everything has to be immediately relevant.

177
00:14:41,026 --> 00:14:42,527
But on the flip side,

178
00:14:42,747 --> 00:14:47,549
That means that we aren't getting the time outside of academia to experiment effectively.

179
00:14:47,549 --> 00:15:00,175
That teams should actually be experimenting because they may find a way to drastically
increase the query speed or performance resource usage of their database clusters.

180
00:15:00,315 --> 00:15:06,678
on the flip end, so I think it's what you're saying is really both areas that are separate
need to learn from each other.

181
00:15:06,678 --> 00:15:12,531
More experimentation in the private space and in academia, more attention.

182
00:15:12,531 --> 00:15:22,130
to like what's relevant in the next you know one to ten years that has a business impact
that that you know where the industry is going what's relevant for them otherwise an idea

183
00:15:22,130 --> 00:15:26,594
is just really an idea and it's not going to get accepted into any conference talk.

184
00:15:27,545 --> 00:15:29,237
I think that's a good way of solving it up.

185
00:15:29,237 --> 00:15:32,440
think the kind of balance I found is very useful.

186
00:15:32,440 --> 00:15:36,994
It's like you find real world problems by definition in the industry.

187
00:15:37,202 --> 00:15:40,607
I mean, I think in the industry, actually have this counter.

188
00:15:40,680 --> 00:15:45,363
perspective, which now seems like it actually has a lot of paradoxical negatives.

189
00:15:45,363 --> 00:15:56,831
I hear very frequently hit the ground running, like setting up onboarding docs and tooling
and resources so that you can just get started on your first day working at a new

190
00:15:56,831 --> 00:15:58,933
organization and a new company.

191
00:15:58,933 --> 00:16:03,436
And you just already know how it's supposed to work and already start providing value.

192
00:16:03,436 --> 00:16:08,263
And now I'm getting the thought of like, well, actually spending time learning.

193
00:16:08,263 --> 00:16:18,530
the backwards way that an organization is working before you actually start delivering
value may be an opportunity that we've squandered in a desire to move quickly and get

194
00:16:18,530 --> 00:16:28,517
everyone on the same page as fast as possible, there's a much lower opportunity for
learning and I'd say failure, which I think a lot of people agree is a strategy that

195
00:16:28,517 --> 00:16:31,418
really drives future innovation.

196
00:16:32,176 --> 00:16:32,696
That's right.

197
00:16:32,696 --> 00:16:38,058
think another way to look at it is I think there's nothing wrong moving fast.

198
00:16:38,058 --> 00:16:38,729
That's the thing.

199
00:16:38,729 --> 00:16:41,320
That's my own model.

200
00:16:41,320 --> 00:16:46,912
Whether I'm working in an academic setting at Kibo, we want to move fast.

201
00:16:46,912 --> 00:16:50,843
But sometimes people have the perception of moving fast.

202
00:16:50,943 --> 00:16:52,684
Sometimes if you're a

203
00:16:53,244 --> 00:17:00,868
If you're building a house but you're not taking the time to really understand the
measurements and what you're doing and one side of the wall is shorter than the other

204
00:17:00,868 --> 00:17:04,449
side, you're not really fast because all that work is going to be throwaway work.

205
00:17:04,449 --> 00:17:10,102
So I think the right speed is actually failing fast because you can't know everything.

206
00:17:10,701 --> 00:17:12,072
I sort of want to go back.

207
00:17:12,072 --> 00:17:20,331
It's been mulling over my head about your AI agent that runs to reduce your snowflake
costs.

208
00:17:20,972 --> 00:17:22,303
How does this actually work?

209
00:17:22,303 --> 00:17:25,477
How does it just go in?

210
00:17:25,477 --> 00:17:27,999
Is it deduplicating data?

211
00:17:27,999 --> 00:17:31,763
Is it improving query search performance?

212
00:17:31,763 --> 00:17:33,805
Is there some other magic going on?

213
00:17:33,967 --> 00:17:43,512
So like one of the biggest things that's happened in our industry over the last, I would
say decade, decade and a half is like the rise of cloud databases or in particular, like,

214
00:17:43,512 --> 00:17:51,756
know, the success that the likes of Snowflake, BigQuery, Redshift and more recently
Databricks have seen.

215
00:17:51,756 --> 00:17:56,998
And if you think about what's happened there is that these cloud data warehouses

216
00:17:56,998 --> 00:18:00,859
the likes of Snowflake have really lowered that adoption barrier.

217
00:18:01,439 --> 00:18:13,333
So now it's significantly easier for anyone, any organization of any size, any team with
any level of skills to go spin up a cloud data warehouse and start analyzing that data,

218
00:18:13,333 --> 00:18:15,763
querying that data, getting that data very quickly.

219
00:18:15,763 --> 00:18:18,024
So that adoption barrier has gone down.

220
00:18:18,078 --> 00:18:30,184
But the byproduct of that, the side effect of that is that because it's so much easier to
leverage data, now you have more users with varying levels of database proficiency and

221
00:18:30,184 --> 00:18:32,115
skills writing queries.

222
00:18:32,115 --> 00:18:35,536
They're querying more data and they're combining more data sources.

223
00:18:35,536 --> 00:18:48,012
So as a result, I would argue that the data pipelines that organizations are dealing with
right now are an order, at least if not two orders of magnitude more complicated.

224
00:18:48,012 --> 00:18:51,024
than what we used to have 15 years ago.

225
00:18:51,104 --> 00:18:54,185
So for instance, if we just take Snowflake as an example,

226
00:18:54,185 --> 00:19:00,129
You know you have to pick a size for your warehouse right you have to decide for example
What's your partitioning key?

227
00:19:00,129 --> 00:19:08,195
I give it the side how long you want to keep this warehouse running after the query has
finished if I Shut it off right away.

228
00:19:08,195 --> 00:19:08,505
Well.

229
00:19:08,505 --> 00:19:19,202
I saved money I don't have to pay snowflake for just keeping an idle warehouse running
because it pays you go But then if the next query arrives and my warehouse is shot down

230
00:19:19,202 --> 00:19:23,865
Then I have to spin up a cold instance and now equated would have taken a couple of
seconds

231
00:19:23,865 --> 00:19:32,481
Otherwise now has to take maybe a couple of minutes because it has already top of course
stories like okay So what's the optimal time to shut down a warehouse and then this

232
00:19:32,481 --> 00:19:33,431
warehouse?

233
00:19:33,431 --> 00:19:35,072
You know, I bought most data teams.

234
00:19:35,072 --> 00:19:43,618
They say hey, I need a medium for my you know, BIA workload I need an enlarge for this but
do you really need a large warehouse 24-7 is your workload?

235
00:19:43,618 --> 00:19:53,695
Constantly steadily at a level where it warrants a large maybe sometimes you need an X
large Maybe it's actually cheaper to use an X large because you pay more per unit but then

236
00:19:53,695 --> 00:19:53,765
the

237
00:19:53,765 --> 00:19:58,325
query finishes in less than half the time that it would have otherwise.

238
00:19:58,325 --> 00:20:00,225
Maybe it's underutilized.

239
00:20:00,225 --> 00:20:12,005
Can you wake up your data team and send up, and can you page your DevOps team to go and
reduce the size of your medium warehouse at 2 a.m.

240
00:20:12,005 --> 00:20:17,805
to a small warehouse and after seven minutes wake them up again and say, the workload
increased again.

241
00:20:17,963 --> 00:20:19,576
go back to the default size.

242
00:20:19,576 --> 00:20:24,715
can do that, but you can actually train reinforcement learning models, for example, to do
that, right?

243
00:20:24,715 --> 00:20:26,276
So you just...

244
00:20:26,477 --> 00:20:27,601
you can do that.

245
00:20:27,601 --> 00:20:31,240
I feel like there's going to be a bunch of very unhappy people at the end of the day.

246
00:20:32,587 --> 00:20:40,786
Well, there are things that humans can do and there are things that humans want to do.

247
00:20:41,057 --> 00:20:41,267
Right?

248
00:20:41,267 --> 00:20:49,491
Like if you find the intersection of what is it that humans cannot do or don't want to do
and automate that, that's how you've empowered your data team.

249
00:20:49,491 --> 00:20:49,971
Right?

250
00:20:49,971 --> 00:21:01,136
Like no one, I've never met a data engineer who's told me, my dream is to wake up at 2
a.m., reduce the size of a warehouse for seven minutes and then go back to sleep.

251
00:21:01,136 --> 00:21:01,437
Right?

252
00:21:01,437 --> 00:21:09,400
Like I've never seen anyone who tells me, I wish I could just squint my eyes and look at 2
million queries and figure out which one should be routed to which warehouse.

253
00:21:09,400 --> 00:21:11,031
But the reinforcement learning

254
00:21:11,031 --> 00:21:12,591
agent is more than happy to do that.

255
00:21:12,591 --> 00:21:24,436
You just have to have the right reward function where you penalize the agent every time
that it causes a slowdown and you basically reward that agent every time that it manages

256
00:21:24,436 --> 00:21:33,248
to make some configuration changes or how to create the right warehouse that actually
saves money for that customer without actually impacting their performance.

257
00:21:33,283 --> 00:21:38,578
I know it's coming, but I'm still so freaked out by the idea of having these agents that
are just doing stuff.

258
00:21:38,578 --> 00:21:43,782
I mean, I guess it's not really any different from your case isn't like that different
from an auto scaler.

259
00:21:43,903 --> 00:21:49,397
And that's a known problem, but just in general, I just I'm not there yet.

260
00:21:49,397 --> 00:21:50,629
And I really like AI.

261
00:21:50,629 --> 00:21:52,931
I think like I'm all I'm all about the AI over here.

262
00:21:52,931 --> 00:21:53,181
Right.

263
00:21:53,181 --> 00:21:54,814
But like, just yeah.

264
00:21:54,814 --> 00:21:56,230
spot on,

265
00:21:56,230 --> 00:22:08,528
So one of the things that I identified early on with the AI hype bandwagon is I think a
lot of companies were using AI on their marketing pages as a proxy for, I don't actually

266
00:22:08,528 --> 00:22:12,361
know how to talk about the value our product delivers.

267
00:22:12,361 --> 00:22:19,856
I'm just gonna put these two letters on there and pretend that that means something to
someone and they'll.

268
00:22:19,856 --> 00:22:22,738
bring their own ideas about how that could be valuable.

269
00:22:22,738 --> 00:22:27,771
And I think before that, we saw similar things happen in the past.

270
00:22:27,771 --> 00:22:32,184
I think just the speed and the velocity of change that's happened.

271
00:22:32,292 --> 00:22:43,827
for the AI cycle has been so fast that it's really easy to see from innovation, like
innovation hitting the market, not like outside of academia, because we all know AI has

272
00:22:43,827 --> 00:22:47,599
been around for much longer than just the five years now.

273
00:22:47,679 --> 00:22:59,215
You know, we're going back 20, 30, 40 years where there's lots of papers out there, but in
business, realistically, and we can actually see the change from innovation all the way to

274
00:22:59,215 --> 00:22:59,965
exploitation.

275
00:22:59,965 --> 00:23:02,326
And I still think that we, the same number of companies

276
00:23:02,326 --> 00:23:11,335
startups or even big, giant Fortune 50 companies that honestly have no idea of how to
actually convey their value effectively.

277
00:23:12,461 --> 00:23:14,172
That's fair and I think that's a big challenge.

278
00:23:14,172 --> 00:23:27,928
I think it's it goes both ways right you've got at the top of the food chain right like
CIOs and CTOs who hear these buzzwords and They feel like we have to do something about

279
00:23:27,928 --> 00:23:28,038
it.

280
00:23:28,038 --> 00:23:29,468
The board is asking about it.

281
00:23:29,468 --> 00:23:40,393
I'd like you know that we got to do something about it and then and the other side of the
equation you've got sometimes I sees who are worried about like

282
00:23:40,429 --> 00:23:51,609
their jobs like hey if we you know, adopt this thing then what's gonna happen to my job
and like my reaction to that usually is like if you worry that AI is gonna take away your

283
00:23:51,609 --> 00:23:55,162
job it's probably going to to take away your job

284
00:23:55,501 --> 00:24:03,115
like a lot of the machine learning experts that come out of academia don't have the
faintest idea of how to deploy something to real world.

285
00:24:03,115 --> 00:24:14,040
So like you need these engineers who can understand high level concept and you can partner
them closely with your machine learning researchers and experts to sort of build stuff

286
00:24:14,040 --> 00:24:23,674
that can actually get deployed, get trained at large scale and get trained and have the
right level of robustness and reliability.

287
00:24:23,674 --> 00:24:25,435
So there's a lot of things that people

288
00:24:25,435 --> 00:24:26,665
can do to protect their jobs.

289
00:24:26,665 --> 00:24:33,378
Just go take an online class and brush up on your stats class and take a machine learning
course.

290
00:24:33,378 --> 00:24:35,198
Try out the few tools that are out there.

291
00:24:35,198 --> 00:24:49,163
One of the biggest anti-patterns I'm seeing these days is, which I think has plagued the
software industry on the consumer side of things, is this unreasonable urge for build

292
00:24:49,163 --> 00:24:52,504
versus buy.

293
00:24:52,504 --> 00:24:54,805
And I think significant amount of...

294
00:24:54,925 --> 00:25:03,848
engineering cycles are getting wasted by people giving in to their own natural instinct
of, I just want to build everything in-house.

295
00:25:03,848 --> 00:25:13,852
And you'll be surprised, like very few CIOs and leaders are able to sort of tell what's
the right time, what's the right thing to build versus buy.

296
00:25:13,852 --> 00:25:16,473
And I see people get that wrong all the time.

297
00:25:16,920 --> 00:25:20,808
I liked your call out here on.

298
00:25:20,892 --> 00:25:25,956
where you should be concerned and how to train yourself or grow further.

299
00:25:25,956 --> 00:25:36,624
I mean, the idea that if you fixate on the fact that your job is gonna go away, then it
probably is, actually really reminisces for me a concept from, of all places, Hawaiian

300
00:25:36,624 --> 00:25:42,068
shamanism, which is like, if you fixate on this thing, you are actually bringing it into
reality.

301
00:25:42,068 --> 00:25:44,630
You are making it the case.

302
00:25:45,471 --> 00:25:45,671
do really think that...

303
00:25:46,035 --> 00:25:48,337
Manifesting, yeah, for sure.

304
00:25:48,638 --> 00:25:50,770
So I do think that there is a lot there.

305
00:25:50,770 --> 00:25:58,591
Like if you want to be, like you can figure out what your job should be and what you want
to be an expert in and how to achieve that.

306
00:25:58,591 --> 00:26:04,354
And maybe it's not a fit for your current company, but for sure, if you just worry
about...

307
00:26:05,355 --> 00:26:13,697
the fact your job may or may not be going away, there's definitely an aspect of, and this
is something that I've picked up recently and I've been trying to live by.

308
00:26:13,697 --> 00:26:25,081
It's not necessarily the easiest thing, but I think it's ancient Confucius wisdom here
that if you worry about the future, then you cry twice.

309
00:26:25,461 --> 00:26:26,631
You feel the pain twice.

310
00:26:26,631 --> 00:26:32,803
There's something you can do about it right now, and rather than worry about a future that
probably won't even come.

311
00:26:33,215 --> 00:26:34,440
do that thing.

312
00:26:34,505 --> 00:26:36,916
And if it does come, then you're at least prepared.

313
00:26:38,100 --> 00:26:38,860
No, 100%.

314
00:26:38,860 --> 00:26:40,596
No, 100 % agree.

315
00:26:42,111 --> 00:26:45,973
I'm sort of curious about the verticals that you see.

316
00:26:45,973 --> 00:26:49,260
I mean, we talk about data-intensive systems a lot.

317
00:26:49,260 --> 00:26:51,523
And what falls into that category?

318
00:26:51,523 --> 00:26:52,765
Concrete things.

319
00:26:52,765 --> 00:26:54,286
Yeah, what kind of data?

320
00:26:56,052 --> 00:27:03,172
One of the interesting things that again has happened with the rise of, I mean there's a
reason why Snowflake had one of the largest software IPO ever.

321
00:27:03,172 --> 00:27:18,092
One of the things that this new breed of technology has actually, one of the changes that
it's made in the way that data is being consumed is that it's become number one size

322
00:27:18,092 --> 00:27:19,632
agnostic.

323
00:27:19,632 --> 00:27:22,692
Like back in the day if you had a bigger company you had more data.

324
00:27:23,112 --> 00:27:25,992
And if you were a smaller company you probably had small data.

325
00:27:26,216 --> 00:27:37,007
And then there were certain industries that were like, know, tech was known to be like,
you know, much more data savvy than for example, government or, you know, healthcare was a

326
00:27:37,007 --> 00:27:39,140
lot more protective of their data.

327
00:27:39,140 --> 00:27:44,515
And, you know, there was certain segments or sectors of the industry that were more data
driven.

328
00:27:44,515 --> 00:27:49,400
I think what we're seeing is that it's penetrating everywhere.

329
00:27:49,522 --> 00:27:58,399
Like I was talking to a local government in one of the states where you wouldn't think
they would be looking at snowflakes and they're like, no, no, we gotta get on that.

330
00:27:59,048 --> 00:28:02,951
We're going to get that cloud data warehouse for these five reasons.

331
00:28:02,951 --> 00:28:05,623
is what we're trying to do.

332
00:28:05,623 --> 00:28:07,415
was like, do you guys even have the budgets?

333
00:28:07,415 --> 00:28:08,916
That's irrelevant.

334
00:28:08,916 --> 00:28:10,997
We've got to do it for these reasons.

335
00:28:10,997 --> 00:28:13,829
then, so that's from a sector perspective.

336
00:28:13,829 --> 00:28:20,083
But the other thing, which I think is even more interesting, even from a sales and
go-to-market perspective, is that...

337
00:28:20,916 --> 00:28:27,956
You have no idea how much a customer is going to spend on their data infrastructure by
looking at the size of that company.

338
00:28:27,956 --> 00:28:36,956
Kibo has customers who are spending north of 15 and more million dollars here just on
their snowflake bill.

339
00:28:37,676 --> 00:28:39,536
And they're a tiny company.

340
00:30:08,274 --> 00:28:41,519
That's a lot of complexity right there.

341
00:28:41,519 --> 00:28:45,142
Companies should not have to deal with this with their own resources.

342
00:28:45,142 --> 00:28:49,777
If you're a bank, you gotta focus on what's making you a differentiated bank.

343
00:28:49,777 --> 00:28:52,879
If you're a marketing company, you have to focus on your core business.

344
00:28:52,879 --> 00:28:58,164
You shouldn't be in the business of building and optimizing your own data infrastructure.

345
00:28:58,164 --> 00:29:01,098
You gotta automate that part too.

346
00:29:01,098 --> 00:29:13,058
I think part of the problem here is I think it's sourced from humanity, this idea that
growth equals good and that your total addressable market can actually increase in size

347
00:29:13,058 --> 00:29:15,170
over time and you can make it happen.

348
00:29:15,170 --> 00:29:21,386
And these companies are lacking ways of growing still just a little bit bigger.

349
00:29:21,386 --> 00:29:24,178
And so they're spending a non-trivial amount of money.

350
00:29:24,282 --> 00:29:34,813
pulling in almost nonsensical data nonsensical sources things that aren't so relevant in
order to even increase their market share by percent like

351
00:29:35,034 --> 00:29:39,456
pips, know, hundreds of percentage points because that's all they can do.

352
00:29:39,536 --> 00:29:46,499
But once if you realize that your market is only so big and that's it, you know where you
should optimize for and potentially just stop there.

353
00:29:46,499 --> 00:29:55,624
Focus on cost reduction, on optimizations and what you're doing rather than trying to add
yet another product or another feature or service in a way that doesn't really add

354
00:29:55,624 --> 00:29:58,785
fundamental value to to your users.

355
00:29:58,785 --> 00:30:02,037
You actually opened the you stepped in this and you opened the door.

356
00:30:02,037 --> 00:30:03,547
And I want to ask you about this.

357
00:30:03,547 --> 00:30:05,148
I feel like since

358
00:30:05,148 --> 00:30:18,164
the exploitation of LLMs and the data that we have that's been created since the internet
was conceived as an idea, we're losing public access data, like the data sets that are

359
00:30:18,164 --> 00:30:24,457
available just from scraping individual websites or just freely available, I think is
actually decreasing.

360
00:30:24,457 --> 00:30:33,281
I dare say that the end of the internet has come, or it's on its way, that connectivity is
no longer what we're optimizing for.

361
00:30:34,126 --> 00:30:37,068
I'm wondering where you see this going.

362
00:30:37,575 --> 00:30:40,067
I think people are going to move on.

363
00:30:40,067 --> 00:30:46,751
It's hard to predict the future, but it's also easy because no one's going to remember to
come back and hold you accountable for misprediction.

364
00:30:46,751 --> 00:30:47,922
So I call that out.

365
00:30:47,922 --> 00:30:56,257
But I think if we're within the next decade, I usually have a...

366
00:30:56,745 --> 00:31:06,845
The thing is if you spend too much time in any particular area, you can see things that
are pretty obvious to you, but maybe they sound weird to others who have not been

367
00:31:06,845 --> 00:31:07,905
following that thing.

368
00:31:07,905 --> 00:31:16,045
For a of things that might be a surprise to others, for example, the success that ChatGPT
has was a surprise to a lot of others, but not to those who were tracking the progress

369
00:31:16,045 --> 00:31:17,205
over the years.

370
00:31:17,205 --> 00:31:24,925
So I think in terms of data and selling data as an asset, I think we're actually already
moving past that.

371
00:31:24,925 --> 00:31:26,699
Now people are selling agents

372
00:31:26,699 --> 00:31:28,650
trained on that data.

373
00:31:28,650 --> 00:31:40,738
There is a reason why there is all this excitement about, you guys have seen the news
about DeepSeek and what it means for the use of GPUs and the investments that companies

374
00:31:40,738 --> 00:31:42,229
like OpenAI have done.

375
00:31:42,229 --> 00:31:44,081
But the bottom line is that there's an arms race.

376
00:31:44,081 --> 00:31:55,238
You basically train these AI agents instead of having companies just go and purchase this
data and then clean the data and then combine the data and then build apps on it and then

377
00:31:55,238 --> 00:31:56,659
monetize it and then maintain

378
00:31:56,659 --> 00:31:59,666
and tune it like you just buy these agents.

379
00:31:59,666 --> 00:32:06,259
I think we're past selling data and we're at the place where we're selling agents that are
already trained and ready to be deployed.

380
00:32:07,862 --> 00:32:12,655
If the data goes private though, no new agents are going to be able to be spun up.

381
00:32:12,655 --> 00:32:19,018
you from that standpoint, we're at the road's end of where the AI innovation can take us.

382
00:32:19,058 --> 00:32:30,355
Like, I feel like fundamentally in order to keep evolving and innovating, we still need
new fresh sources of data with combining all of the humanities collection so far in order

383
00:32:30,355 --> 00:32:34,407
to actually train on all of it and get the most effective agent being built.

384
00:32:34,407 --> 00:32:36,268
Or is I missing something here?

385
00:32:36,361 --> 00:32:43,921
I mean in theory, but if you think about it the majority of the humanity data is actually
held by minority of humanity, right?

386
00:32:43,921 --> 00:32:46,001
There's like two, three big players.

387
00:32:46,001 --> 00:32:52,221
Like I mean, that's the almost sad part of like how consolidation has been working.

388
00:32:52,221 --> 00:33:01,575
Like, you you have two or three major providers who are seeing and recording and
monitoring 99 % of aspects of your life.

389
00:33:01,575 --> 00:33:04,069
Stack Overflow, like what's the third one?

390
00:33:04,069 --> 00:33:15,774
I don't even to talk for coding but like I made to think about like, know Google doesn't
need me to send them a copy of my hard drive like they see my emails they see basically my

391
00:33:15,774 --> 00:33:26,219
you know usage pattern on my Android they see like, you know the They're the content that
I'm consuming they see the books that I'm basically searching for Amazon knows the items

392
00:33:26,219 --> 00:33:31,581
I'm buying they're looking at every book that I'm reading like they have a lot of this
data

393
00:33:31,699 --> 00:33:39,255
that, you know, at least in the US, like I can't speak for Europe, I think they have much
better laws when comes to privacy protection.

394
00:33:39,256 --> 00:33:44,170
You don't even think twice about, you know, clicking and saying, I agree to these terms of
use.

395
00:33:44,170 --> 00:33:46,332
And I think they have the majority of that data.

396
00:33:46,332 --> 00:33:52,166
Like, will we be better off if everyone shares everything and then, you know, we build
this stuff?

397
00:33:52,787 --> 00:33:53,238
I don't know.

398
00:33:53,238 --> 00:33:58,192
I think it easily gets into the area of security and privacy, which I don't know anything
about.

399
00:33:58,192 --> 00:33:59,312
But I think...

400
00:33:59,817 --> 00:34:02,617
If that was not a concern, probably the answer would be yes.

401
00:34:02,617 --> 00:34:04,397
But I know that is a concern.

402
00:34:04,397 --> 00:34:09,157
But I also know that there's very few players who have already plenty of data.

403
00:34:09,157 --> 00:34:12,657
mean, OpenAI has the data that they're actually scaping.

404
00:34:12,657 --> 00:34:14,317
But is it going to plateau?

405
00:34:14,777 --> 00:34:18,317
Probably there's going to be, I think these things are going to become a commodity.

406
00:34:18,317 --> 00:34:20,517
These agents will become a commodity.

407
00:34:20,517 --> 00:34:22,237
The arms race will not continue.

408
00:34:22,237 --> 00:34:24,677
And then we'll move on to the next thing after that.

409
00:34:25,072 --> 00:34:26,592
That's an interesting point.

410
00:34:26,872 --> 00:34:39,812
you all like there there's this idea in biology where you just need a limited set of
unique individuals in order to propagate the species without too many mutations of which

411
00:34:39,812 --> 00:34:42,612
it will collapse under inbreeding basically.

412
00:34:42,724 --> 00:34:52,675
Like maybe there is some set of data that we only need that much in order to uniquely be
able to create even the best trained agents that we possibly can.

413
00:34:52,675 --> 00:34:54,657
Additional data won't help us in that way.

414
00:34:54,657 --> 00:34:56,208
And maybe we've gotten that.

415
00:34:56,208 --> 00:34:57,273
Maybe we'll get it through.

416
00:34:57,273 --> 00:35:00,316
the crux of learning theory, right?

417
00:35:00,316 --> 00:35:05,570
That basically the error will go down, you one over N when N is the size of your data set,
right?

418
00:35:05,570 --> 00:35:14,006
So that basically means more data at some point is not going to significantly reduce, more
training data will not significantly reduce your error.

419
00:35:14,067 --> 00:35:17,289
Obviously that depends on the sparsity of the data, you know.

420
00:35:17,309 --> 00:35:19,710
the whole idea behind VC Dimension and what not.

421
00:35:19,710 --> 00:35:30,505
But the main idea is this, I know we're not really good at particularly predicting
election outcomes, but the idea of these election surveys is exactly the same thing, that

422
00:35:30,505 --> 00:35:40,249
you don't need to go and ask every one of the 300 million voters, if you have a sample
that's large enough past that, you're not going to significantly increase the accuracy.

423
00:35:40,249 --> 00:35:44,214
I think that's definitely true, that there's a diminishing return.

424
00:35:44,214 --> 00:35:54,668
Yeah, yeah, I definitely agree with you guys on the, like, you can keep adding more data
and that doesn't necessarily make it better, but we're also always getting new data, like,

425
00:35:54,668 --> 00:36:02,262
we're always producing, like, new and different data, and we need the new and different
data too, so I work with, like, a lot of medical data and we're kind of constantly

426
00:36:02,262 --> 00:36:03,282
changing.

427
00:36:03,654 --> 00:36:13,437
just everything, the resolution that we can see the data at, the amount, just more
insights, more everything.

428
00:36:13,437 --> 00:36:14,155
So I don't know.

429
00:36:14,155 --> 00:36:21,009
I have very mixed feelings about this because I've definitely been on projects where
somebody's been pushing more like, well, just make it better.

430
00:36:21,009 --> 00:36:22,199
Can't you just add more data?

431
00:36:22,199 --> 00:36:25,090
And I'm like, no, you see the last three data sets that we added to train it.

432
00:36:25,090 --> 00:36:26,260
They didn't actually do anything.

433
00:36:26,260 --> 00:36:27,501
Like here's the graph.

434
00:36:27,501 --> 00:36:28,551
And then they're like more data.

435
00:36:28,551 --> 00:36:29,801
And I'm like, well, you're my boss.

436
00:36:29,801 --> 00:36:31,802
So like, OK, but this is silly.

437
00:36:32,676 --> 00:36:39,126
I mean, I'm with you and I also think that the medical industry at Vertical is actually
more unique in this way.

438
00:36:39,126 --> 00:36:42,894
I think our lack of full understanding of...

439
00:36:42,894 --> 00:36:53,889
even our human bodies, but organic material organisms in general, means that we could
benefit from having more data there realistically.

440
00:36:53,889 --> 00:36:57,781
And I feel like there's so many things that we haven't figured out there.

441
00:36:57,781 --> 00:36:59,862
The other verticals, I question a lot.

442
00:36:59,862 --> 00:37:06,425
Like I've worked, think at five different companies now in total, separate from all of the
consulting that I've done and advising.

443
00:37:06,425 --> 00:37:09,016
And all of them were like, our data is precious.

444
00:37:09,016 --> 00:37:10,167
We must save all of it.

445
00:37:10,167 --> 00:37:12,928
And I'm like, you don't need that data from 10 years ago.

446
00:37:12,928 --> 00:37:20,030
where you were measuring the deviation on vibration tests of this one product that you
don't even manufacture anymore.

447
00:37:20,030 --> 00:37:23,271
Like, I assure you, you can throw that away.

448
00:37:23,751 --> 00:37:25,552
It's not going to help you.

449
00:37:25,612 --> 00:37:27,782
And yet they're like, we gotta keep it.

450
00:37:27,782 --> 00:37:31,944
I'm like, okay, AWS, you know, Glacier.

451
00:37:31,944 --> 00:37:33,360
Yeah, dude.

452
00:37:33,360 --> 00:37:36,400
Well, so is it cheap, But I mean, you're right to your point.

453
00:37:36,400 --> 00:37:37,120
I medical data.

454
00:37:37,120 --> 00:37:49,620
I remember I was working with one of my colleagues from the med school and we were trying
to predict, he was a cardiologist and we were trying to predict.

455
00:37:49,670 --> 00:38:02,331
trained models that predict the chances of an organ after a, I forgot the medical term,
when they basically do an organ transfer, the body, the host body, there's a chance they

456
00:38:02,331 --> 00:38:08,036
might reject that organ and they use antibiotics and whatever to suppress the immunity
system and whatnot.

457
00:38:08,036 --> 00:38:09,957
And there's complications, all of that.

458
00:38:09,957 --> 00:38:13,980
And the idea was to predict the risk of an organ rejection.

459
00:38:14,922 --> 00:38:15,199
It's

460
00:38:15,199 --> 00:38:15,977
Transplant.

461
00:38:15,977 --> 00:38:18,219
Yeah, transplant, I think that's a medical term for it.

462
00:38:18,219 --> 00:38:25,144
But I remember, you know, the saying like, you know, at Umich we have one of the largest
cancer data sets on the planet.

463
00:38:25,144 --> 00:38:32,693
And then when we looked at it, it was a number that like, I forgot the exact number, but
it was something like close to 300.

464
00:38:32,693 --> 00:38:35,733
And I was like, how is this the largest data set on planet?

465
00:38:35,733 --> 00:38:44,393
It's just like medical data by definition is way more sparse because there's, you know,
there's only whatever, six billion or seven billion cap on how many you can collect.

466
00:38:44,393 --> 00:38:48,912
And for any particular disease, there's a very small subset of them you have access to.

467
00:38:48,912 --> 00:38:55,053
So I don't think the laws of large numbers do apply to anything that's about, I mean, with
DNA and stuff, that's different.

468
00:38:55,053 --> 00:38:59,353
But like, you know, when we talk about individual humans as data points, I agree.

469
00:38:59,353 --> 00:39:00,913
I think that's, that's probably an exception.

470
00:39:00,913 --> 00:39:02,677
I don't think we're at the place where

471
00:39:02,677 --> 00:39:04,055
We don't need more data.

472
00:39:04,253 --> 00:39:04,857
I mean, we...

473
00:39:04,857 --> 00:39:13,403
we need we need the data science companies to just go sit off in like a corner for this
conversation when we're talking about like, you know, building agents off of data and how

474
00:39:13,403 --> 00:39:14,493
much data should we have?

475
00:39:14,493 --> 00:39:15,714
And when do we stop?

476
00:39:15,714 --> 00:39:21,608
Because like medical data climatology, I don't I don't think the answer is ever or not
right now.

477
00:39:21,608 --> 00:39:23,564
Anyway, it's not any that I can see.

478
00:39:23,564 --> 00:39:24,554
Yeah, I mean, I'm with you.

479
00:39:24,554 --> 00:39:28,076
think the problem in the medical field though is that it's not public.

480
00:39:28,076 --> 00:39:31,868
feel like the climate data and tracking, like there's a lot out there.

481
00:39:31,868 --> 00:39:40,563
Whereas in the medical field, like that's controlled by private entities who are bound by
local regulations on even sharing that, which is in a ridiculous way.

482
00:39:40,563 --> 00:39:44,835
And the data, there are companies out there that do...

483
00:39:45,285 --> 00:39:49,597
anonymized data exchange in the medical field specifically to sort of help overcome this
problem.

484
00:39:49,597 --> 00:39:52,529
And you know, like there's no not a benefit for the patients.

485
00:39:52,529 --> 00:39:56,031
There's not a benefit for the providers, for the for the government.

486
00:39:56,031 --> 00:40:03,325
Like there's there's very little benefit here unless except the end company who may be
able to use all this for for the good of humanity.

487
00:40:03,325 --> 00:40:08,247
And that's a hard sell, I think, when there's dollars on the table on the other side.

488
00:40:08,751 --> 00:40:12,837
A lot of medical data is supposed to, like if it's used for research, it's supposed to be
public.

489
00:40:12,837 --> 00:40:22,011
I mean, it's not always, or it's maybe not in like organized in such a way that it's even
usable or like there's a lot that can go wrong with that.

490
00:40:22,011 --> 00:40:24,794
there is supposed to be a lot of medical data that's public.

491
00:40:25,207 --> 00:40:33,563
think part of it is legacy systems that aren't optimized for even storing the data in an
electric medical record format.

492
00:40:33,563 --> 00:40:41,798
Like if it's not electronic, you end up, before we were talking about hallucinations in
the world, which is, you know, still something AI focuses, we had the giraffe problem

493
00:40:41,918 --> 00:40:42,999
where

494
00:40:43,115 --> 00:40:59,078
Looking at an image from a medical document would likely render positive on whatever the
diagnosis is that you were trying to track just from the existence of Ruler or the way

495
00:40:59,078 --> 00:41:06,655
that because it was an x-ray or things that had nothing to do with the actual information
that was contained in the document so

496
00:41:06,717 --> 00:41:09,885
I don't know, I'm with you more data in medical field for sure.

497
00:41:09,885 --> 00:41:12,169
Anyone who's working on that, don't stop.

498
00:41:12,169 --> 00:41:13,130
party.

499
00:41:14,943 --> 00:41:16,683
always more data.

500
00:41:17,643 --> 00:41:20,763
Yeah, I don't know, storing biological data is like such a problem

501
00:41:20,883 --> 00:41:27,431
And they told me like, yeah, AWS basically made our business model obsolete because we're
trying to save money.

502
00:41:27,431 --> 00:41:30,394
Although this is a lot of hearsay, so I'm not sure that I should be repeating this.

503
00:41:30,394 --> 00:41:41,387
But anyways, it did seem like they had something where they had agents or AI running
around in the background to try to cut down on costs, and it was not well received.

504
00:41:41,675 --> 00:41:52,121
I mean, if you build something on a hyperscaler, there is a chance that they will find a
way to recapture that value and claim it for themselves.

505
00:41:52,121 --> 00:42:00,786
Like if every one of your customers needs to do something, it benefits everyone to bring
that value that you can deliver back into the platform.

506
00:42:00,907 --> 00:42:08,170
I know AWS is actually pretty good about doing that rather than forcing everyone to use a
third party company to achieve the same.

507
00:42:08,651 --> 00:42:09,071
benefit.

508
00:42:09,071 --> 00:42:15,495
Like, you know, it's surprising to me that companies like Snowflake or Databricks, and
there's a couple other ones out there.

509
00:42:15,495 --> 00:42:17,596
think Datadog is another good example.

510
00:42:18,196 --> 00:42:23,315
There's companies that just sit around and help customers spend less money on these
platforms.

511
00:42:23,315 --> 00:42:35,720
And if that was me, like if I'm Snowflake or a data dog, I'm just like, okay, I think it
was like Coinbase was spending almost $100 million a year on just data analytics coming

512
00:42:35,720 --> 00:42:36,371
from their platform.

513
00:42:36,371 --> 00:42:38,941
And they're not, they weren't very big when this got reported.

514
00:42:39,261 --> 00:42:42,843
And then they're like, we're going to have to do something about this because that's
apparently too much money.

515
00:42:42,843 --> 00:42:45,304
And that is a lot of money to be spending on it.

516
00:42:45,604 --> 00:42:52,003
It's just, it's just a bit ridiculous because if you know lots of customers have this
problem, like you would think that lowering the price point.

517
00:42:52,003 --> 00:42:57,543
in some way, not by changing your pricing, but figure by doing those optimizations helps
all of your customers in some way.

518
00:42:57,543 --> 00:43:03,623
Otherwise, they're just like, otherwise they're going to pay a third party company to help
them do the same thing anyway.

519
00:43:03,783 --> 00:43:15,303
So I think over time, as you get more and more customers who all have similar problems,
they have no choice but to bring that effort in house, either by buying a company that is

520
00:43:15,303 --> 00:43:19,863
doing that for them or spinning up their own internal version of it to optimize.

521
00:43:21,331 --> 00:43:24,574
I think there's two parts to it.

522
00:43:24,574 --> 00:43:30,298
One of it is like why would a big vendor invest in reducing their own revenue?

523
00:43:30,298 --> 00:43:34,622
Snowflakes, stock price is a function of their revenue, right?

524
00:43:34,622 --> 00:43:40,767
And if they want to reduce their own profit margin or actively be in the business of
reducing their own revenue.

525
00:43:40,859 --> 00:43:45,301
I think that that will not go very well with the shareholders.

526
00:43:45,301 --> 00:43:48,723
the other thing, but the other part is like focus, right?

527
00:43:48,723 --> 00:43:55,036
Like, you know, as a, as a vendor, you always have to protect the main body of your
revenue.

528
00:43:55,036 --> 00:43:56,572
Like this is like the innovator's dilemma.

529
00:43:56,572 --> 00:44:01,969
Like, like then you have to, like, you can't work on niche opportunities.

530
00:44:01,969 --> 00:44:05,310
Like your job is to build a database that anyone on the planet can use.

531
00:44:05,310 --> 00:44:05,911
Right.

532
00:44:05,911 --> 00:44:10,853
Now what's going to optimize this kind of workload might be different than what's going to
optimize.

533
00:44:10,853 --> 00:44:19,590
other customers particular use case and that's where I startups excel a lot but I think
they also realize that if they

534
00:44:20,351 --> 00:44:22,822
There is a reason why like we're partners with snowflake.

535
00:44:22,822 --> 00:44:33,397
There's a reason is because there is a reason for this like they see value in us serving
their customers almost in a unpaid customer success capacity.

536
00:44:33,397 --> 00:44:41,341
I call ourselves I sometimes joke that Kibo is snowflakes unpaid customer success
department because like we prevent their customers from turning right like at the end of

537
00:44:41,341 --> 00:44:50,165
the day if I'm spending a lot of money and I'm not able to get all my use cases on board
and I'm under pressure and the CFO is yelling at me I'm

538
00:44:50,165 --> 00:44:51,590
to look outside.

539
00:44:51,590 --> 00:44:53,191
yep, yeah, I think that's the biggest problem.

540
00:44:53,191 --> 00:45:02,396
If you look at the brand of a large data company or even any large company, you have to
look out at multiple, multiple years and you're absolutely right.

541
00:45:02,652 --> 00:45:14,678
the value that you're providing them as part of the Snowflake network is higher than the
amount that it would cost them to maintain that same piece of functionality internally or

542
00:45:14,678 --> 00:45:21,561
the amount of revenue that they would lose if say all their customers had access to that
functionality just straight away or it was automated in some way.

543
00:45:21,561 --> 00:45:29,345
I mean if you look at that equation then realistically you know how you want the network
to be you want everyone to be happy in a way and so if what makes them happy is that

544
00:45:29,345 --> 00:45:32,526
there's little startups out there that are helping them reduce their bills

545
00:45:32,526 --> 00:45:35,947
a little bit, then you let that be the case.

546
00:45:36,207 --> 00:45:44,367
mean, the economics obviously change at larger scale when all of your customers have this
problem or they're all unhappy because of how it's going.

547
00:45:44,367 --> 00:45:57,218
I think when it comes to software design one of the things I've sort of recently seen it
explained very well sometimes technical people like to have a lot of knobs because you

548
00:45:57,218 --> 00:46:07,206
know we usually think more flexibility means more options means better adoption and all of
that stuff I think one of the things we've learned

549
00:46:07,614 --> 00:46:18,690
the hardware is that actually the fewer choices you give people the more likely they'll
make the you'll get adoption right there's a but this week I you know I read this

550
00:46:18,690 --> 00:46:30,948
somewhere and it was summarized pretty well I think apparently there was a very successful
shoe salesman in LA back in the 50s and and they interviewed him and asked him like what's

551
00:46:30,948 --> 00:46:34,629
your secret and and he said

552
00:46:35,302 --> 00:46:38,422
My secret is the law of two, not three.

553
00:46:38,422 --> 00:46:39,962
And they ask them what you mean by that.

554
00:46:39,962 --> 00:46:46,242
It's like whenever a customer asks me to bring down the shoes that they can try, and then
they ask for a second one, I give them to them.

555
00:46:46,242 --> 00:46:49,062
I give those shoes to the customer as well.

556
00:46:49,062 --> 00:46:55,202
But if they ask for a third pair, then I tell them, which of these two would you like me
to put away?

557
00:46:55,462 --> 00:47:02,902
And the reason is they figured out that when they give customers two choices, if they give
customers three choices or more, they're likely to buy none.

558
00:47:02,902 --> 00:47:04,902
But when they give them two choices,

559
00:47:05,296 --> 00:47:06,788
they're likely to pick one.

560
00:47:06,788 --> 00:47:12,921
And I think that actually applies in some really profound ways to software design and AI
adoption.

561
00:47:12,921 --> 00:47:14,061
No, I think that's a really great point.

562
00:47:14,061 --> 00:47:23,881
I think it's really interesting perspective there, which goes in the direction of
developer experience and user experience for not just selling the product, but making sure

563
00:47:23,881 --> 00:47:25,901
people actually understand what they're doing.

564
00:47:27,289 --> 00:47:34,671
there is an aspect of decision paralysis there that really drives into what people are
going to do or how they're going to use the tool effectively.

565
00:47:37,571 --> 00:47:40,864
Okay, well then, Jillian, should we move on to PICS?

566
00:47:40,864 --> 00:47:41,692
Sure.

567
00:47:42,541 --> 00:47:44,418
What do you got for us today?

568
00:47:44,896 --> 00:47:46,958
I'm gonna pick Infinity Nikki.

569
00:47:46,958 --> 00:47:52,041
It's a video game and it's just this open world game where you just like, it's...

570
00:47:52,462 --> 00:47:58,906
you just run around and you just try on pretty dresses and it's nice and it has very
satisfying mechanics of jumping off buildings.

571
00:47:58,906 --> 00:47:59,436
That's it!

572
00:47:59,436 --> 00:48:00,247
That's the game.

573
00:48:00,247 --> 00:48:07,572
I think there is actually more that you could do in the game but there's not more that I'm
going to do in the game so that's like the extent of my knowledge.

574
00:48:07,757 --> 00:48:13,468
I think what everyone needs an answer to is how much AI is in the game.

575
00:48:13,758 --> 00:48:14,795
I don't know.

576
00:48:16,638 --> 00:48:17,069
I don't know.

577
00:48:17,069 --> 00:48:19,177
mean, think it's all procedurally generated.

578
00:48:19,177 --> 00:48:21,571
I don't think it has any AI anything.

579
00:48:21,571 --> 00:48:27,349
So you're saying is there's some future DLC for the game studio that's coming.

580
00:48:27,349 --> 00:48:37,780
I have been wondering if video games are gonna start to make NPCs, if they'll just have
them, just have agents and, or not agents, but those will all be AI, so then you could, I

581
00:48:37,780 --> 00:48:41,423
don't know what, ask it for a cake recipe or something.

582
00:48:42,064 --> 00:48:50,152
that does seem like some low-hanging fruit for the video game industry is to just do that,
but I don't know if it would be cost-effective rather than just probably not a script.

583
00:48:50,200 --> 00:48:56,182
was a you know, the video game industry notoriously super high margins and lots of extra
capital to spend.

584
00:48:56,182 --> 00:48:56,903
You

585
00:48:57,871 --> 00:48:58,872
they do.

586
00:48:58,872 --> 00:49:02,146
I don't know, so I don't see it happening there, but maybe it'll come up someplace else.

587
00:49:02,146 --> 00:49:07,723
Yeah, because video games are interesting, because like everything else can be
procedurally generated, so I don't know where you would put AI.

588
00:49:07,950 --> 00:49:09,156
I like that pic.

589
00:49:09,156 --> 00:49:10,742
Bearzahn, what do you got for us?

590
00:49:10,742 --> 00:49:15,045
really love books this much but I think this is one of the good ones.

591
00:49:15,045 --> 00:49:17,346
It's called Never Split the Difference.

592
00:49:17,486 --> 00:49:19,009
It's a...

593
00:49:19,009 --> 00:49:20,599
I read that book, it's amazing.

594
00:49:20,599 --> 00:49:25,299
It just talks about the different characters of people when it comes to negotiations.

595
00:49:25,299 --> 00:49:31,442
It talks about you've got the analyst, you've got the negotiator, and then you've got the
accommodator.

596
00:49:31,442 --> 00:49:35,633
if you're basically, sorry, you've got the assertive type.

597
00:49:35,633 --> 00:49:44,856
So if you're an accommodator and you talk to an assertive person, you're just giving them
an opportunity to socialize with you and that's just offending them and things of that

598
00:49:44,856 --> 00:49:45,066
sort.

599
00:49:45,066 --> 00:49:47,066
So I thought it was pretty interesting.

600
00:49:47,066 --> 00:49:49,025
A lot of those things where you kind of

601
00:49:49,025 --> 00:49:58,943
learn from muscle memory and think if you sort of be more intentional about it just makes
a lot more effective in day to day communications anyways not just in negotiations so I

602
00:49:58,943 --> 00:50:03,218
thought it was an interesting book that I recommend to

603
00:50:03,218 --> 00:50:04,939
No, I actually, I actually really liked it.

604
00:50:04,939 --> 00:50:15,374
One of the things that I took away from it really importantly, that it's helped me a lot
is to understand the, like, I always thought the idea of like a win-win scenario was made

605
00:50:15,374 --> 00:50:16,664
up nonsense.

606
00:50:16,744 --> 00:50:23,827
But the way he puts it in the book is that you're optimizing for certain things and the
other person's optimizing for different things.

607
00:50:23,828 --> 00:50:30,751
And you can both optimize for the things that you want as long as you make that
information public and you share it and you converse about that.

608
00:50:30,751 --> 00:50:33,372
As long as you keep it hidden and secret.

609
00:50:33,372 --> 00:50:37,455
then you can't ever really get the other person to move on that potentially.

610
00:50:37,455 --> 00:50:40,937
So I think about like salary negotiations and engineering.

611
00:50:41,137 --> 00:50:44,759
I don't have them at my company for engineers that we hire.

612
00:50:45,260 --> 00:50:47,802
We don't just say like, hey, you know, this is how much you get.

613
00:50:47,802 --> 00:50:54,126
If someone wants more money, we have a conversation about like, what is that expectation
that comes with the change in salary?

614
00:50:54,747 --> 00:50:56,127
It makes sense to talk about that.

615
00:50:56,127 --> 00:51:00,190
If you want this, then there's this other part that's important for us.

616
00:51:00,190 --> 00:51:03,082
Like for instance, people that want to be

617
00:51:03,082 --> 00:51:12,424
say a senior engineer and we think they're more at just the engineer level to level we
would say like well there's higher expectations and that means that if you don't meet

618
00:51:12,424 --> 00:51:20,776
these expectations there's a greater chance that we'll have to either reduce your level in
the future or we'll have to let you go so you know is that a risk that you want to take

619
00:51:20,776 --> 00:51:27,068
increased risk for increased reward potentially no I I really like the book so

620
00:51:27,349 --> 00:51:28,229
no, I think it's great.

621
00:51:28,229 --> 00:51:28,789
It's great.

622
00:51:28,789 --> 00:51:43,217
My pick today is going to be the L eight conference, which this year was in Warsaw and I
just got back from speaking at, I did a short talk about, uh, building.

623
00:51:43,217 --> 00:51:50,963
highly reliable software and why having five nines is nearly impossible, more so than
anyone thinks.

624
00:51:51,104 --> 00:51:59,951
So if the LA conference is in your area and you're thinking about where to go, also highly
recommend this one along with what I said last week.

625
00:52:00,272 --> 00:52:02,712
So that's it for today's episode.

626
00:52:02,712 --> 00:52:11,332
I want to thank Berzon for coming as our guest and I want to thank the audience and all
our viewers for listening to this episode of the podcast.

627
00:52:11,372 --> 00:52:16,512
And that's it and have a good rest of your week until next time.

