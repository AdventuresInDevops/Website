Will (00:00)
What's going on everybody? Welcome to another episode of Adventures in DevOps. Warren, joining me again. I keep making you feel like the new guy, but it's been like what a year now.

Warren (00:12)
Almost that long and

I've got my I got my fact prepared There was a recent well, I don't want to spoil my pick So I'm not gonna say what it is But the conclusion is that AI may be making us stupid the truth is that AI it has a huge decrease on our critical thinking or how much we're utilizing it and Not necessarily training that skill and this could be the beginning of the downfall of humanity and that's all I'm going to say

Jillian (00:42)
of take like issue with that because I remember hearing the same thing like my teachers telling me all about spell check like oh you're not going to have a computer in your pocket you need to get over this dyslexia thing and as it turns out I do have a computer in my pocket and no I still do not need to know how to spell so like we're fine the skill sets evolve it's gonna be okay everybody

Will (00:55)
Ha

Warren (00:57)
Yeah, that's...

The same thing happened with calculators as well, but I'll say more about that at the end of the episode.

Jillian (01:04)
Yeah.

Will (01:08)
Hi Jillian, welcome. right, this is going to be a cool conversation. Joining us today, have the founder and CEO of Warp, the Warp Terminal, Zach Lloyd. Zach, welcome.

Jillian (01:10)
Hello.

Zach (01:22)
excited to be here. Thanks for having me.

Will (01:24)
I'm

excited to have you on here and just to pick your brain about this, because I first saw the Warp terminal. It's been several years now, so you've been working on this for a while. And it was just like, at first it was so confusing to me because I was like, wait, this isn't what my terminal is supposed to do. It's like offering up stuff. Like, how do I trust this? So before we dig into that, tell us.

Zach (01:49)
Ha ha ha!

Will (01:53)
Tell our listeners a little bit about warp and what it does.

Zach (02:00)
Yeah, so warp is a...

It's a re-imagination of the terminal. You can use it like a regular terminal. So you drop it in and use it in place of, I don't know, whatever you're currently using. If you're a Mac, I term or there's just the stock terminal app. The idea behind it is that it has a much more sort of user-friendly user experience. So, you know, basic stuff like the mouse works for instance in warp, but it's also increasingly it's, it's about being intelligent and

Will (02:25)
Yeah.

Zach (02:32)
So when you use warp, the main distinguishing thing these days is that you don't have to enter commands. You can just instruct the terminal in English, tell it what you want it to do, and it will sort of solve your problem for you by translating your wishes into commands using AI.

It looks up whatever context it needs and kind of guide you through whatever task you're doing, whether it's a coding task or DevOps task or setting up a new project. So it's a totally different way of using the command line that I think is like pretty fun to use and definitely more powerful than your standard terminal. like we're kind of having an internal debate at this point about whether or not it's even right to call it a terminal because it's so fundamentally different from what

that people expect when they use a terminal, but it does work. It's like, think a really, really nice to use terminal as well.

Will (03:28)
Yeah, for sure. Like the terminal features are definitely all right there and ready to go. And then it just keeps, I think that's a really a cool way to get used to is just drop it in as your replacement terminal. And then you can start picking and choosing like all of these other things that it has as you get comfortable with it.

Zach (03:41)
Yep.

Jillian (03:50)
I really like that it uses the mouse because I have like a bit of a horror story of trying to get somebody set up with Vim and I felt like very proud of myself like, look I got the scientist using Vim and then they were like great how do I use a mouse and I was like oh no so I think I think that's a nice feature

Zach (03:50)
Yeah, definitely.

Yeah.

The other thing that it will help you do is figure out how to quit Vim if you end up stuck in Vim by accident. is one of our most popular features is you can ask the AI how to quit Vim. It's very funny. guess people do end up in there and they're like, what? That's common. Yeah, quit the application, yeah.

Will (04:10)
Yeah

Jillian (04:11)
No! That's not what we're trying to do here.

you mean like quit the application, not like quit

the addiction. Okay, alright, we're fine.

Zach (04:30)
No,

I have some people love him. Yeah. Yeah.

Will (04:31)
Now that's a 12 step program for that part.

Jillian (04:35)
It

is. They need a new one. They need 20 steps.

Will (04:41)
Cool, so how long have you been building Warp?

Zach (04:45)
God, we've been at it for a while. started, the company started during COVID, so 20, like the middle of 2020. And we first launched something publicly in 2021. And it's just sort of evolved.

from something where the main value initially was, hey, let's make this tool a little bit easier to use and like fix some of the UX into something that is much richer, especially when when chat.tpt came out and we were even doing some AI stuff before that, but they've been working on it for a while now.

Will (05:26)
Right on. What's the?

What's the thought process that goes into figuring out how to integrate AI into this?

Zach (05:41)
Yeah, so we went through a bunch of different stages. So the first, the first stage of AI and warp was

essentially like translate English into a command. So you could, you could bring up this little thing and it actually predated, chat GPT, it used something called codex, which was a, I think an open AI like coding API. And you could be like, you know, search my files for this specific term. And it might generate like a find command or a grab command, something like that. And it's very much like one-to-one English to command translation. The, the next thing that we did,

was when Chat2Kt came out, we did what I think a lot of apps did at that time, which was put a chat panel into Orp. And so you could have a chat panel on the side where you could ask coding questions. You could be like, how do I set up?

a new Python repo with these dependencies and it would give it to you as a chat. And then you had sort of like a copy paste type experience where you would take what was in the chat and move it into the terminal. And that was cool, but kind of, I would say like limited extra utility compared to just like doing it in chat GPT. The biggest change that we made was basically the idea that the terminal input where people type commands also could be used directly as

a conversational input to work with an AI and that the AI itself would end up in like sort of interspersed in the terminal session and we call this agent mode and so in this world it's not just that you chat with it it's that you tell it what to do and it's able on its own to invoke commands to get kind of like gather the context that it needs and help you do a thing so for instance if I was like go back to that same example like help me

me

set up a Python repo with these dependencies, instead of doing it in a chat panel, which we got rid of, you just type that into the terminal input and we detect that you're typing English and not a command. And when you hit enter, it follows up and says like, okay, like what directory do you want this in? And you tell it what directory and then it will say, it'll make the directory for you. It'll CD into it. It'll create the Git repo.

It'll do all the PIP install. It will even generate the initial scaffolding of the code. If it hits an error, can debug its own error. And all of this is happening within your terminal session. And so, you know, you get to a point where it's like, you're actually driving the terminal.

a little bit more in English than you are in commands. And it's kind of crazy how it's changing how people use a terminal. Like I was just looking at this yesterday, like in warp now, like a quarter of what is going on in the terminal sessions is actually just English and AI generating commands and not people typing CD and LS anymore. So that was the sort of evolution. So from a very bolt on thing to something where it's like the actual fundamental experience of how you use the tool has changed a bunch.

Will (08:49)
Yeah, so you're completely changing the interaction there instead of saying, how do I just saying, go do it.

Zach (08:55)
Exactly, exactly. that actually takes like, developers don't necessarily think to do that. They're very much into like, okay, let me Google this. Let me go to Stack Overflow type of mindset. And it's a totally new behavior if you're a developer to just be like, I'm just gonna tell the computer what to do. It's a little bit scary because like,

What's your terminal? it's like, now the computer is just like doing stuff in your terminal. But I do think that's the future of how development DevOps, whatever you're doing as a developer, it's going to move from this, like, let me run a bunch of queries or let me like open up a bunch of files and hand edit things to a world where you're just sort of like, Hey, let me actually tell my smart AI, whatever you want to call it, assistant agent, whatever to start me on this task. And the, um, you know, the, the,

agent will loop me in, get more info, know, leverage me when there's ambiguity to resolve, but it's like, it's like gonna be an imperative, I'm telling it what to do way of working. And like the cool thing about the terminal for doing that is like,

That's kind of what the terminal is set up for. If you think about it, it's like the terminal is set up for users to tell the computer what to do. It's just that we're like upping the level of abstraction from you telling it in terms of like grep and fine and CD and LS to telling it at the level of like a task, what you want it to do. And so that's like the vision that we're building towards.

Will (10:30)
Right on. I think it's a really great analogy, you know, because we've seen that in other areas of software development where you just keep abstracting things away more and more and coding at a higher level. But this is one of the few projects where you're actually doing that outside of doing it at like the task level rather than at the coding level.

Zach (10:52)
Yes,

correct. And like we are, so you can, you can code in warp. I don't know if, did you all see Claude code? Have you played with that at all?

Will (11:04)
Mm-hmm.

Jillian (11:05)
I have a little, yeah.

Zach (11:06)
So, Code is super interesting from our perspective, because it's all terminal-based. And it's all this imperative, like, you run a terminal program, you tell Cloud Code, like, hey, make this change for me. And it skips the file editor and IDE entirely to do coding stuff. And so...

We're also, we have very similar feature in warp. It's not to access it. You don't run a program within the terminal. You just tell the terminal what to do. But I think it's interesting in terms of like the types of tasks that you can do. And if you even look at like, have you all used cursor and windsurf, those types of apps to do any coding?

Will (11:51)
Yeah, a little bit.

Zach (11:53)
So yeah, in those apps, the initial feature that was the magic feature, and this is true for GitHub Copilot too, was it will do great code completions for you. So it gives you this ghost of text as you're typing, and it completes your thought. And the thing that they're building out now is also, it's much more like a chat panel within.

within those apps where you can tell the computer what to do and it generates code diffs. And they're creating something that looks an awful lot like a terminal interaction, but within the code editor. And so I do think there's this general shift that's going on.

for coding and I think it's also going to really impact people who are doing production, DevOps, basically any type of interaction with systems where you just sort of start by telling the computer what to do somehow. So it's pretty neat to see.

Jillian (12:49)
So I really like this because I spend a lot of my day trying to convince biologists that like, you need to be able to use a terminal at least a little bit. And it's always such a tough sell because being like, well, I'll go over here and take this Linux class is like not, not what they want to be doing, let's say. So just being able to say, you can just type in English and it will at least get you to the directory and install your Python environment and do this kind of stuff is just so much nicer than what I've been doing in the past.

Zach (12:57)
Hey.

Will (13:05)
you

Zach (13:05)
fair. Why not?

Jillian (13:18)
I like this. This is great.

Zach (13:20)
Yeah, I mean, it's the other cool thing for people who are, it's not their natural environment, let's say, and like they have to use it, is that as you use warp to do this stuff, it teaches you. So it doesn't just like obfuscate like, at least for now, the way it does it is like you type in like, hey, I want to create this project. And it says something back to you like, okay, here are the commands that need to be run in order to create this project. Are you cool if I run these commands? And so,

to Warren, your earlier points, like, is this just making us all like kind of dumber and not knowing how to do anything? It's possible, but there is also an aspect of like, it's kind of like working with like the smart person on your team who can show you how to do things and like, you know, hopefully you pick it up because it is, it is in some ways faster if you know what you're doing, just type the commands. and I think in general, like, I don't think it's a great outcome if,

everyone who's doing development or working in the terminal doesn't know what the hell is going on because inevitably you're going to get to some point where you kind of need to know in order to fix something. And so, you know, the hope is that this doesn't make people like dumber. This makes people more proficient, but there is like, I think there's a risk for sure.

Warren (14:34)
There's

actually two things that this reminds me of a lot. And the first one is a long time ago, and I don't know how well it's maintained, but there was a program that you could install into your terminal called Fuck. And...

Zach (14:45)
Yes, no, no, we've partnered with them.

I know exactly what this is. It's awesome.

Warren (14:48)
And if you've never seen this before, something that actually happens sort

of often is that a command line program you run will tell you sort of what you did wrong in a way. Like, did you mean this? And instead of having to like retype the command and fix the problem, you could just type fuck and it would read the output and then do that thing. And that's the first one. So if you haven't seen that, like I highly recommend at least checking that out. And the other one is this thing that totally changed how I use the term

Zach (15:06)
Yeah.

Warren (15:17)
for doing software development for interacting with Git repositories is there's actually a Git configuration that you can set up to automatically fix typos. So if you type something wrong, it will swap the letters around and be like, okay, you probably meant this with a 99 % accuracy and then just do that command anyway. And you can also set a timeout. Like, you know, if you accidentally type something and it's gonna start deleting all of your code base, you can be like, wait, no, I...

I don't want you to do that. But that actually brings me to a question I want to ask, which is I see more and more of pieces of software, I'll call them agents, that are interacting with your operating system directly. And for me, I'm super risk averse. I want to keep every LLM or non-thinking creature in its own private box where I can't accidentally delete my entire operating system because that's what I thought I wanted.

Zach (16:00)
Yep.

Jillian (16:11)
I've done that before though. Like as a person,

it's just like a human being. I've absolutely done

Will (16:13)
You

Zach (16:14)
People can also do that, just to be clear. Exactly, Gillian. I might trust the agent more than myself. But yeah, go ahead. This is a fair point, I think. Yeah. Like, so how do you, how to manage this is the question or like?

Will (16:20)
right?

Warren (16:26)
Yeah,

I mean, it's just, it's almost like I would want to run like two computers side by side, one of them, I mean, I already am really concerned about running external software on my machine from a malicious standpoint, very rarely is it will break my operating system. I don't remember the last time it happened. was probably when I was using Windows like over a decade ago. But when it comes to LLMs and things that like I know from firsthand experience, sometimes it's like there's

Zach (16:37)
Yep.

Totally.

Warren (16:54)
non-zero chance that it just figures out the wrong thing to do. And like that's the sort of thing that I almost want to sandbox as much as possible and I feel like we're not getting closer to that because our operating systems aren't don't allow it as much.

Zach (16:58)
So, I'm gonna

So it's a great point. I mean, you have a couple of choices if let's say you're using Warp. So one, you can just turn this stuff off. Like if you're just like, I don't trust it, I don't want it. So that's fair. There's one, there's like a toggle that just says AI off and like, that's it. You're back to like, you you're in control. There's also like a sort of like, you can control the level of autonomy it has. So the, one of the levels that you could have is just like,

It can't do anything on its own. So it can suggest commands.

you can then manually approve anything it suggests. There's a level up from that, which is like, can kind of provide like an allow list and deny list. You could be like, it's fine. It can run cat. It can run ls. can't run RN. you can go level up from that. If you're like, I want it to be able to run read only commands and let, let an LLM determine what it thinks is a read only command, which it's pretty damn good at, but not perfect. Like if you had some crazy like pipe thing or like here doc or something like that, it might

It might get confused, but it's pretty good. Or you could be like, you know, like yellow, like, like I just want to, it's not that big of a deal if it messes up my like Git repo or whatever, and I'm going to let it run. And then the other thing that we're working on that we don't have yet, but I think is really important in this world of like more autonomy is, is what's the fastest way to like spin up a sandbox where,

you know, your whatever state you want it working on is replicated and it can just go to work there without you losing any sleep that's gonna do something irreparable. I think like an undo functionality is super interesting too. It's not like trivial to do that in the terminal. Like the terminal is a stateful place where

you know, you can delete files and there's no like undo. So you kind of got to figure out like sandbox is probably the safest, but we're aware of this issue. And it makes sense. A surprising number of people don't give a shit, I will say. Like they're just like, this thing is just magic and like just, it makes me so much faster and makes my life so much more fun that I don't really care. But it's a totally fair point.

Will (19:16)
Hahaha!

Zach (19:29)
using this in NASA. I hope. Maybe, for all I know, honestly. I've used a lot of places.

Warren (19:31)
Well, I think you really hit the... Not yet, right? But... I have some theories

Will (19:34)
We hope.

Warren (19:41)
there, but I think if I say them, we'll definitely get cancelled. So...

Will (19:44)
Yeah.

Zach (19:44)
Haha!

Warren (19:47)
Yeah,

I think that's sort of the problem and I think this is, again, I don't want to spoil my pick, but realistically it's that a large majority of the population falls into this area of maybe they have concerns, but they're...

Zach (19:53)
Yeah.

Warren (20:01)
they're apathetic to actually turning off whatever the source of the potential problem is. There's not a good way to moderate AI from outside or LMS from outside the black box. It's really like all or nothing in a lot of ways. And most people are not going to turn it off because they still perceive some huge amount of value from utilizing them. so, you know, I'm not gonna turn off the feature. I'm just gonna be really scared now what it's going to do when I'm not looking.

Zach (20:23)
Yep.

Yeah, I think that that's right. People obviously have a strong predisposition to do whatever you set the default to. It might not even know what the heck is going on. I don't know, developers are maybe a little different. I feel like if anyone's gonna go tweak the knobs, it's gonna be like, you don't think so?

Warren (20:40)
Yeah.

I don't think so. think everyone has their depth where they feel comfortable controlling. if they're comfortable pulling an LLM to solving part of their job or part of what they're doing, it's probably in an area they don't care about. so they're probably not going to. I think another aspect here is I have a very close friend that went away on vacation and the person who was cat sitting for them left some plastic on the...

Zach (21:12)
Yeah.

Warren (21:23)
stove which was induction and it was totally fine. It was off but one of the cats managed to turn the stove on and actually melted the plastic. And so this is really funny though because there was no LLM in there. The cat was fine. The cats were fine. The thing is like I really do fear at some point like there's going to be someone's going put an LLM in my

Jillian (21:32)
no.

wait was the cat okay? Like this is this is the story that I need to know okay

Will (21:44)
Hahaha!

Zach (21:45)
Hey.

Warren (21:53)
in my stove, it's gonna happen at some point. And I don't think we can avoid that future. And I do fear that it will just turn on one day when I'm not here and start doing things where like I have no need for that. I'm not thrilled about this future, but it's coming.

Zach (21:55)
You

That's fair.

Kelsey Hightower had this good tweet which was, he was like, I'm actively at the point where I will pay more to not have a smart appliance. So that was pretty good. Like I get it. Like I don't need like my refrigerator having wifi or whatever. That makes sense. On the LLM side for if you're a developer, this might not be a popular opinion, but I think.

You're not really going to have a choice as a developer if you want to continue being a productive developer on whether or not you adopt this technology. It's kind of like being like, I only want to work in assembler. I'm not going to use like a high level language. Like that's not a viable choice going forward. The, I think what you're going to have to do is developer, if you want to be like productive and efficient is like learn how to use all this stuff and learn how to use it in a safe and productive way.

Is that unpopular? I think that... I don't know, I really believe that.

Warren (23:06)
Let's have a fight. No,

let's go around, know? Jillian, what do you think? Agree or disagree?

Jillian (23:12)
think so. I'm pretty judgmental over developers that don't use a debugger. So I can see this kind of being just the next iteration in that process. I don't know, developers are... I think at some point everybody's kind of drawn to development because everybody has I like to learn new things disease and writing code is really good for that. And then at some point you get really tired of it.

Zach (23:22)
Yeah.

Jillian (23:39)
And so then AI is really good for that process when you're like, alright, I'm sick of having to learn the new things, I just want for the AI to tell me what to do, and then there we are. So... I'm gonna go with mostly yes, except that I feel like I might get some angry responses on the internet for that, so I'll give a little bit a preview of it.

Will (23:59)
Hahaha!

Zach (23:59)
It's like,

there is a fear, an understandable fear that developers have, this is gonna replace them. I don't think that's even remotely true. There's also like a thing that I've noticed, which is that a lot of like the more experienced, really strong developers on our team who I've worked with, like they kind of get the least value out of it initially and are most likely to be like, this is a stupid suggestion from this thing, like, or it's like creating bad code. And so,

They have like a kind of anti take on it, but eventually people get to a sort of moment with it where they're like, shit, this actually makes my life a lot easier and does some of these, the stuff that I find super annoying. And they, I think the proper outlook to have towards it is like, this is like another tool that I can use just like.

Jillian (24:48)
you

Zach (24:51)
If I like master like said and grep like I'm like awesome as a developer. think if you could figure out how to effectively use the LLM, I think it just makes you better. I think that's like the right for now, the right way to look at it. don't know, Warren, what do you think?

Warren (25:07)
Well, I have the opposite controversial opinion, I was maybe thinking about keeping my mouth shut. So I have this perspective that it definitely replaces inexperienced engineers. so the problem with that is, and I think this is where the fear comes from, is that LLMs do not replace inexperienced engineers. People think that LLMs will replace inexperienced engineers.

Zach (25:14)
Heh.

Yep.

Warren (25:33)
do that anyway. And I think we're already starting to see that happening. And the problem with that is you're paying money for these tools and you're not training your organization's people on leveling up their skills in these areas.

Zach (25:36)
Interesting.

Warren (25:46)
and will become more more dependent on them and definitely move away from it. Now, on the productivity side, I still think it costs way too much. I think there has to be magnitudes cost reduction in generating answers before this becomes of high value.

Jillian (26:04)
cost you mean like

monetary cost like the LLM itself?

Warren (26:06)
monetary, environmental,

et cetera. It's still like none of the AI companies are making money, like the ones that are pumping out AI. Open AI, I'm sure whatever. We know Anthropix aren't making money. know whatever they are, zero, like it's negative, negative billions of dollars per year on this. So that's not a sustainable model from a society standpoint.

there's going to have to be something to change. Either these tools will completely go away or the costs will have to come down. I think the last thing is that we find from productivity standpoint, at least for me and myself and the companies that I work with, is that the bottleneck isn't doing more work, specifically writing out code or pushing that out. So the tools don't solve the needs that we have. It's okay for us to still be slow in this way or not be productive in this way because that's not where our bottleneck is.

Zach (27:07)
I disagree with almost everything you just said, but I'm gonna, I'll let Will go. It's interesting to have this discussion because I'm so in the like AI bubble of like Silicon Valley people and like AI tech companies and like.

Will (27:10)
Hahaha!

Warren (27:10)
Hahaha

Zach (27:29)
like the main contention that I hear amongst these like the people I talked about on the investing side and the other AI company side is like how quickly are we getting to AGI and Warren is coming in hot with being like these things are not even valuable. And I think

Warren (27:43)
It's not even AI. I hate this term. These companies are lying

to the masses of people saying we have AI. All we have is transformer architecture, which is able to create LLMs. And they will always hallucinate. And this is the ridiculous thing. I'm waiting for someone to say, how is open AI going to recoup the billions of dollars they're losing every single year? Where does that change? Because money will run out at some point.

Zach (28:00)
You

Will, will you want to go or do want me to respond to that? This is good.

Warren (28:14)
Hahahaha

Will (28:17)
I'm gonna jump in real quick, then we can come back to that. I

think I tend to agree with you, Zach, that there's gonna be people who are resistant to AI. And I think the primary place I've seen this is people who really are, they're really passionate and vested in their chosen language. I think if we look at the category of people who will argue Go versus Rust,

And they've pinned their career on, I'm a Rust developer or I'm a Go developer. And so they'll try something like AI or any of those related tools and say, well, it got this wrong. That's clearly why I'm not going to rely on this thing because it got this one thing wrong. And you'll get a lot of resistance from those people.

Zach (28:44)
Mm-hmm.

Warren (29:14)
I think that.

Jillian (29:14)
I just think

of AI as another tool. guess, Warren, what you're saying with all the money being spent and the environmental costs, that is very valid. But from the tool perspective, I'm already so dependent upon tools. Without dictation software, PyCharm, and Vim, I am completely useless. I have zero utility to anybody, anywhere, at any time, and in a professional context anyways. So this is just another tool that I am... I do have kids. Occasionally I'm useful in a human context.

Zach (29:18)
I'm enjoying it.

Warren (29:18)
Yeah.

Will (29:36)
Hahaha!

Jillian (29:44)
from a professional standpoint, if I don't have those things, I'm not gonna get any work done. so AI has just become like another tool for me to use. And so I just see it from that perspective. From the money perspective, like, I don't know, but humanity spends a bunch of money on a bunch of things that we don't recoup an investment from. Like it's just, the money never actually runs out. We don't have a gold standard anymore. Like there's, there's always, like it's an arbitrary concept. There's always money.

Warren (30:15)
You

Will (30:16)
As long as the printer companies keep making printers that print the money.

Zach (30:18)
You

Jillian (30:20)
I mean,

isn't that kind of what we're doing at this point though? Like, isn't that what the governments of the world have sort of decided we're doing?

Warren (30:26)
Well, there's.

There's a secondary problem here actually, which is that the energy consumption is too high. Like even shave off the environmental impacts. The energy cost is so high that people are now starting to have their lives affected by having spotty continuous energy flow into their own appliances in their house, lights and stoves, ovens, whatever. And that's happening near data centers where increased energy usage is required to run LLM. So I think that problem is likely to get worse, even if the money doesn't run

Jillian (30:29)
you

Warren (30:57)
but...

Will (30:58)
But if you had a smart refrigerator, it could adjust for that automatically.

Zach (31:00)
Hmm.

Jillian (31:03)
Exactly, if the things are smart, then what do you even need the energy for? I don't know, we're fine.

Warren (31:09)
I

like the perspective. I mean, it is a tool for sure. And I think the thing that I see is that it used to be the fact you could type into Google and get a website that helped you answer the question you have. And you can't even do that anymore because at least that search engine has become utterly worthless. And so you need a replacement for that. And I think it's worse.

from a accuracy standpoint, then Google at its best, but it's for sure better than Google now. And I think that's a worthwhile trade-off that you have to change. If you're still using Google or you still believe that your one true programming language is the only one for the future, I think that's just the mindset which doesn't make sense.

Will (31:52)
So Zach, you wanted to come back and answer or respond to the money issue.

Zach (31:54)
I can't

speak to the energy stuff. can speak to just like, it's valuable. for developers paying 20 to 40 bucks a month for AI in their core tools, if you just think of how much development time costs, the...

you have to save, I don't know, 20 minutes or something for that to be a worthwhile thing. that threshold has been crossed a long time ago, in my opinion, just from using these as a user of these tools, the amount of like time that they save me.

It's like a no brainer trade off. I don't know if anyone on the back end of this is making money yet. do know warp, like we have a positive margin and people pay us for AI. And so it could be that the model companies or the hyperscalers are just taking a huge loss on warps profit. But, uh, it, uh, you know, from like just pure economics, people find the value they pay for it. They stick with it at like a surprising higher rate. Like we don't have very high churn on it. And so I have to believe that.

that

just from that and from like actually using it that there's a ton of value. It's certainly true that these things are not infallible. And like, I guess you could debate from like a philosophical perspective whether or not they're intelligent. I actually think that they have some level of like intelligence. Now it's not like, doesn't quite work the same way that human intelligence works, but they're able to like, I don't know, they're able to do things that up until.

couple years ago, you would only say a human could do. So there's, it's, I personally am like super excited by the progress. Like I was a, like I studied a bunch of philosophy, I have a philosophy degree in addition to a CS background. I think it's like absolutely fascinating what it says about.

what intelligence means. It's not, like I said, it's not perfect human intelligence, but it's something. And it's like, I think it's a pretty awesome technological advance. So I'm more pro AI, more bullish. I think Warren's a little bit more on the skeptic side. That's legit. Okay.

Warren (34:11)
Rock.

Well, I think I can't assign the word intelligence to it yet. because

of the architecture that it's utilizing, it's just a probabilistic word predictor. And I think we need a different architecture other than the transformer architecture to actually reach anything that would be fair to call AI in any capacity. I do want to jump into how you're utilizing it, though, at Warp.

Zach (34:41)
Sure.

Warren (34:42)
Are you running your own foundational models or are you passing queries to something configurable for like I can put in OpenAI API key or Anthropic API key? What's going on there?

Zach (34:55)
You can pick your model. we support the Anthropic models, the OpenA models, Google's models. We support a US hosted version of DeepSeq models, even some of the open source models. You can't go directly to them because our server has like a whole bunch of logic on like the prompt engineering and sort of different agents for different types of tasks. So there's like a logic layer between them, but the basic...

The basic intelligence underlying the AI warp currently is the foundation models. There's a chance at some point that we'll get a little bit more into the like, make a model to predict your command type business. But currently we're, we find that the best thing for our users is to sort of use the, like we're not gonna spend a billion dollars on.

know, GPUs or whatever and train models right now. I can contribute to the energy problem. Yeah. Yeah. So we're, I would say like we are at the application layer. If you look at this as like application layer, model layer, hyperscaler layer, we're at the application layer.

Will (35:52)
That would probably change that profitability statement you just made earlier.

Warren (35:58)
I mean from that, from

Jillian (35:59)
Possibly.

you

Warren (36:08)
No, it makes sense. mean, but in that way,

the model providers are definitely subsidizing the profitability because they're taking huge losses. I mean, I don't know who's making money from there down. It is their problem. Yeah.

Jillian (36:18)
No problem,

Zach (36:20)
It's a super interesting question of like, where's

the value go in this whole thing? the, you know, the other thing, like, so the model providers, I think, like the big question mark to me is like open source models. And like, if you have open source models, especially ones that are of like comparable quality, like open AI and anthropologists of the world can't maintain like a real leading quality or latency or something like that.

How does the world work in that? And so the open source alternative where you run it yourself and you don't have to pay the sort of margin to open AI is super interesting to me.

I think that the one place that there's definitely going to be someone's going to make money is just on like serving these models. So I feel like for better or worse, if you're Amazon and AWS, you know, G cloud, Azure, whatever, they're going to make money because someone needs to these models and the, the local versions, which is, think another interesting thing to consider are at least currently they're not at the, they're not at, it's not really practical to like get the same level of power from like downloading.

know, llama, but that's another thing I'm looking at is like, maybe it's just local models that totally disintermediate the need for these like huge API based cloud models. Who knows? Yep. Yep.

Warren (37:47)
No, I mean, you're onto something really there

because it would cost you way more than the price that you would pay to the model providers to utilize their LLMs if you tried to run the open source models locally on hardware that is comparable and gets you speed and accuracy precision in order to utilize that. Yeah.

Zach (38:04)
Yeah, that's the problem right now.

Jillian (38:11)
think we should talk about Warp some more and like its features and what it's doing and all that kind of stuff. Specifically, I want to be able to like speak into the terminal, my commands, so that I don't have to type. When can I expect that?

Will (38:11)
soon.

Zach (38:15)
Sure. I'm down to talking about whatever, by the way. I fuck...

Warren (38:16)
Jillian's all in.

Zach (38:22)
You can do that. So, okay, so

we added this feature. It's super cool. If you're using warp, you can hold the function key or you can configure it and you can talk to your terminal. It's magic. You can just tell it what you want to do. It translates it.

into English and then it runs it. And so it's pretty Star Trek-y from like a user experience standpoint. So yeah, that is something that we launched and it's cool because yeah, like why should people have to do anything? Like if we could just plug your brain up directly.

Will (38:47)
Hahaha!

Jillian (38:50)
That's great. Saving the people from the repetitive stress injuries. Like this is what I need. Exactly.

Will (38:58)
Hahaha

Warren (39:00)
I know what Jillian's waiting

for. She wants the brain interface device so she can...

Zach (39:03)
Yeah, exactly.

Jillian (39:04)
That is really what I want, like...

Zach (39:07)
I

Jillian (39:07)
I wanna just like play in bed with my heated play kid and do nothing.

Zach (39:08)
think think Jillian would be a really good warp user. I'm getting the sense that warp is actually very well suited to Jillian's work for us.

Jillian (39:15)
It really is, like, especially since

you just said the speech thing, because I'm getting older and I can't type so much, so like, I very specifically need the speech thing.

Zach (39:18)
Yeah.

And why should you have to type?

Jillian (39:25)
I know, I shouldn't. That's right. Zach, I feel like you're really my kind of person here.

Zach (39:27)
Thanks

Will (39:28)
That reminds me of like an episode.

Zach (39:31)
You

Will (39:32)
It reminds

me of an episode of The Simpsons where Homer's in the hospital and the guy in the bed next to him is on a breathing machine. And he's like, hey, how come that guy gets someone to breathe for him and I'm over here doing it by myself?

Zach (39:46)
I love it.

Warren (39:47)
See,

I thought you were going to bring up the episode where he tried to get to 300 pounds so he could be classified with a disability and had to use a wand to dial the iPhone.

Will (39:54)
Hahaha, yes!

Zach (39:54)
That's funny.

Jillian (39:57)
That must be an old episode.

Like, that must be a real old episode. Okay.

Warren (40:01)
Yeah, that was when it was still good.

Will (40:03)
Yeah,

Dr. Nick's food philosophy was if you rub a newspaper on the food and the newspaper turns clear, it's good to eat.

Zach (40:14)
I mean, one of my, I'm pretty lazy and like, I'm not like ashamed to be lazy when, especially when it comes to, when it comes to development, I don't want to have to do more work than I have to do to ship something that's useful. So like my, but like what I care about as a developer, like again, there's different kinds of developers, but to me, I'm like all in it for the, I want to build something cool. I want to ship it out to people.

Will (40:19)
you

Zach (40:42)
I want to be proud that I built it, that it, want it to work really, really well. And I want to do that with like the minimal possible effort. and to the extent that I have to put effort into it, I want it to be effort that goes towards thinking about how it ought to work. And I don't want to spend effort on like annoying shit in a terminal. Like that's like the last place that I want my limited brain cycles to go. I don't want to spend effort either on like.

changing function signatures in my files. just, know what I want it to be. And I want like to get from A to B as quickly as possible. And so, yeah, it's the extent that something like AI, I think warp for the terminal, especially like makes it so can be like a little bit lazier. Again, this isn't like the advertising I'd put on our homepage or whatever, but I think it's like, I think that's a valuable thing. And like, honestly, like a lot of the...

Will (41:28)
Maybe you should.

Jillian (41:32)
advertising in my head. It's great.

Zach (41:37)
Best developers I've worked with in my career, just kind of all about that, like don't make me spend my brain cycles on like tedious shit and toil. And so I feel pretty good about trying to eliminate that stuff for developers so that you can do the more fun stuff. Cause the really fun stuff is like, it's like to me at least it's like, how should the product work? And then it's like, how do I architect this thing so that I can make the product work the way that I want. And then the least fun thing is like the like,

typing in the words in the text editor or the terminal to do that. I don't know if everyone's the same way as me. Yeah.

Warren (42:15)
No, think I, you're onto something. Will is gonna say it.

Will (42:19)
Yeah, no,

I was gonna say, it's much more exciting to work on how the application works than how to center this fucking div.

Zach (42:28)
Right?

Can an LLM do that? If an LLM can do that, I'm in. Yeah.

Jillian (42:32)
for me too.

Warren (42:33)
Vertically, you know, I to settle a sudden of the div vertically that on the page that's that that's the key vertically, you know horizontally you just

use flexbox. No problem. I think Well, you know, there's interesting thing here because I feel like if we take this to natural conclusion, it's probably like the Managing directors who will then be responsible for building the product by communicating with

Zach (42:43)
We yeah go ahead, sorry about that.

Warren (42:59)
the AI that we technology that we have available and not needing so-called technology department in any of our companies anymore.

Zach (43:08)
So that's like a horrible outcome to me. Like if it's just product managers making software. Sorry, no offense to product managers.

Warren (43:11)
Hahahaha

Will (43:13)
Hahaha!

Warren (43:15)
arguably

that's what's happening right now. There's just a couple of people in their way that are telling them that they can't have it exactly what they want.

Zach (43:25)
That's interesting. That's well, I think that's, that's not how it works at warp. Like, I could be at work some places, but so at warp, for instance, when we built something,

Jillian (43:27)
That's like what all the U.N. buildings were for.

Warren (43:30)
Hehehehehe

Zach (43:36)
And we may be, again, we may be different than other places. It's primarily engineers who are driving the product direction. Now we're working on a product that is used by, we use, we're the customer, we're the audience. And so we have this awesome virtuous feedback loop of like, we build it, we use it, we like something, we don't like something. And so we drive a lot of it.

I don't want to change that at all. Like, actually, I think that's not a good thing to change. so I also just, I don't think, as bullish as I am on AI, I don't think that we are close to the point where you can build something meaningful without having some technical knowledge. And if anything, like,

Again, it's probably not the prevailing wisdom, but it's like, I think you need to be more technical to be able to sort of guide and correct and be like the tech lead for an AI. And it's, if you are a aspiring developer these days, I would say like, learn the shit better, like learn the fundamentals of CS better because...

if you want to effectively produce software in a world where you have someone who's like pretty smart but also kind of like a savant and like dumb in a bunch of ways, you need to know what the heck is going on for when you hit a wall. And so I think, you know, I don't think we're close to a world where it's like...

MBAs building all of our software, offense to MBAs, MBAs are great, but like I feel like it's you're gonna need people who are experts in order to effectively use this tool to get its max capacity. And I do think like, Warren, I don't know, you're pointing out like if you're really junior and you don't learn, if all you do is say maybe like you've only learned how to build web apps, I do feel like you're like a little bit at risk. Like my advice to those people would be like up level your CS skills. But I don't see a world anytime soon where if you're in a

Will (45:01)
Hahaha.

Zach (45:27)
professional

software development setting that developers are going away. I sincerely hope not. I mean I'm screwed if that happens. I'm a developer.

Warren (45:36)
I think it's the leap there that's problematic. It's that we know you need the skills in order to utilize LLMs effectively. Like you're not going to be able to just job off your entire brain to this vehicle and have it go at full speed without thinking. It really does require critical thinking to interact with it effectively. And that's what you're saying. And I think the problem is, yeah, I think part of the problem is that some companies believe that that's not necessarily the case, that you can delegate this out to an LLM.

Zach (45:44)
Yeah.

I think so.

That's what I'm

Warren (46:06)
and have it up. mean, there are products.

Zach (46:07)
you're saying

some companies are just buying the hype that we don't need to hire developers anymore and we'll just hire Ella.

Warren (46:11)
I mean, there's like,

there are companies out there that are like, you know, we are an agentic building thing. There's like the software developer, Devin or whatever. Yeah. So I mean, and I think what I'm saying is I know those can't work. But I think the

Zach (46:18)
Yeah, yeah it's adventure.

But

those companies will find out when they try to replace their development teams with Devin.

Warren (46:28)
I know. Yeah, I know. Who's building Devon?

Is Devon building Devon? Because I don't think he is, or they are doing it that. Yeah. But I think the bigger problem is that the leap from, hey, I'm someone who doesn't have technical capabilities to.

I want a job utilizing technical capabilities. That gap is growing and harder to get into it now because the technology available for us to interact with is much more complicated than it was five years ago, 10 years ago, 20 years ago. And...

Zach (46:52)
Hmm.

Warren (47:02)
the skills that you get from even training a little bit, like teaching yourself up-skilling even a little bit, is much further away from what companies are looking for. At least that's my perspective that I think I'm seeing, and I think the LLMs are contributing to that gap.

Zach (47:19)
I'm sure like, okay, so say you're a company and you're spending hundreds of million dollars on software developers. I'm sure you're like, God, I would like to spend less money and have equal output. And you could be like, okay, I'm gonna hire AI software engineers to take the Devon example. And I've tried Devon.

It's a neat vision. Devin, I don't want to, I'm not going to shit on Devin. It didn't work that well for us. I know they're improving it, but it doesn't, it's like that model today does not work. Will that model work in five, 10 years? I don't know. I'm still skeptical. I think any company that finds that they want to improve their like cost efficiency on the software side by replacing their developers is going to be in a

Warren (47:42)
Yeah.

Zach (48:07)
I think it's just they're gonna find that they don't get the ROI on that and that the better ROI right now is to empower your developers and like give them tools that let them be more productive. I'm saying this, I'm obviously super biased. I run a developer tools company where I'm something where the mission is to empower developers, but I truly believe that that's like the right way to approach this. you know, it's like companies will try whatever they're gonna try, but I think that they're gonna stick with.

Will (48:21)
Hahaha!

Zach (48:34)
something that actually gives them the result. don't think that they're like the economic incentives are such that like if JP Morgan replaced all their developers with AI software engineers and then like all their banking transactions failed, they would be like, this is not the right move. And so I do think that there's like back pressure on doing something that actually works.

Will (48:56)
I think that's a great model and I encourage them to do that. And then when it blows up, I want them to head over to my website where I have my consulting rates listed.

Jillian (48:56)
See you.

Zach (49:02)
Exactly, gonna

need some smart people. I'm pretty bullish on smart people still.

Warren (49:11)
We actually went,

yeah, mean, for sure, for sure. mean, we actually went, we actually did a deep dive in this area in our episode on the DevOps report from Dora in 2024, where like the, I don't know if you've read it, but the actual results was like the value that LMS were providing to organizations was suspect. Like it wasn't significantly different than.

Zach (49:16)
Yeah.

Okay.

Warren (49:40)
where they had been before. It was very difficult for organizations to justify the value to the bottom line or the value to the products that were being delivered. I think the interesting thing was, the one thing it did say is that people were happier with using the LLMs, but it didn't actually reduce toil and it didn't reduce the amount of time spent doing things that they didn't like, which is interesting. I think it gives the most value to people who are positive, optimistic about AI. So if you like AI, you should use it.

Zach (50:08)
That makes sense.

I can tell you experience from Warp, so...

So there's the way we think about users coming into warp. There's some users who are coming into warp because they're like, I love AI. They're like, I want, I love this new technology. I want to like use it in all of my tools. And those are great users for us. Like they come in and they're like, holy shit, I can, I can use a terminal in this totally new way. That is not the majority user. So the majority user for us is what I would call like an AI neutral.

developer who might be like, okay, I'm open to this, it's like, there's a lot of hype, I have a bunch of inherent skepticism. And for those users,

the challenge for us is to get them to actually see the value of the AI and like actually use it. And so the way that we've like figured out how to do that is that it's very similar to that tool that you mentioned earlier, the fuck. And so like, when you have an error in warp and it's like, oh shit, like I'm missing this, I don't know if I'm allowed to swear on this podcast, but I was like, I'm missing this.

know, Python dependency, we show something where it's like, hey, we can fix this for you. And like, all you have to do is hit command enter and we fix it for you. And then that's like like a conversion moment. And so I guess my, my point here is like kind of piggybacking off your point is like, there's some people who are just like into this and like, they're going to love it. And maybe they love it even if it isn't really helping them. And they're just like,

messing around with LLMs all day. But I do think based on our experience converting people who don't inherently want to use this technology, that there must be value because we have, like I said, we have a lot of people paying us for something that, like, and I don't think that people are just going to pay us for something they don't find valuable. Like, and a lot of them were not AI enthusiasts to start. They were people who tell us like,

Oh shit, like this thing just saved me hours and I love that. So that's like the, you know, my kind of counter to what you're saying.

Warren (52:22)
I'm really curious, you said, so the commands are going through this proxy layer that you're hosting and before interacting with the model prior. So I don't know if you can share, but maybe there's some interesting metrics or data that you've been able to collect based off of what people are looking for, what has been searched, what sorts of problems are being fixed, anything in this area.

Zach (52:28)
Yes.

Yeah, so we have a group of like alpha testers who give us like data collection access essentially. And so really common use cases where we're helping people are the like install dependencies, the like.

Git, my Git is messed up. Like I did something, I'm in some weird Git state and I need to get out of it. We are increasingly fixing compiler errors for people. So people who have like simple compiler errors and the error log is in the terminal we fixed. We get people who...

Warren (53:10)
It's so interesting.

Zach (53:19)
a lot of Kubernetes, Docker, Helm, those types of issues where there's very heavy command line usage and pretty complex commands that you need to do is another really popular area. We do things where we write scripts for people to automate things that they're doing over and over again. It's a mix. would say the really prime use cases for us to start are things that are pretty terminal oriented.

Increasingly as people realize you can fix coding stuff and work and we guide them into that the coding stuff matters a bunch too because it's just like developers spend a lot of time writing code

Will (54:02)
I think one of the things that doesn't really get highlighted enough is that there actually is a pretty steep learning curve to using these AI tools. I think there's an expectation that, it's AI. I just go in and it's going to make my life magical. But really, my experience with it has been learning how bad I actually suck at communication.

Zach (54:13)
Yes.

Will (54:26)
And like that was the first job. Yeah, like that was my first job is to figure out how to communicate.

Zach (54:27)
I couldn't agree more with this. I don't agree that you suck at communication, it's

weird. It's turning every programmer into someone who needs to learn how to write, which is kind of a crazy skill. But yeah, the quality of what you get out of these LLMs is highly dependent upon how good you are at prompting them, how good you are at providing them with the right context to answer your question. And if you...

Yeah, who would have thought that being really good at writing English would have been the core thing? guess engineers write design docs. It's not that different from that skill. It's a real behavior change, and it's a real skill, and I think it's a great observation. I agree with that.

Warren (55:11)
I mean,

I know I went to university specifically to study engineering so that I wouldn't have to read and write words. And now my life, I pretty much just write a lot of blog posts, knowledge-based articles, chat with LLMs. Every single day, it's just words. It's just words. That's my whole life now.

Zach (55:19)
Heh.

Yeah.

Will (55:34)
Yeah, I think it's worth elaborating on though, just to like, that's one of the reasons I'm...

being more pushing people more into AI is like, yeah, I know you get it. You tried it. It made a mistake and you're ready to write it off. But I really need you to stick with this and learn how to use it because by putting that time and effort in now, you're going to figure these skills out and learn how to make it productive. And then as the technology itself improves, you're going to start getting to take exponential benefits of that. And so you and your career are going to be way, way

of everyone that you're sitting with now who says, AI sucks five years from now.

Zach (56:21)
I'm with, I'm 100 % with you that that's the smart approach is like, again, I think it's like the tool analogy is the right analogy right now where it's like, you can't get mad at them if you like.

didn't learn the VIM shortcuts and so how do you can? People get mad at me, but it's like counterproductive. I think if you remove the hype for a second and just think of it as like, it's a computer program that you're using. It's like, yeah, you gotta learn how to use it. And like, what is it, like RTFM? Like I kind of hate that, it's like learn how to use it if you wanna get the most out of it is 100 % right. And if you are...

Jillian (56:36)
Yes, I can. Like, I would just like say right off the bat, like, yes, I can.

Will (56:37)
You

Warren (56:37)
Hahaha

Jillian (56:44)
That's what matter to me.

Zach (57:05)
if you think of it instead as like a dumb coworker you don't want to associate with but that dumb coworker is like someone who's on your... well I don't know where I'm going with this. Don't think of it as a tool that you gotta get the most out of. That's where I've gone.

Will (57:18)
Hahaha

Warren (57:22)
think you're onto something

there really important because I think one of the things that a lot of the LLMs we see out there, and I think this is where some of the value is definitely lost, they don't do a great job of teaching you how to be an effective prompt engineer, like how to actually create communication with the tool to Will's point. And I think part of it is because those same companies have no idea how their own thing works, so they can't actually give good recommendations. But I think they do figure it out over time because there are communities that pop up that are discussing this and then they bring

that knowledge back in where we see examples where the Dali model that OpenAI has is the prompt is being mutated by their...

01 or whatever based on what the user inputs because it's just nonsensical and needs to be mutated and like those instructions it would be great to be exposed and I just feel like these tools don't do this good of a job but you work on the application layer and so I feel like you're providing a much better experience for teaching people how to utilize the tool effectively because you have to because you're actually selling a real product.

Zach (58:17)
Yep.

Yes.

Right, right, no, no, no. And it's like, it's a thing that we're constantly thinking through. Like we have a feature that is suggested prompts, essentially, where...

you know, if there's a like the most common use case again, it's like an error resolution, we'll see, well, based on the error that we see, we will suggest a prompt and the prompt probably is a little bit more than just like fix this, which is what a person might write. It is probably like fix this Rust error that is caused by incorrect mutability. And like you provide, we do everything we can to make it the minimal amount of work. And also to show the user like, Hey, here's what we're

actually telling the model so that if you want to do this on your own next time without like warp doing it, you can do it. So that, that, that, that's a, it's a key skill. I totally agree with it. That's something that matters.

Jillian (59:21)
I think this kind of shows my bias because like forcing developers to have to communicate properly, I just don't see that as a problem. I'm like, this is a good thing. This should be a feature, not a bug. This is fine.

Zach (59:31)
Ha ha!

Will (59:31)
Hahaha!

Warren (59:32)
Okay, maybe I'll put this into perspective, Jillian, in a different

Zach (59:35)
Yeah.

Warren (59:35)
way.

Communicating correctly is a subjective perspective based off of the people involved in that collaboration. When you're communicating with a second person, there's a culture involved, there are your values involved, the definitions of words that you grew up with, all these things in that. And when you're using an LLM, it's challenging to figure out what its culture is, how it responds to certain things. And so you have to learn that tool. So I think there's a difference between, you're not becoming a better communicator, you're becoming a better

communicator

with that thing. I think, is it the good thing to force people to do? mean, communicating with other human beings that you work with, yes, for sure. Forcing them to learn how to use, you know, end tools out there that all are slightly different in individual mindsets or cultures or whatever, the corpus of material, you know, that I think is open for challenge and debate.

Zach (1:00:09)
That's right.

Jillian (1:00:33)
But I just see this as like a people living in society kind of issue. Like when I was a kid, my dad was like, you're going to take a typing class because that wasn't just an automatic thing back then, you guys, all right? Like this was, this was a while ago. And I kind of just see AI as sort of like, think it's, it's very like pivotal. It's paradigm shifting, but it's, it's another iteration of that. It's another tool that we're adding on that people are going to learn how to use that everybody's going to have to use. Just like, I don't know, like now my kids just-

I did not have the option to sign them up for a typing class or not, it's just part of their curriculum that they are just doing.

Will (1:01:13)
I think you should put them in a typing class.

Jillian (1:01:17)
They already are though, that's the fit, like it's not an option. Unless I just pull them out of school all together, which is... Oh, with like a typewriter? Yeah, I should do that. I think that'd be fun.

Zach (1:01:18)
you

Will (1:01:19)
No, no, no, no, like an old school with the old manual typewriters, just to screw with them.

Zach (1:01:27)
Ha ha ha.

Warren (1:01:27)
See, you know, here, I

have a good parable here because when I was in the fifth grade, I think, I was in a typing class in my school, was provided public school, you got a typing class, and I learned...

Jillian (1:01:34)
you

Warren (1:01:41)
ASDF JKL Semi over and over again for a year. And realistically, I don't use QWERTY, actually. I find it to be a lackluster subpar keyboard layout. And so I was taught something that took me many months to unlearn so I could be more effective on my keyboard.

Will (1:02:02)
You used Vorac.

Jillian (1:02:02)
What do you mean?

Zach (1:02:03)
What do you

use, Sporak? What are you gonna do with that? You can't just hang it right there.

Warren (1:02:04)
Yeah, I'm a Dvorak fan.

Well, actually, I'm a programmer Dvorak fan, but I have used Linux to configure almost all of the keys so that the third level, not shift and control, but the special alt gr key to give me other things that are beneficial for programming and German and Greek and Roman, however I want to utilize them.

Jillian (1:02:30)
Sounds like a lot of work.

Warren (1:02:32)
Well, this is the thing. We're talking about productivity and optimizing your flow. And I find that I type, you know, U with an umlaut or A with an umlaut or a dollar sign or the euro sign frequently. And so I want an easy way to type those. I don't want to Google euro sign and then copy and paste that somewhere. You know, it's like on your phone, isn't there an emoji key where you can hit emoji and then find the emoji you want? I mean, I see the LLMs as sort of similar tool from that perspective, right?

you're hot keying over to your terminal to type those things out and get the answer rather than having to search on the internet.

Jillian (1:03:11)
Yeah, if it's what you're doing, it's what you're doing and there should be like a productive way to accomplish your goals.

Warren (1:03:16)
Look,

Zach (1:03:16)
you

Warren (1:03:18)
my keyboard layout is open source. It's available on my public GitHub repository.

Zach (1:03:21)
This is amazing.

Will (1:03:22)
Yeah.

Do you have blank keycaps too, Warren?

Warren (1:03:29)
So this is not the episode where we talk about my keyboard.

Will (1:03:33)
Yeah

Jillian (1:03:34)
I think it's

becoming that though because now we all need to know.

Zach (1:03:35)
I think people are going be very interested in this keyboard.

Warren (1:03:39)
I took a QWERTY

keyboard, it's the Logitech, I don't even remember what number it is, like K400 or something, it says on here somewhere, I have no idea what it is. It's their silent version, the one that makes the least amount of sound possible, because I care about noise more than anything else. And then I just moved the keys everywhere I could, and this thing you'll find out about keyboards that are not designed this, the F key and the J key have a different form factor than all the other keys on the keyboard, so you can't swap them around. I don't know why they do this, just to piss you off apparently.

You know, it's like these two keys are going to be different. I don't know why, but they are. And so all the keys on my keyboard are in different spots, except for the F and the J. They're exactly where they started on the QWERTY.

Zach (1:04:22)
think it's because that's like home based, right? Like you want like a tactile way of finding out where those are.

Warren (1:04:29)
But it's the keycap form factor, not like the key itself. So it's like, the only justification that I can figure out is that if you took all the keys off the keyboard and you're like, where do I put them back? I don't know. these two have a different form. Maybe the F and the J goes there, and then I can figure out where the other ones go. And I'm like, that's pretty suspect. But it's like every keyboard I've seen has this problem.

Zach (1:04:34)
I don't know.

Will (1:04:35)
out.

Zach (1:04:52)
I got a mechanical keyboard once and my wife made me stop using it. She's like, that is the most absolutely obnoxious, annoying sounding thing. Like, put that away, I don't want to see that again. I was like, no, it's cool. I love the feel of it. It's like, k-k-k-k-k. You know, it's like really loud. And she's like, no.

Will (1:04:59)
It's same.

Warren (1:05:01)
you

See ya.

Jillian (1:05:15)
Yeah, I removed

those from my kids Christmas list. I was just like, we're just not there anymore. We're not doing this.

Will (1:05:18)
Yeah.

Warren (1:05:22)
I know that that would not work for me because I'm a very angry typer sometimes. Like my wife can figure out like what application I'm using and what I'm doing, but based off of how angrily I'm typing on the keyboard. Like when I'm typing a blog post or writing a message in Slack somewhere or an email, it sounds different to her. And so I think it's like how angry I am, you know, when I'm in an email.

Zach (1:05:43)
I have exact

same thing. If my wife can be like, don't send that. be like, take a breath. She'll be like, don't send. I'm like, I'm like, ah. And she's like, no, take a breather. Don't, don't. And it's the thing actually is like, as a manager, I try to remind myself of like, don't, don't, no angry slacks, no angry emails. Take a breath. Yeah.

Warren (1:05:49)
You

Jillian (1:05:49)
Don't do it.

Will (1:05:49)
Right?

no, he's typing the manifesto. Get in the car, kids, get

in the car.

Jillian (1:06:11)
So maybe like, you know, doesn't Google have like a drunk email detection? Maybe what we need is for the keyboard to have like an angry sort of email detection and be like, you know what, we're gonna wait, we're gonna wait 15 minutes and then we're gonna revisit this and see if you would still like to

Warren (1:06:11)
You can.

Zach (1:06:19)
You're pushing it too hard.

Warren (1:06:25)
Look, I feel like, Jillian, you haven't tried searching hard enough. I'm sure there's some extension out there for your browser, which runs some sort of LLM in the background and determines whether your email has some sort of angry tone to it and will prevent you from sending an email if it contains that message.

Jillian (1:06:40)
If you use like ProWritingAid, it will detect the tone of your email and maybe course correct you a little bit. And I do have that, yeah. I do.

Zach (1:06:49)
You

Will (1:06:52)
You hit send and it comes back and says, I didn't send this, but I feel like it's a good time to talk about your feelings. What's the source of this anger for you?

Zach (1:06:59)
Yeah.

Jillian (1:07:01)
the bottom of these issues.

Will (1:07:03)
He did.

Jillian (1:07:05)
Speaking of which, think we need to get back to work because I have like specific questions and more like more feature requests for like me personally. Because this is the point of having the app people on the show is that I can be like, if I use this, I have things that I want. All right, like that's that's what.

Zach (1:07:10)
Go for it. Okay. Do it, do it.

Warren (1:07:11)
You

Will (1:07:13)
Bring it on, bring it on.

Zach (1:07:20)
Let's do it.

Yeah, tell me what I look at it for you. Customer support here.

Jillian (1:07:23)
So I saw that there's

like warp workflows and I'm wondering, can I do those in reverse? Like I can, can I go through and figure something out and then be like, all right, warp, I'm stupid and I don't remember anything that I just did, but I'm probably going to have to do this again. So I would like for you to go through my history, figure out what I did and just go put it in like a markdown file or some notes or something as opposed to like history. Yeah.

Zach (1:07:47)
doing it proactively.

It's a great idea. We don't quite have that. We have the ability to take command that you've already run and turn it into workflow. Just so folks know what a workflow is. A workflow is a...

It's kind of like an alias, but it's like a templated command. And so if you have like a complicated thing you're doing in Docker or like, what's your workflow for like cherry picking something into a release, you can make it one of these templated commands. And then we actually make it so it's shareable, which I think is kind of like the killer value of it. And so if you're working on a development team, you can build up a library of these things that you can use in different situations. So if you're like an SRE team and it's like, okay, what are all the commands that I need to be able to run in the

of a firefight, you can have that and they're all sort of in a common library that you have directly within warp. We don't have the feature yet of like intelligently make these for me from a session, but that's a super smart feature. We do have a thing that we're haven't launched, but are experimenting with, which is like essentially like run the output of your command through an LLM and have it summarize for you and pick out the interesting and important parts.

But I like your idea, Jillian. Figure out what I did, record it for me so I can do it again. Smart.

Warren (1:09:04)
So

I found that some people have sworn by this. If you're running a context session at the end, just tell it to echo back at you what you did. Say, what did I do? And then when it's done, then say, OK, now I want you to take that and write a document for me that includes that information so that the next time I have this problem, I can go and reference it. And with warp, can say, OK, now turn that into a templated command for warp. Yeah.

Zach (1:09:08)
Yeah.

Yes.

You could totally do

that today in warp. The one piece of it that's not like, we don't tie the loop of turning it into this specific, like executable thing that is a workflow. you know, we also have like a notebook concept in warp. So you could be like, hey, LLM in warp, summarize everything I did, turn it into a notebook, extract the relevant commands for me. But it's not quite as seamless, I think, it could be for Jillian. It's good idea.

Jillian (1:09:56)
Yeah, I'd really like to be able to have different... I don't know if it's sessions or contexts, but I suppose one of those where I can say... I don't know, I mean, I suppose for me it would be like client-dependent or context-dependent or even like tell me which environment I'm in, which version of Terraform I'm using, like, you know, all that kind of stuff. Save all of that and be like, it's here. It's right here.

Zach (1:10:02)
Yeah.

You can do that, for sure. That'll just work today. Yeah.

Yeah.

Jillian (1:10:23)
Well, that's what I want.

Zach (1:10:24)
Yeah, you can,

yeah. We don't, like, I think that's a super interesting idea. I mean, you can use Warp to learn anything about your environment today. So you can be like, what tool chain am I using? What are my environment variables? what, anything you can ask about your history. And so you can get some of that today, but you can't quite get, we don't have it like packaged up so that when you start a new session, you can get all that stuff, which would be cool.

Jillian (1:10:54)
Well, I would, if we're taking feature requests, I would like that. Um, you know, just, just saying.

Zach (1:10:57)
Yeah, sure. I'm going to force

Will (1:10:58)
Hahaha!

Zach (1:11:02)
everyone on our team to listen to this episode.

Warren (1:11:03)
Well, you should probably wait until

Will (1:11:05)
Hahaha!

Jillian (1:11:05)
Okay, great.

Warren (1:11:08)
the episode drops and then use an LLM to summarize the episode and extract the feature requests from it and just feed those.

Zach (1:11:15)
We could do it that way, or

I think there's been so much interesting discussion about philosophy of AI in here that I think I'm gonna make them all listen to it. I don't think it's a still summarized version is gonna do it justice. Yeah.

Warren (1:11:30)
I totally agree.

Jillian (1:11:31)
We need a human in the loop

here, Warren.

Will (1:11:33)
I'm gonna put him in a dark room and play it back to half speed.

Warren (1:11:37)
god.

Zach (1:11:37)
Ha ha ha

ha ha!

Warren (1:11:39)
I don't listen to content any slower than like two these days. I, before we get on another tangent, I have this feeling that we should move off onto, onto picks for the episode.

Jillian (1:11:40)
really torture them.

Zach (1:11:41)
Yeah.

Will (1:11:51)
It's probably a good part, good point, good time.

Good words. Look at me, workin' my words.

Warren (1:11:58)
Well then, Will, why don't

you go first?

Zach (1:11:59)
Heh.

Will (1:12:01)
Right on. Okay. So, I'm, I have a couple of picks today. one, I'm blaming you, Warren and Matt from last week because I got the book, dungeon crawler, Carl, and I hate how much I like this book.

It's it's dumb and it's funny and it's entertaining and it's engaging and it sucked way too much of my time last week. So Dungeon Crawler Carl, I can't even remember who the author is. Do you remember, Warren? No. All right. Yeah. Just Google Dungeon Crawler Carl. It's a stupidly fun book. Very entertaining.

Warren (1:12:40)
I didn't look it up.

Well,

if you're listening to this episode, the link will be included with the podcast just, you know, down below. So you don't even have to Google it, just click the link.

Will (1:12:53)
yeah, there you go.

Right. And then the other pick I'm going to recommend is if you haven't, Zach, you mentioned this earlier, if you haven't gone to your favorite AI tool and just started a chat about philosophy with it, I highly recommend that. And that's going to be my pick for the week because it's just, it's so much fun to do. And

Warren, I know you said that AI is not intelligent, but neither are some of the people I hang out with. So chatting with AI about philosophy seems to be working out quite well because it's just a really cool perspective of some of the stuff that it has and some of the insights it has to offer. And I've used it for setting goals as well and challenging me on those goals. And it's been pretty insightful for that. So I think that's one good way to start working with AI.

Zach (1:13:27)
Ha ha ha ha!

Jillian (1:13:29)
Fine.

Will (1:13:53)
And those are my picks.

So Jillian, about you? What'd you bring this week?

Jillian (1:14:00)
I'm going keep going with the self promotion until I'm back up to the lifestyle with which I've become accustomed. And if you go to my website, yeah, yeah, that's right. dabbleofdevops.com slash AI. have a data discovery tool for mostly for data science companies. If you're not a data science company, like I don't, I don't like even really know how to talk to you. So maybe just ignore this portion.

Will (1:14:05)
Right on.

Jillian (1:14:23)
But the idea is that you get your data, you load it into the LLM, and then you can start asking your questions. It kind of acts like maybe like a junior grad student. You don't want to like completely trust what it says, but it gives you a very good first draft. I'm adding the PubMed interface so you can go search medical literature and say like, okay, get me all the papers back on this disease or this protein or this drug interaction, know, whatever the things are.

Will (1:14:23)
You

Jillian (1:14:50)
load that into the LLM, start asking you questions, I've got a couple different datasets, open targets, a couple single cell datasets, I want to add a couple transcriptomics datasets, even though those might be out of vogue because they're still cool you guys, okay? They're still cool. So anyways, cool things are being added to the platform. Anybody wants it, mostly, in the biotech space. Again, if you're not biotech, don't really... I don't even know why you're listening to me, like, just tune me out, it's fine.

Will (1:15:17)
I'm

Warren (1:15:17)
Don't

reduce your TAM, your total addressable market here. If you don't understand what Jillian's saying, maybe you should go to the website anyway and see if you can figure out a use case for yourself.

Jillian (1:15:27)
That's true. You could. I do have some companies that use it just for meeting notes. They have something like Otter record all of their meetings and then Otter kind of gives them like, you know, the different summaries and images and things like that. It's pretty cool. And then you can feed that into the LLM and have sort of like a just a history of meetings. So then you don't have this, didn't we have a meeting about this? Didn't somebody make a database? Like, wasn't, wasn't there a thing? Wasn't there a person we can talk to here? You could just go and query it and then, and then it will tell you.

Zach (1:15:27)
Ha ha.

Warren (1:15:32)
There you go.

Jillian (1:15:57)
Sometimes it gives you the answer you want and sometimes it's like, no, that conversation never happened. You're hallucinating now. But you know, like it's, either one. It's one or the other. And then.

Will (1:16:06)
Well, there's a big overlap between

biohackers and software engineers as well. So that they may find that interesting.

Jillian (1:16:16)
Yeah, they could put all the literature and data in there around biohacking that I'm not totally familiar with. Although I am very much looking forward to having like bionic limbs, that would be great. Like that would just be amazing for me.

Will (1:16:28)
you

Warren (1:16:31)
Because

you want it that you don't have to think about moving your limbs anymore. You want something else to do it for you, right?

Jillian (1:16:37)
No,

I just want limbs that work at this point. Like that would be nice. That's like just on a mechanical level. Like that's what I need. And then, you know, and then on that note, we're kind of talking about like philosophy of AI and so on. And you know, and we can kind of argue about the tools, but from an accessibility viewpoint, AI is really great and doing some really great things. You know, like I have some issues with typing as I age out of this career field.

you know, I have some like low vision people in the family that AI is very helpful for them being able to dictate being, you know, there's like, there is kind of a lot of cool accessibility things that can be done with AI. And I do always kind of like to give a little bit of a shout out to that. Cause I, I do think it's like all of that is pretty great. You know, like I have somebody who's low vision who can now listen to audio books and, know, basically like kind of still go through the internet just with voice. And I think that's pretty cool. So, I don't know.

That's it. That's my picks.

Will (1:17:34)
Alrighty then.

Warren (1:17:36)
So.

For my pick this week, I primed it at the beginning. It's this Microsoft-backed research paper that came out of Carnegie Mellon, The Impact of Generative AI on Critical Thinking. And I think it's just absolutely fantastic paper about the correlations between utilizing AI tools and developing critical thinking processes and expanding in usage of that sort of brain muscle. And I think some people have misinterpreted the paper as, Microsoft paper on AI is making us stupid.

but I think the one thing that really does come out of it is that if you have low confidence in an LLM doing the right thing

you will be able to get much better answers out than if you have high confidence in the current tools that we have because the current tools are transformer networks that hallucinate. And if you just assume that it gives you the right answer like your calculator, you are going to stop developing the muscle of challenging where you got the information from and trying to understand it.

I will say that this leads me to a great interview question. know that interviewing candidates today can be challenging because they may be using LLMs to answer your questions. And for me, I think that naturally you can just ask them, how much confidence do you have in the LLMs that you use to produce the right answers? The more confident they are, the more likely you know they're not using critical thinking to challenge what comes out of them. It could be a useful litmus test for what sort of person

and you're hiring into your organization.

Will (1:19:14)
Right on. And so by, are you phrasing the question that way, just presuming that they are using AI to make them more comfortable with admitting that they are, rather than trying to hide it?

Zach (1:19:27)
you

Warren (1:19:28)
Well, I think realistically, part of our interviews now should be dedicated to solving problems that don't rely on using LLMs.

or problems that can use LLMs to be solved better and then asking them to use LLMs and which LLMs they're utilizing to solve the problem and how they're going about it. Because I think where you're trying to hide this perspective from yourself, you're lying to yourself if you believe that you don't want to pull these tools into your company to utilize in some fashion and that people aren't utilizing them irrelevant. If you give them a take-home assignment,

for your company that takes four hours or eight hours, some of them are going to utilize tools. And I don't think it says a lot on the type of person based on whether they utilize the tools, but it does say something about them about how they're utilizing them or what their expectations are on how they utilize those tools.

Will (1:20:29)
Cool. All right, Zach, what'd you bring for a pick?

Zach (1:20:33)
I have an AI tool that I like. Why not? So it's a tool called Granola and it's a note taker. It's a meeting note taker that uses AI. But the thing that I like about it compared to all the other ones that I've tried using is that you don't end up with like a little like black box in your Zoom.

Will (1:20:35)
Yeah

Zach (1:21:00)
for the note taker. The note taker works just off of your computer audio. And so there's no like, this is weird, who is this like Zach's note taker thing joining the meeting? it, not as it take notes, it doesn't like the default way that it takes notes isn't by transcription, it's by like semantically summarizing.

Will (1:21:05)
nice.

Zach (1:21:24)
and giving you the key points of what happened in the meeting. So I don't like taking meeting notes, so this is a cool thing. It's called granola. So that's one thing. A second thing, I'm reading a book. It's pretty nerdy. I don't know why I'm reading it. It's called a travel guides of the middle ages. And it's a history book. And it's all about from the year like 1100 to 1500, how do people travel?

Like, what was it like for them to take a vacation? They weren't really taking vacations. They were primarily going on pilgrimages. Or at least that's what the written record survives. And it takes you all over Europe, the Middle East, and the Near East. I'm not through it yet, so I don't totally know where. But to me, what I like about it from a history perspective is that it's just about like...

It's about a relatable experience, not about a series of historical events. It's not about historical leaders. It's about, say you happen to be living in the year 1300, what the heck were you doing? How did you pack? How did you travel? Where did you stay? What were the inns like? What were you trying to go sightsee at? I don't know why I like it so much, but it's like, I really like it. It's like a...

puts me in a very different mindset from like how we're living today. So I like that book. Yeah.

Will (1:22:51)
That's super cool. It's like

national lampoons, middle ages vacation.

Zach (1:22:56)
Yeah, except I

guess it didn't seem like very funny to be traveling then. It was a lot of like very serious, got to get to this religious site, like seemed like, and you got to see these relics. Like people were really wanting to see a bunch of, you know, historic relics, or at least that's what the written record survives. And that's where the history comes from. So that's pretty cool.

Jillian (1:23:21)
I

used to really like all those, like, the diary type books, like they're fiction, but they're sort of written as diaries of like the kids that would do the Oregon Trail and travel across the United States and they're from like other places as well too, so you have people come in to Plymouth Rock and doing the Oregon Trail and just sort of, yeah, in general. People go in different places, like across history. It used to be a lot harder. You used to have to worry about more things than like if the gas station has your preferred chicken tenders or like whatever, you know?

Will (1:23:22)
Right on.

Zach (1:23:46)
yeah.

Yeah, it's... That's hilarious.

Warren (1:23:53)
There are so many questions, Jillian.

Will (1:24:01)
Awesome. Zach, thank you for joining us. This has been a super entertaining episode. Yeah, really appreciate it.

Jillian (1:24:06)
Yeah, this has been fun.

Zach (1:24:08)
I thought it was great.

I thought it was super interesting conversation and like, it was, yeah, it was fun. I really appreciate you all having me on here.

Will (1:24:15)
For sure. I'm going to challenge Jillian to go download Warp, try it out, and then invite you back on the show for a head-to-head rematch.

Jillian (1:24:23)
going to her best of

place. Like that's the one thing I really want. So there we go.

Zach (1:24:28)
It's at warp.dev is where you get it and it's now available Mac, Linux and Windows. So all platforms currently available.

Will (1:24:39)
Right on.

Cool, well thanks everyone. Zach, thank you again. Warren, Jillian, thank you. And we'll see everyone next week.

Jillian (1:24:48)
Thanks guys.

