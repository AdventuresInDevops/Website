1
00:00:00,044 --> 00:00:02,235
Welcome back to Adventures in DevOps.

2
00:00:02,235 --> 00:00:03,605
I'm your host Warren.

3
00:00:03,605 --> 00:00:08,866
And because today will is away this week, I have an opportunity to sneak in a sponsorship.

4
00:00:08,866 --> 00:00:10,762
Today's episode is sponsored by Attribute.

5
00:00:10,762 --> 00:00:18,232
I actually met the team and honestly what they're doing in the FinOps space is absolutely
genius that I believe actually everyone can benefit from.

6
00:00:18,232 --> 00:00:20,813
They call it FinOps without tagging.

7
00:00:20,813 --> 00:00:27,361
It's the first runtime technology that analyzes infrastructure instead of relying on
billing reports, exports, and tagging.

8
00:00:27,361 --> 00:00:37,384
It's for architecture, ops, and platform teams that need visibility into product, customer
attribution, or insight into cost anomalies without wasting hours of guessing how to

9
00:00:37,384 --> 00:00:39,535
allocate spend to shared services.

10
00:00:39,535 --> 00:00:45,216
So there's no spreadsheets, no extra logging, attribute solves it all with just one line
of code.

11
00:00:45,216 --> 00:00:55,245
They capture costs based on actual application usage generated from anywhere, Kubernetes,
databases, storage, and over 35 multi-cloud services.

12
00:00:55,245 --> 00:01:01,420
In their UI, they break it down by microservice and even attribute cost at the database
query level, all tied back to the business.

13
00:01:01,420 --> 00:01:03,862
I really find that pretty interesting.

14
00:01:03,983 --> 00:01:06,595
Recently, they were recognized by six Gardner hype cycles.

15
00:01:06,595 --> 00:01:11,909
I honestly have no idea what that is though, and are working with impressive companies
like Akamai and monday.com.

16
00:01:11,909 --> 00:01:20,005
So you'll want to check them out and I'll drop a link in the description for the episode
that's attribute at attrb.io and now back to the show.

17
00:01:20,432 --> 00:01:31,075
And today, I have to say I'm actually pretty intrigued by the guests that we brought on
because this is an area of technology that I have zero experience with.

18
00:01:31,075 --> 00:01:33,263
We're gonna be talking all about vector databases.

19
00:01:33,263 --> 00:01:40,325
And I feel like we brought in one of the experts from the industry, from a company that
has been doing vector databases for quite some time now.

20
00:01:40,325 --> 00:01:43,386
I wanna say since the beginning, but I think she's gonna correct me.

21
00:01:43,386 --> 00:01:45,299
So welcome to the show, staff.

22
00:01:45,299 --> 00:01:47,655
Developer Relations, Jenna Patterson.

23
00:01:47,655 --> 00:01:50,002
Hello, thank you for having me on today.

24
00:01:50,002 --> 00:01:58,851
I know I'm really interested because actually in part of preparing for this episode, I
went around and asked a lot of my colleagues at different companies if they had any

25
00:01:58,851 --> 00:02:08,520
questions that I should ask someone who's, I mean, you sort of just got into the work at
Pinecone there just under a year if I'm right, ah what I should ask them.

26
00:02:08,520 --> 00:02:10,478
And they're like, I don't know what that is.

27
00:02:10,478 --> 00:02:10,838
Yeah.

28
00:02:10,838 --> 00:02:15,330
So high level, it allows you to compare vector embedding.

29
00:02:15,330 --> 00:02:18,330
numerical representation of a piece of data.

30
00:02:18,330 --> 00:02:23,297
And so And the vector database allows you find similar matches.

31
00:02:24,065 --> 00:02:33,060
If you think about searching on, for instance, an e-commerce store, retail store online,
and you want to find all the shirts that are red.

32
00:02:33,060 --> 00:02:35,652
might not all have the word shirt in the description.

33
00:02:35,652 --> 00:02:41,745
So we want to find everything that is related or closely related, the most closely
related.

34
00:02:42,165 --> 00:02:46,275
And so it does that based on a distance metric ah to find

35
00:02:46,503 --> 00:02:51,819
everything that has a meaning similar to shirt ah or in this case a red shirt.

36
00:02:51,819 --> 00:02:56,003
So it could be a blouse, it could be a top, it could be a short sleeve shirt.

37
00:02:56,003 --> 00:03:07,134
ah And then it comes back as scored result so that anything that is close is going to have
a better score than the things that are further away from that particular query.

38
00:03:07,134 --> 00:03:07,434
I see.

39
00:03:07,434 --> 00:03:17,169
So you take the original request, so red shirt, and you're converting it to some just a
set of numbers and then using that to look through the database and get back uh equivalent

40
00:03:17,169 --> 00:03:17,409
results.

41
00:03:17,409 --> 00:03:28,826
So there's some upfront converting being done, I assume, when you're sticking data into
the database in order to store the numerical representations, not just the raw, say, text

42
00:03:28,826 --> 00:03:29,593
properties.

43
00:03:29,593 --> 00:03:30,573
Yes, exactly.

44
00:03:30,573 --> 00:03:33,776
So at the beginning, you're going to ingest all of your data

45
00:03:33,776 --> 00:03:39,287
So you chunk it up, and then you absurd it into a vector database uh ahead of time.

46
00:03:40,838 --> 00:03:49,930
your users are querying, we take that query, we embed that query, and use that as a way to
compare against those existing embeddings that are in the database.

47
00:03:50,591 --> 00:03:54,556
There's also kind of another piece about that, if that data changes,

48
00:03:54,556 --> 00:03:57,557
ah or you get new data, can also re-up-sert that

49
00:03:57,557 --> 00:04:03,626
I So, I mean, this I assume is true for all databases that claim that they're vector
databases.

50
00:04:03,626 --> 00:04:09,829
How do you do the computation of figuring out what the numerical value should be for a red
shirt?

51
00:04:12,120 --> 00:04:11,880
we do that through what is called an embedding model.

52
00:04:11,880 --> 00:04:16,940
but this embedding model is trained on specific data that is for embedding.

53
00:04:16,940 --> 00:04:21,102
And we pass in the text value, so that chunk of data or a piece of text.

54
00:04:21,102 --> 00:04:22,363
In this case, it's text.

55
00:04:22,363 --> 00:04:24,284
it spits out some numbers.

56
00:04:24,284 --> 00:04:25,815
And it happens to be in vector form.

57
00:04:25,815 --> 00:04:28,186
So if you remember back to like,

58
00:04:30,171 --> 00:04:30,272
I don't know, fifth or seventh grade geometry.

59
00:04:30,353 --> 00:04:31,704
We worked with vectors.

60
00:04:31,704 --> 00:04:34,236
It's essentially a list of numbers.

61
00:04:34,236 --> 00:04:38,679
But these are very, very long vectors, so very high dimensional data.

62
00:04:39,332 --> 00:04:41,794
1024 dimensions in this vector.

63
00:04:41,834 --> 00:04:47,139
And these represent different pieces of meaning about that piece of data.

64
00:04:47,139 --> 00:04:49,761
um So it could be about the color.

65
00:04:49,761 --> 00:04:51,741
could be, in this case, like we're

66
00:04:51,741 --> 00:04:56,835
outside of the query, like the data, we might embed uh the product description.

67
00:04:56,835 --> 00:05:00,086
We might embed product title or all of that together.

68
00:05:00,086 --> 00:05:04,635
That could be part of our chunking strategy is to put it all together and embed that whole
thing as one piece.

69
00:05:04,635 --> 00:05:06,510
I just have like a lot of questions now.

70
00:05:06,510 --> 00:05:14,594
Like first and foremost, is it your fault that when I search for red shirt on websites, I
find things that aren't red shirts now?

71
00:05:14,978 --> 00:05:16,764
I hope it's not our fault.

72
00:05:17,488 --> 00:05:29,848
I understand you correctly that actually the model, I assume you're using like something
similar to an LLM to convert from the original text into what the embedding value should

73
00:05:29,848 --> 00:05:31,708
be that you're storing in the database.

74
00:05:32,248 --> 00:05:34,415
You're doing that because Pinecone has this capability.

75
00:05:34,415 --> 00:05:41,858
But if you're using, let's say, I don't wanna say one of your competitors, but an open
source vector database or I think Postgres supports vectors now,

76
00:05:41,858 --> 00:05:48,144
That responsibility is on the implementer of or the team that is actually implementing
search in their application, right?

77
00:05:48,144 --> 00:05:51,726
uh Yeah, so there's a couple different approaches to it.

78
00:05:51,726 --> 00:05:54,037
At Pinecone, we have two different approaches.

79
00:05:54,037 --> 00:05:55,588
We support two different ways.

80
00:05:55,588 --> 00:05:59,350
So you might actually have your own embeddings already.

81
00:05:59,670 --> 00:06:02,815
You might want to manage that part of the process yourself.

82
00:06:02,975 --> 00:06:10,139
And so you might use something like an OpenAI model to do embedding or an Amazon model to
do embedding.

83
00:06:11,480 --> 00:06:11,277
We also host our own models.

84
00:06:11,277 --> 00:06:12,995
It's an NVIDIA model for embedding.

85
00:06:12,995 --> 00:06:16,597
we have a number of those uh based on your use case.

86
00:06:18,440 --> 00:06:19,892
right now, we're talking about text, like product descriptions, product titles.

87
00:06:21,014 --> 00:06:20,702
audio.

88
00:06:20,702 --> 00:06:23,941
It could be images, any sort of data.

89
00:06:23,941 --> 00:06:30,587
that you want to actually do a meaningful search over and find meaning as opposed to
specific keywords

90
00:06:42,588 --> 00:06:33,760
is the comparison being done like in the database side?

91
00:06:33,760 --> 00:06:42,407
like I get the part where you run it through an embedding model and you get back an array
of 1024 integers or floating point numbers.

92
00:06:42,407 --> 00:06:44,827
that somehow gets stored in a database.

93
00:06:44,827 --> 00:06:47,868
Is it being stored as a row value?

94
00:06:47,868 --> 00:06:51,921
Is there a special format that the database saves data in?

95
00:06:51,921 --> 00:06:54,887
Is this just an incredibly complicated question to answer?

96
00:06:54,887 --> 00:06:58,906
This is for me, it's an incredibly complicated question to answer, but I can answer part
of it.

97
00:06:58,906 --> 00:07:05,291
the comparison that's being done, that we are taking those two vectors and seeing how far
away they are from each other.

98
00:07:05,291 --> 00:07:14,715
So if you think of a vector like this representing, you know, the red shirt and a vector
going like this representing like pants.

99
00:07:15,155 --> 00:07:17,259
But maybe you have something a little bit closer.

100
00:07:17,259 --> 00:07:22,725
It's really hard to do this as a podcast and just with my hands, ah visuals are much
better.

101
00:07:24,428 --> 00:07:28,887
for those of you who are just listening, Jenna is attempting to draw vectors with her
arms, and if that somehow makes sense.

102
00:07:28,887 --> 00:07:35,722
But I totally get like you have a triangle of vectors, and you're calculating the
difference between those vectors from each other.

103
00:07:35,722 --> 00:07:40,363
And that's how you're calculating distance, and I assume you're optimizing for the
smallest distances possible.

104
00:07:40,363 --> 00:07:43,815
one of the questions I have is like, you have a lot of these embedding.

105
00:07:44,191 --> 00:07:46,112
vectors in your database.

106
00:07:46,112 --> 00:07:54,059
And aside from how they're actually being stored, you still have to fetch some set of that
data optimally rather than fetching all the data.

107
00:07:54,059 --> 00:07:58,803
I assume rather than fetching the entire database in order to compare each vector one by
one.

108
00:07:58,803 --> 00:08:07,025
Any thoughts of how you are able to pare down the total amount so that you're only data in
order to do that comparison?

109
00:08:09,408 --> 00:08:09,159
this is definitely beyond my knowledge.

110
00:08:09,159 --> 00:08:16,244
um I will say there are strategies for how it's being stored and close the data is.

111
00:08:16,244 --> 00:08:23,666
it's like the thing where I definitely try to pose this to people who come up with new
database formats, because there's always some in the space.

112
00:08:23,666 --> 00:08:26,796
when I asked them about it, they were like, yeah, we built a new database format.

113
00:08:26,796 --> 00:08:35,293
And I'm like, well, yeah, but really what you did was just use your underlying database
engine, and you just put a service on top of it, and you're calling it a database.

114
00:08:36,525 --> 00:08:41,526
like it's really not at the end of the day it's just a relational database what I really
understand here is that it's fundamentally different how you're storing the data it's not

115
00:08:41,526 --> 00:08:51,078
in arbitrarily row based information or know binary blobs that are being stored which can
be fetched fundamentally the vector database is storing data not only in a different way

116
00:08:51,078 --> 00:08:58,012
but has to be optimized in order to find locality of these vectors and the the ones that
are close in distance.

117
00:08:58,012 --> 00:09:06,284
I'm not a database expert, honestly, what you shared so far is still fine by me, but I'm
sure someone will call me out on it.

118
00:09:08,693 --> 00:09:11,129
I do think it's interesting in that like, fun to understand like the underlying
technology.

119
00:09:11,129 --> 00:09:15,049
And I'm like, as someone who has been here for six months, I'm still learning that.

120
00:09:15,049 --> 00:09:16,657
It's very complicated.

121
00:09:16,657 --> 00:09:20,857
our customers are like, they're learning it along with us as well.

122
00:09:20,857 --> 00:09:22,874
And like, kind of what the strategies are so that

123
00:09:22,874 --> 00:09:25,799
they can implement it in the best possible way, right?

124
00:09:25,799 --> 00:09:27,379
I think this is the right word.

125
00:09:27,379 --> 00:09:37,111
If you have an application where historically you would have done something like free text
searching in Elasticsearch, using an embedding model to calculate the numerical values to

126
00:09:37,111 --> 00:09:42,051
handle a semantic search is just the progression in the industry.

127
00:09:42,051 --> 00:09:43,531
You no longer need free text search.

128
00:09:43,531 --> 00:09:51,303
This I don't want to say be all end all of the future of e-commerce websites, but it does
seem like there's just such an improvement.

129
00:09:51,303 --> 00:09:53,892
in the strategy here from what was done before.

130
00:09:53,892 --> 00:09:57,753
it is not necessarily that you're not going to use a keyword search.

131
00:09:57,753 --> 00:10:01,074
You might actually pair them together, and I'll talk about that in a second.

132
00:10:01,115 --> 00:10:04,696
then e-commerce is just a simple example.

133
00:10:04,696 --> 00:10:12,845
It's the use case I typically start out ah But there are other reasons why you would use
this type of search, semantic search.

134
00:10:13,854 --> 00:10:22,825
For instance, in your AI applications where you are getting your maybe you're chatting
back and forth with a model, but the model doesn't it has its own limitations,

135
00:10:22,825 --> 00:10:35,490
As we know now, semantic search is about the meaning behind your query and your intent
behind what you're trying to find um based on what context the data is in.

136
00:10:35,490 --> 00:10:47,933
um But sometimes you have keywords, or you have domain-specific language, or acronyms, or
stock tickers is another common one that we use as an example.

137
00:10:47,933 --> 00:10:49,034
within your company,

138
00:10:49,034 --> 00:10:50,866
You have product names.

139
00:10:50,866 --> 00:10:55,329
You have your own company-specific language.

140
00:10:55,549 --> 00:11:02,023
You have technical terms that might not be in the public domain and might not be trained
into those models.

141
00:11:02,023 --> 00:11:09,657
we might pair a semantic search with what is called a lexical search or a keyword search
in order to make those results even more accurate, even more

142
00:11:09,657 --> 00:11:10,638
correct

143
00:11:21,881 --> 00:11:17,657
think you really stumbled onto a good thing here that's worth talking about because I
think where you're going is for those of you who are unfamiliar or may have heard of RAG

144
00:11:17,657 --> 00:11:27,071
before, I think you're really jumping onto the retrieval augmented generation where
fundamentally, if you think about it, the models that you're using that are proprietary by

145
00:11:27,071 --> 00:11:31,993
third party companies don't understand how you talk about your business.

146
00:11:32,013 --> 00:11:37,414
And so how do you do the mapping from one of these models to what's internally?

147
00:11:37,414 --> 00:11:49,438
And what you're saying is if you've uploaded all the data or you have hooks into your
knowledge bases and you throw that through an embedding model into Pinecone or another

148
00:11:49,438 --> 00:12:01,685
vector database, using the magic of MCP or some other magic that no one knows about,
somehow the model gets access to this data and understands how to

149
00:12:01,685 --> 00:12:13,338
use the embedding values from their own model to map to what's internal because there is a
semantic likeness between those things even though fundamentally the actual words and even

150
00:12:13,338 --> 00:12:16,463
outside in say a dictionary those things are fundamentally different.

151
00:12:16,463 --> 00:12:17,533
Yeah, exactly.

152
00:12:17,533 --> 00:12:25,550
think one way I like to look at it is like we, you and I, like we have a spoken language,
we speak English, it's natural to us.

153
00:12:25,550 --> 00:12:35,063
And so like if we can interact with our data and gain insights and find information using
the language we know the best, that's gonna be even better for our output.

154
00:12:35,063 --> 00:12:39,917
LLMs have limitations and so, and part of that is it doesn't know all about our data.

155
00:12:39,917 --> 00:12:49,809
And so if we bring in the knowledge, bring in the factual data, the authoritative data
into that process, then our output can be even better.

156
00:12:49,809 --> 00:12:56,706
You may be the first person I've talked to that suggested that English was like a good
strategy for communication here.

157
00:12:56,706 --> 00:12:59,078
I don't know if I'm suggesting it's a good strategy.

158
00:12:59,078 --> 00:13:01,575
I think it is what humans.

159
00:13:01,693 --> 00:13:10,330
ah I mean, there's just so much that's not shared as far as the context goes uh that is
just fundamentally lost.

160
00:13:10,330 --> 00:13:17,886
And I can see how troubling it is to actually communicate in that way because we each have
our own internal view of the world.

161
00:13:17,887 --> 00:13:28,195
And I know as a technologist, a long time technologist, a non-trivial amount of my
conversations have pivoted from talking about whatever the topic is to like a meta level.

162
00:13:28,195 --> 00:13:30,353
Like, what are we actually talking about here?

163
00:13:30,353 --> 00:13:32,715
you know, let's define some of these words.

164
00:13:32,715 --> 00:13:42,872
There was a recent conversation I was having about the effectiveness of feature flags and
what came up, it started with like, well, how do you use them evolved into, it good to use

165
00:13:42,872 --> 00:13:43,573
them?

166
00:13:43,573 --> 00:13:46,155
And even if I say that, it's like, what does good mean?

167
00:13:46,155 --> 00:13:52,499
And one of the terms that came up was, yeah, you know, if you have everything that's fully
tested and I'm like, what does fully tested mean?

168
00:13:52,499 --> 00:13:54,348
Like, how do you actually define that?

169
00:13:54,348 --> 00:14:00,206
regard, feel like it's a very well, I know it when I see it, but defining it is hugely
problematic.

170
00:14:00,206 --> 00:14:13,472
Super hard, I remember back in one of my college courses a long, long time ago where we
talked exactly about this and how you a specification everyone understands ah and could

171
00:14:13,472 --> 00:14:15,414
potentially be executed, right?

172
00:14:15,414 --> 00:14:20,712
So it's not just that you and I can talk about it, but that our computer understands it.

173
00:14:25,165 --> 00:14:22,444
So I think you bring up a really good point.

174
00:14:22,444 --> 00:14:31,509
I think you're right in that it depends on who we are and what our experiences are and
where we come from and what language we speak.

175
00:14:31,697 --> 00:14:33,738
be some of that that happens.

176
00:14:33,738 --> 00:14:35,497
And I there are ways around that.

177
00:14:35,497 --> 00:14:38,868
And I will say right now, I don't know all of those ways around that,

178
00:14:38,868 --> 00:14:50,841
something that I think frequently comes up on the show is the mentioning that each new
release of a model public or proprietary is like having another child where they're like

179
00:14:50,841 --> 00:14:52,182
so fundamentally different.

180
00:14:52,182 --> 00:14:54,732
It's not like an upgrade is fun.

181
00:14:55,092 --> 00:15:04,734
And so I think you brought this up a little bit is the idea where if you're upgrading
your, I say upgrading, side grading your model for one reason or another that

182
00:15:04,734 --> 00:15:14,643
revalidating the embeddings that you got previously matched was coming out of the new
model to ensure that you aren't just gonna start getting nonsensical outputs.

183
00:15:14,643 --> 00:15:22,461
mean, in some way you're upgrading the model because you think the new embeddings will be
better, but that obviously has an opportunity for unexpected Does Pinecone have a strategy

184
00:15:22,461 --> 00:15:23,622
for dealing with that?

185
00:15:23,622 --> 00:15:28,623
I I assume if you have an embedding model, given what the company is doing, you're
building the model yourself.

186
00:15:28,623 --> 00:15:34,164
the Nvidia model, that one that I mentioned, that's a hosted model that we have, we host
it internally.

187
00:15:34,264 --> 00:15:38,085
We also have our research team and they have created models for us.

188
00:15:38,085 --> 00:15:41,709
So we have, The one that I am most familiar with is Pinecone Sparse.

189
00:15:41,709 --> 00:15:50,290
is used for a lexical search, keyword search, with sparse vectors as opposed to dense
vectors which are used for semantic search.

190
00:15:50,621 --> 00:15:52,299
What's the difference between them?

191
00:15:57,488 --> 00:15:57,153
um a dense vector, you have, a vector of numbers, they represent different parts of...

192
00:15:57,153 --> 00:16:00,974
of meaning about that particular piece of data that was embedded.

193
00:16:02,335 --> 00:16:05,944
But with a sparse vector, you have more zeros than you have actual numbers.

194
00:16:05,944 --> 00:16:12,281
And it is either a zero or a one that essentially represents, the frequency of our
particular word.

195
00:16:12,281 --> 00:16:17,265
I think realistically, you, no one wants to listen to this.

196
00:16:17,365 --> 00:16:23,973
You think back to when you had, you went further and you're like normalizing arrays ah
where you,

197
00:16:23,973 --> 00:16:34,227
you create some sort of orthonormal basis for actually moving the data out so that you end
up with these arrays where you have just an amplitude at one position in the vector, which

198
00:16:34,227 --> 00:16:38,009
makes it much easier to identify things in close proximity.

199
00:16:38,009 --> 00:16:48,633
So you can imagine that you're not just letting the embedding model come up with arbitrary
numbers to represent your semantic text, but you're then applying some clever mathematics

200
00:16:48,633 --> 00:16:52,669
on top of it to organize the vectors in a way so that

201
00:16:52,669 --> 00:16:58,962
when you actually go and do a search, you're not having to pull out every single piece of
data from the database.

202
00:16:58,962 --> 00:17:06,812
being specific about how you can utilize the mathematics to optimize your vector database
goes into some of the creation of these.

203
00:17:12,154 --> 00:17:10,431
a lot of engineers got into, I'd say software engineering, or became engineers in the
first place,

204
00:17:10,705 --> 00:17:15,199
they told themselves, and I'm going to say a lie, that they wanted to work on hard
problems.

205
00:17:15,199 --> 00:17:24,357
And it does actually sound like that building, like compared to building up other
databases, I do feel like understanding the mathematics behind a vector database is not

206
00:17:24,357 --> 00:17:24,795
trivial.

207
00:17:24,795 --> 00:17:26,244
ah Yeah, I agree.

208
00:17:26,244 --> 00:17:29,410
I am that person who like, I want to work on our problems.

209
00:17:29,410 --> 00:17:30,790
I like knowing how things work.

210
00:17:30,790 --> 00:17:35,200
And so again, I'm I'm still learning a lot of this, obviously.

211
00:17:35,200 --> 00:17:39,881
think for me, that is one of the fascinating pieces about it is that it's math.

212
00:17:39,881 --> 00:17:41,161
it's not easy math.

213
00:17:41,161 --> 00:17:45,032
mean, like we we go to school for it and people study it for a really long time.

214
00:17:45,032 --> 00:17:48,533
maybe the more fascinating piece is that we can actually use math to do that.

215
00:17:48,533 --> 00:17:57,044
we're not discarding lessons learned of the past, actually figuring out how to use the
physics and specifically in this case, the theoretical application in a real world

216
00:17:57,044 --> 00:17:58,214
scenario.

217
00:18:04,074 --> 00:18:05,334
You know, one of the things that I originally had thought of when we were talking about
vector databases is that like, surely this is only applicable to LLMs.

218
00:18:05,334 --> 00:18:12,783
But I feel like said that the e-commerce example that you brought up that really is about
doing semantic search is like a simple example.

219
00:18:12,783 --> 00:18:16,125
And I don't know if I agree with you, actually.

220
00:18:16,125 --> 00:18:21,988
I feel like getting the search right in e-commerce is like literally the most complicated
example of search.

221
00:18:23,839 --> 00:18:29,767
when I say simple, mean it's simple for people to understand because as an example because
they shop online, right?

222
00:18:29,767 --> 00:18:30,357
Everyone shop.

223
00:18:30,357 --> 00:18:31,453
Most people shop online.

224
00:18:31,453 --> 00:18:32,126
I don't want to say

225
00:18:32,126 --> 00:18:33,067
I'm with you there.

226
00:18:33,067 --> 00:18:40,609
I think there is something where it does seem simple in the service, but like as soon as
you get to thinking about how the search actually works, it starts to get very

227
00:18:40,609 --> 00:18:41,239
complicated.

228
00:18:41,239 --> 00:18:51,221
think one of the examples that comes up a lot for me when I've reviewed other companies
database architecture, like which type of database they're going with, or what sort of

229
00:18:51,221 --> 00:18:54,142
indexes they have, it always starts as a generic problem.

230
00:18:54,142 --> 00:18:55,849
Like, I have some data and I want to search it.

231
00:18:55,849 --> 00:18:57,553
And I'm like, well, what kind of search are you doing?

232
00:18:57,553 --> 00:19:04,413
Well, we have some attributes on each of the rows of our data or the items and we want to
filter by the attributes.

233
00:19:04,413 --> 00:19:06,193
I'm like, well, which attributes do you want to filter by?

234
00:19:06,193 --> 00:19:13,713
it always like each item has three attributes and you always filter on the first attribute
and the second and third are never used?

235
00:19:13,713 --> 00:19:16,693
Or is it something common or something simple?

236
00:19:16,693 --> 00:19:24,893
And they're like, oh, no, it's always, well, the user could give us attribute one, two, or
three, and we need to figure out all of the items that match one, two, or three.

237
00:19:24,893 --> 00:19:25,755
And I'm like, well, that's

238
00:19:25,755 --> 00:19:30,658
pretty much just eliminated every single NoSQL database out there because, you know, good
luck.

239
00:19:30,658 --> 00:19:41,050
I mean, I will say in some scenarios, you can be very clever where you can just index all
three attributes, ah make three queries to your database, and then merge the results.

240
00:19:41,050 --> 00:19:46,627
I was just going to say, but also your products might all have different number of
attributes and different attributes.

241
00:19:46,627 --> 00:19:47,848
yeah, for sure, right?

242
00:19:47,848 --> 00:19:54,331
And actually think the one that's more complicated is that like some of the attributes
have like an array of values, right?

243
00:19:54,331 --> 00:20:02,486
So it's like, yeah, well, this product, I've used these commerce example, like this
product is a shirt, it can be in small, medium, and large, like the sizes that are

244
00:20:02,486 --> 00:20:03,076
available.

245
00:20:03,076 --> 00:20:11,991
And so when someone search sizes, you're not gonna have an attribute column that's like,
you know, small exists, you know, true or false, know, know, shirt can be in medium, true

246
00:20:11,991 --> 00:20:13,682
or false, like this is just nonsensical.

247
00:20:13,682 --> 00:20:14,391
So.

248
00:20:14,391 --> 00:20:20,766
I think once you see that you're like, well, okay, search and e-commerce that that's gotta
be like the most complicated thing ever.

249
00:20:20,766 --> 00:20:29,714
to say like, we've maybe collectively, we may have stumbled upon the limit for improving
search in that way by using an embedding model.

250
00:20:29,714 --> 00:20:37,981
We're taking the core, the essence of what the search query is, trying to figure out what
it is and then mapping it to an optimized way of storing the data and a database that

251
00:20:37,981 --> 00:20:42,018
isn't a row based where you're doing column by column matching.

252
00:20:42,018 --> 00:20:51,012
uh or some sort of non-efficient index lookup like in a NoSQL way where you're just like
somehow you know exactly which item the person is talking about.

253
00:20:51,012 --> 00:21:00,797
But I think that'd be pretty great reality get to where I just search for red shirt and I
get exactly the green trousers that I actually wanted

254
00:21:08,580 --> 00:21:11,448
I don't know if you're familiar with like the dupe trend, the duplicates of like high end
fashion, where these like lower end cheaper alternatives come up with dupes.

255
00:21:11,728 --> 00:21:13,720
So we see it a lot in fashion.

256
00:21:13,720 --> 00:21:16,241
It's in other other areas too.

257
00:21:16,241 --> 00:21:20,204
But what if you could search for like ah this?

258
00:21:20,204 --> 00:21:28,954
this high-end fashion brand name paired with like white t-shirt frilly edges of the
sleeves.

259
00:21:28,954 --> 00:21:35,639
And then you come up with like a different, the actual duplicate of that on this
particular site.

260
00:21:37,225 --> 00:21:41,287
I agree that like these products that you're talking about and the categories and the
different ways that

261
00:21:41,287 --> 00:21:46,985
It's not even just categories, but just the different uh features of them are very
complicated.

262
00:21:47,347 --> 00:21:55,630
But what if we could actually search for those based off of, you a few words that describe
what it is, but don't necessarily use those same words.

263
00:21:55,630 --> 00:22:03,960
you brought up, I think one of the major problems giant online search engines that are
dedicated consumer buying experience.

264
00:22:03,960 --> 00:22:09,200
I'll say, I think there's one that comes to mind in the Western world, which is Amazon.

265
00:22:09,200 --> 00:22:18,960
They have a huge problem with basically, I don't know if they call them duplicates as much
as counterfeits, where if you're not doing the manufacturing yourself,

266
00:22:19,060 --> 00:22:29,326
that you've sent the pattern to another company and in their off time when they have extra
resources, they just print out more of your item without the logo on it or with the logo,

267
00:22:29,326 --> 00:22:30,877
because they don't care.

268
00:22:30,937 --> 00:22:40,383
And then when you go and search on any website for the thing you're looking for, even with
the brand name, you're getting the competitor version that's just not as good.

269
00:22:40,543 --> 00:22:42,585
And people can't tell the difference most of the time.

270
00:22:42,585 --> 00:22:46,207
And this actually destroys a lot of small businesses that are doing this.

271
00:22:46,467 --> 00:22:48,108
I'm wondering if there really is,

272
00:22:48,540 --> 00:22:52,581
a smart strategy here where you can use this for fraud detection.

273
00:22:52,600 --> 00:23:01,524
And I know when I'm searching on Amazon, I always want like to filter by results where the
brand of the product matches the store where the product is coming from.

274
00:23:01,524 --> 00:23:03,626
Like that's almost always what I want.

275
00:23:03,626 --> 00:23:05,086
You make your thing.

276
00:23:05,086 --> 00:23:07,947
I don't care what your brand is, but it should match.

277
00:23:07,947 --> 00:23:10,610
And I always get suspicious when it's something else.

278
00:23:10,610 --> 00:23:11,950
thoughts of like the...

279
00:23:12,755 --> 00:23:13,675
you

280
00:23:13,843 --> 00:23:17,583
Yeah, you can see who uses Amazon more than most.

281
00:23:17,583 --> 00:23:18,743
Yeah, for sure.

282
00:23:18,883 --> 00:23:20,383
But I mean, there is an interesting thing here.

283
00:23:20,383 --> 00:23:31,243
Like, do you think that not just for search, generically for end users, are there primary
applications for a vector database outside of search that you see as like, this is just

284
00:23:31,243 --> 00:23:40,500
now a new strategy where if you're not using Pinecone or one of the competitors to do
this, you're really missing out on some of the core value that could be being provided.

285
00:23:40,500 --> 00:23:42,540
think you touch on kind of a hard problem.

286
00:23:42,540 --> 00:23:43,690
You mentioned fraud.

287
00:23:43,690 --> 00:23:58,554
know like, this is an area I don't know very much about, but I know that it is a use case
that we have seen people use vector databases for to find uh identifying patterns um of

288
00:23:58,554 --> 00:24:00,304
fraudulent use and that type of thing.

289
00:24:00,304 --> 00:24:02,860
I can see that that could be a thing.

290
00:24:02,860 --> 00:24:08,202
do you know what sort of customers are primarily that PyCon is looking for?

291
00:24:08,202 --> 00:24:13,331
Or the ideal customer profile that usually makes a good match?

292
00:24:13,331 --> 00:24:18,294
you not pay too much attention to the specific use case in its uh company segment or
vertical?

293
00:24:18,294 --> 00:24:22,858
Or is it you see something about their technology stack that is a good match for you?

294
00:24:22,858 --> 00:24:35,279
I'm working with developers to help them learn and understand, how to use pine cone, how
to use a vector database, how to incorporate retrieval into their systems.

295
00:24:36,580 --> 00:24:40,133
but I would say that like, one of the things we look for are people with, or companies
with

296
00:24:40,133 --> 00:24:44,295
um like large quantities of high dimensional data.

297
00:24:44,295 --> 00:24:55,088
So this is going to be your emails, your contracts, your PDF documents, oh images,
potentially video or audio.

298
00:24:55,088 --> 00:25:04,573
need to get insight from it, whether that is a search result as far as like the e-commerce
example we've been using, or it is...

299
00:25:04,717 --> 00:25:17,250
insights about a particular business unit and how they are operating or how they function
right before this, um I was reading one of our case studies about a company.

300
00:25:17,250 --> 00:25:26,393
it's about a uh medical company that's doing research on medications and are searching
over molecules.

301
00:25:29,636 --> 00:25:27,294
like that's a lot, right?

302
00:25:27,294 --> 00:25:33,600
And so they are essentially embedding those molecules as vectors and then doing searches
over those in order to

303
00:25:33,600 --> 00:25:39,134
gain insights and do research on their work in order to develop medicine.

304
00:25:40,476 --> 00:25:44,263
So I thought that was really interesting just in that it's not just the e-commerce
examples.

305
00:25:44,263 --> 00:25:54,150
I wanted to ask because you're on the other side, just like whether or not the ICP of the
potential customers, like it does match the types of questions or challenges that the

306
00:25:54,150 --> 00:26:02,320
engineers who come to community workspaces to like specifically ask questions about like
whether or not they're like already going in the right direction and the sorts of product

307
00:26:02,320 --> 00:26:04,291
areas they're focused on is a good match for that.

308
00:26:04,291 --> 00:26:07,854
Or you see people just trying to use vector databases in places that like

309
00:26:07,854 --> 00:26:10,859
have no reason to be used there whatsoever.

310
00:26:10,859 --> 00:26:12,756
there's probably a mix of both.

311
00:26:12,756 --> 00:26:23,424
see people who are, they've heard of a vector database, they've heard of Pinecone, and
they're like me, they're developer, they like knowing what's the new shiny thing.

312
00:26:23,424 --> 00:26:26,537
And so they want to learn about it, they want to learn how it might solve their problem.

313
00:26:26,537 --> 00:26:34,521
more ideal customers are people who are a little bit further in their journey, and so they
understand what is the purpose of it, what are some of the problems that...

314
00:26:34,521 --> 00:26:35,383
that they can solve.

315
00:26:35,383 --> 00:26:40,101
They understand that their problem fits in in some way.

316
00:26:40,101 --> 00:26:48,655
And we have a team that helps them figure out how it fits in and how to actually implement
it at production scale.

317
00:26:48,655 --> 00:27:00,003
Do you see them coming over from first the technical problem, then realizing they need a
vector database to store their semantic embeddings and then go to Pinecone?

318
00:27:00,003 --> 00:27:09,489
Or do you see it as a of a nuance play on top of generic databases or even open source
databases that do offer a vector database?

319
00:27:09,489 --> 00:27:13,192
And it's like, well, you know, if you're doing something in this space, you may be fine.

320
00:27:13,192 --> 00:27:18,115
But if you need something more robust or at scale like

321
00:27:18,285 --> 00:27:19,668
you would want to switch in a way.

322
00:27:19,668 --> 00:27:20,110
Do

323
00:27:20,110 --> 00:27:26,323
developers that I've been interacting with, I think they are first and foremost interested
in a piece of technology.

324
00:27:26,323 --> 00:27:29,494
They know that they have some data.

325
00:27:29,494 --> 00:27:33,536
They want to make their retrieval augmented generation pipeline more accurate.

326
00:27:34,917 --> 00:27:35,924
so they understand that retrieval is a piece of that.

327
00:27:35,924 --> 00:27:37,283
And this

328
00:27:37,283 --> 00:27:39,574
that's all I really know about it so far.

329
00:27:39,574 --> 00:27:49,080
uh There's definitely other teams that I work with that are definitely more in the weeds
with people and like where they are in their journey as far as like using a vector

330
00:27:49,080 --> 00:27:52,191
database or coming to Pinecone specifically.

331
00:27:56,395 --> 00:27:53,512
just go and use pine cone.

332
00:27:53,512 --> 00:28:03,975
Don't use a, there's no reason using generic other vector database, especially, mean,
honestly, from what I, the research I've done that trying to build your own model to get

333
00:28:03,975 --> 00:28:08,216
the embeddings working right is just such a huge lift in the first place.

334
00:28:08,216 --> 00:28:15,618
given the challenges from ensuring like similar, like you can't just switch your model
from one version.

335
00:28:15,618 --> 00:28:17,672
Maybe you're just going to tell me I'm totally wrong here.

336
00:28:17,672 --> 00:28:22,115
Don't upgrade your model without also replacing all of your embeddings because the new
results won't make any sense.

337
00:28:22,115 --> 00:28:29,238
if you want to swap out your embeddings, like if you've, you've done evaluations, you've
figured out, you've done testing, you figured out that like, it's no longer accurate

338
00:28:29,238 --> 00:28:30,208
enough.

339
00:28:30,783 --> 00:28:32,065
you're going to have to re-embed that data using a different model.

340
00:28:32,065 --> 00:28:40,778
And so there are approaches to, doing some benchmarking and testing to make sure that like
your accuracy is in the right, the, the acceptable frame of,

341
00:28:40,778 --> 00:28:42,501
use case can actually support.

342
00:28:42,501 --> 00:28:46,386
you would be swapping out the model and re-embedding that data.

343
00:28:46,386 --> 00:28:54,039
makes sense to me, but that means that there's a huge extra cost here to doing a model
upgrade, not just on like future searches and whatever.

344
00:28:54,039 --> 00:28:59,962
even if the model is faster, you're building it yourself, there's some improvement or
using an open source one.

345
00:28:59,962 --> 00:29:02,493
There's some driver there, but that's going to come with a cost.

346
00:29:02,493 --> 00:29:08,045
And I can see that to be a huge reason to go with, just take that all off the table.

347
00:29:08,045 --> 00:29:14,942
And if you know you need some sort of semantic search or some other strategy that uses
embeddings to go with a database that has that.

348
00:29:14,942 --> 00:29:20,025
baked in automatically without having to think about how to do upgrades between models.

349
00:29:20,025 --> 00:29:33,966
a big reason why we want to do evaluations and benchmarking ahead of time on potentially a
smaller set of data uh before committing is because you're right, it uh takes time, it

350
00:29:33,966 --> 00:29:35,211
costs money to do this.

351
00:29:35,211 --> 00:29:40,733
not everybody is going to have the time and money and expertise to fine tune or train
their own model.

352
00:29:40,733 --> 00:29:43,576
I've mentioned retrieval augmented generation a few times here.

353
00:29:43,576 --> 00:29:54,532
I've been spending uh essentially the last quarter, I've been uh not only digging in
myself and trying to learn about the different parts of it, but also different like

354
00:29:54,532 --> 00:29:58,151
approaches to doing it and sharing some of that.

355
00:29:58,422 --> 00:30:00,253
I've shared some of that publicly.

356
00:30:00,253 --> 00:30:04,574
I'm doing stuff internally related to that and also in the future here.

357
00:30:04,634 --> 00:30:11,496
I've got some stuff going on, but we are seeing people doing this and not fully
understanding what they're doing.

358
00:30:11,496 --> 00:30:15,157
And there's like, yeah, right, exactly.

359
00:30:18,290 --> 00:30:16,595
Probably multiple reasons for that.

360
00:30:16,595 --> 00:30:26,385
uh Like we could go down the vibe coding rabbit hole of like how that is contributing to
some of the good and bad parts of this.

361
00:30:26,506 --> 00:30:33,669
Like obviously it's enabling people to do more and to like get further into their problem.

362
00:30:33,809 --> 00:30:40,489
But also it potentially brings more challenges than they even know how to handle.

363
00:30:42,785 --> 00:30:43,330
uh

364
00:30:43,330 --> 00:30:49,127
are vector databases being recommended by coding assistants as a solution for a problem?

365
00:30:49,127 --> 00:30:57,948
Like if it's just like, yeah, know, generate me some code and it pulls in a way to write
to an open source database that requires embeddings or is that just not happening yet?

366
00:30:57,948 --> 00:30:59,348
I do know that it comes up.

367
00:30:59,348 --> 00:31:04,687
It does, it does propose a pine cone and, and other competitors and stuff like that.

368
00:31:04,687 --> 00:31:15,540
one of the things that like, we have a challenge with, I, I expect other people do as well
as like, because these models are trained on old data, like it's using our old data, our

369
00:31:15,540 --> 00:31:17,361
old document, public documentation.

370
00:31:17,361 --> 00:31:18,051
so.

371
00:31:18,081 --> 00:31:33,027
It isn't always the most accurate And so we do stuff on our end to try and uh encourage
those models to, or those tools, not necessarily the model itself, but the tools to

372
00:31:33,027 --> 00:31:35,736
generate the right code, the most up-to-date code.

373
00:31:35,736 --> 00:31:40,516
we're still a little ways away from it being always the right answer popping up in LLMs.

374
00:31:40,516 --> 00:31:42,998
How about the LLM companies?

375
00:31:42,998 --> 00:31:50,039
So companies that claim they have some sort of AI and they are really just an LLM that's
solving a particular use case.

376
00:31:50,959 --> 00:32:00,748
Are they the cornerstone for a company case where like a case study that would be using
RAG more often than not there or is it just

377
00:32:01,942 --> 00:32:07,328
There's a spectrum and it really depends on the vertical or market segment or product
area.

378
00:32:07,552 --> 00:32:10,914
I mean, there's probably a lot of opinions there.

379
00:32:11,175 --> 00:32:16,117
think that's like, depending on who you talk to on how that is.

380
00:32:16,117 --> 00:32:24,979
think like for me, I see maybe the model companies are not necessarily advocating for
retrieval augmented generation.

381
00:32:24,979 --> 00:32:29,971
I mean, maybe they are, but like we keep seeing these models get bigger and better and
faster, right?

382
00:32:30,132 --> 00:32:33,954
But there's still those limitations that I mentioned from the beginning, right?

383
00:32:34,054 --> 00:32:36,405
They're only trained up until a certain period of time.

384
00:32:36,405 --> 00:32:44,860
It's not trained on your company data, your private data, among a few other limitations.

385
00:32:44,860 --> 00:32:48,253
But those are the key ones that people really recognize.

386
00:32:48,253 --> 00:32:50,142
And so that's where this

387
00:32:50,142 --> 00:32:58,777
retrieval part of rag is coming in is like it's the way to give your model more knowledge
to give it more accurate and authoritative knowledge.

388
00:32:58,777 --> 00:33:11,137
So if the primary use case outside like semantic searching or some sort of comparison
search, if you're utilizing it to extend your data from your own data or something else

389
00:33:11,137 --> 00:33:17,861
that you want to pull in and that means you're using RAG, then if you're using RAG, that
means you're using a vector database, Oh, that's interesting.

390
00:33:17,861 --> 00:33:21,848
It could be retrieving from some other database.

391
00:33:40,405 --> 00:33:25,975
If we think about it, I think you recently did like an MCP episode, right?

392
00:33:25,975 --> 00:33:32,089
So that's one approach to getting to kind of interfacing with other tools and services.

393
00:33:32,089 --> 00:33:34,810
even like chat GPT, it can go out to the internet.

394
00:33:34,810 --> 00:33:41,670
That is a tool that it's using to go get other data and augment your result, your output
with more accurate data.

395
00:33:41,670 --> 00:33:52,507
one thing I noticed, especially with companies that are still incredibly technical, but
aren't specifically in any of the quote unquote AI spaces, is that they definitely get the

396
00:33:52,507 --> 00:33:57,663
difference between MCP and rag wrong off then like the.

397
00:33:57,663 --> 00:34:07,399
Byjection of how many of one of these things they need versus another one like I was one
of my colleagues is working at a very interesting company where they have a need to do.

398
00:34:07,399 --> 00:34:17,396
Rag on behalf of their customers so they're they're pulling in knowledge bases from their
customers and they're trying to figure out how that interl plays with mcp servers and one

399
00:34:17,396 --> 00:34:27,764
thing that has been problematic is they don't want to the data but at the same time a lot
of them mcp providers don't have this concept of like multi-tenancy like they understand

400
00:34:27,764 --> 00:34:35,507
you as a customer can only access your data but they don't have a good concept of how to
group or sequester.

401
00:34:35,507 --> 00:34:38,684
parts of the data into smaller and smaller areas.

402
00:34:45,905 --> 00:34:43,931
from a vector database perspective, the way we've implemented multi-tenancy is through
namespaces.

403
00:34:43,931 --> 00:34:55,601
So uh if you think of a company that is offering agents to their customers, each agent or
each user that has an agent would potentially be within its own namespace.

404
00:34:55,601 --> 00:34:59,693
So that is a way to uh actually segregate the data.

405
00:34:59,693 --> 00:35:10,197
I think you touched on something interesting though that like not all the companies out
there are AI first companies or they are not like well versed in AI technologies and

406
00:35:10,197 --> 00:35:11,377
solutions.

407
00:35:11,758 --> 00:35:19,384
And I think we're at the point where that's going to become a huge thing because all of
these companies there's so many more of those companies out there that then there are

408
00:35:19,384 --> 00:35:23,078
uh AI companies that have been doing this for a long time, right?

409
00:35:23,078 --> 00:35:25,558
I think we're going to have a fight on this episode.

410
00:35:25,978 --> 00:35:37,682
I'm going to quote some research that I think came out Nanda AI report from MIT that said
only 5 % of companies are getting value out of implemented.

411
00:35:38,102 --> 00:35:40,802
I'm going to say quote unquote AI, because I don't think we have AI.

412
00:35:40,802 --> 00:35:42,742
That's a different episode where I got into that.

413
00:35:42,742 --> 00:35:45,482
we can say AI for the rest of this one.

414
00:35:45,522 --> 00:35:49,148
Aren't getting the value out, and it's just a

415
00:35:49,148 --> 00:35:52,153
huge cost sync, time sync, resource sync.

416
00:35:52,153 --> 00:36:03,387
If you say that going forward there's gonna it's gonna be interesting i can either read
that as you believe that companies will transition to having.

417
00:36:03,387 --> 00:36:10,670
or all the companies who don't do it will be out of business and therefore the only
companies left will be one to do a.

418
00:36:10,670 --> 00:36:12,108
I don't mean the latter.

419
00:36:12,108 --> 00:36:15,052
I think there is going to be a transition.

420
00:36:19,036 --> 00:36:17,025
But I think there's an opportunity there.

421
00:36:19,157 --> 00:36:18,592
my background is not in AI.

422
00:36:18,592 --> 00:36:22,796
Like me coming to Pinecone is, this is a new space for me, right?

423
00:36:22,796 --> 00:36:28,924
So I bring the lens of the traditional developer who's been tasked with a problem and.

424
00:36:28,924 --> 00:36:34,756
In this case, the problem these days might be a problem that requires a vector database.

425
00:36:34,756 --> 00:36:42,830
And so as someone who has been a full stack developer, now I have to go out and figure out
how do I solve this problem with this tool that I've been told to use.

426
00:36:42,830 --> 00:36:46,191
uh I think we're going to see more and more of that.

427
00:36:48,592 --> 00:36:48,484
definitely don't mean that companies are going to go away.

428
00:36:48,484 --> 00:36:51,326
Obviously, people are still running on mainframes, right?

429
00:36:51,326 --> 00:36:53,506
And so it might not fit in that space.

430
00:36:53,506 --> 00:36:55,865
technology sticks around for a really long time.

431
00:36:55,865 --> 00:37:01,245
I think there's gonna be some change and it's not always going to be easy.

432
00:37:01,245 --> 00:37:13,775
I wonder what the turnaround is for this because I see companies still claiming that
they're introducing agile and like, worse, they've done agile.

433
00:37:14,175 --> 00:37:20,589
Well, I mean, okay, so the manifesto was like in 2000 or like 2001 or something.

434
00:37:20,589 --> 00:37:22,400
I'm terrible with years.

435
00:37:22,401 --> 00:37:26,027
I knew it happened before I really got into software engineering.

436
00:37:26,027 --> 00:37:26,679
I think

437
00:37:26,679 --> 00:37:32,588
Realistically, either companies say they do it and they don't, or they acknowledge that
they don't do it.

438
00:37:32,588 --> 00:37:37,851
And that for me is like mind boggling because from my standpoint, everyone should be doing
it all the time.

439
00:37:37,851 --> 00:37:51,780
I can appreciate the belief that the same thing will happen with AI, but if we're 24 years
out, 25 years out from uh the Agile Manifesto, then I think we're forever away.

440
00:37:52,553 --> 00:37:55,638
Well, think I don't necessarily disagree with you.

441
00:37:55,638 --> 00:37:59,037
think there's a lot of legacy code and applicable.

442
00:37:59,037 --> 00:38:00,879
I mentioned mainframe, right?

443
00:38:00,879 --> 00:38:06,051
There are a lot of different companies who are just at different stages of their maturity
in their organization.

444
00:38:06,051 --> 00:38:07,623
Some are big, some are small.

445
00:38:07,623 --> 00:38:11,604
And not all of them are going to get there at the same time.

446
00:38:17,502 --> 00:38:19,059
you're probably familiar with it, but there's this curve of like where people are at on
the acceptance of a particular product or problem.

447
00:38:19,320 --> 00:38:21,511
Yes, yes, exactly, that's it is.

448
00:38:21,511 --> 00:38:30,216
one thing that I think we have seen is people are running more production workloads now
with Pinecone.

449
00:38:30,216 --> 00:38:39,009
Whereas in the past it has like it has been I don't want to say it has been less
production but we are seeing more people kind of latch on to that and doing stuff in

450
00:38:39,009 --> 00:38:39,599
production.

451
00:38:39,599 --> 00:38:46,271
ah And so as we see more and more of that like that's people like learning how to do this.

452
00:38:46,271 --> 00:38:50,196
So like I'm saying like we're still very early in this

453
00:38:50,196 --> 00:38:51,418
totally on the same page there.

454
00:38:51,418 --> 00:38:55,852
And I think you're absolutely right, those workloads may or may not have anything to do
with LLMs.

455
00:38:55,852 --> 00:39:01,736
Like we've solved, we've identified a new functional way of storing data and searching it.

456
00:39:01,736 --> 00:39:04,678
And there are primary applications where that's valuable.

457
00:39:04,678 --> 00:39:06,689
If you talk about RAG, we're talking about knowledge bases.

458
00:39:06,689 --> 00:39:13,494
We're talking about semantic search, where we're talking about searching through tons of
attributes and fuzzy searching, free text searching that.

459
00:39:13,494 --> 00:39:15,795
never really was that great to begin with.

460
00:39:15,795 --> 00:39:21,568
mean, whole companies have made their entire existence to figure out how to actually do
this correctly.

461
00:39:21,568 --> 00:39:30,489
And now we have a fundamental technology or understanding of how to do this fundamentally
at a principle level with raw mathematics.

462
00:39:30,489 --> 00:39:32,625
do think that there is some coming around there.

463
00:39:32,625 --> 00:39:41,910
And it will be interesting to see whether or not the companies that gravitate towards
needing vector databases are more and more in the alarm space or less and less as

464
00:39:42,038 --> 00:39:44,559
companies figure out the value that they can extract from it.

465
00:39:44,559 --> 00:39:51,837
ah So I guess it'll be interesting to see where Pinecones customers end up a few years
from now.

466
00:39:51,837 --> 00:40:02,606
do want to quickly maybe ask you oh an opinion on I think one thing that has happened in
the last five years is companies having to LLMs being used during the interview process.

467
00:40:02,788 --> 00:40:03,781
Yeah.

468
00:40:04,003 --> 00:40:14,983
I don't know if I should just stop the question there and just let you say something, or
if I should specifically ask like, yeah, has this impacted Pinecones interviewing

469
00:40:14,983 --> 00:40:15,793
practices?

470
00:40:15,793 --> 00:40:21,909
And if yes or no, like are they seeing the use of LLMs intentionally by candidates?

471
00:40:21,909 --> 00:40:28,430
Like, is it encouraged to be done ah or have you worked around trying to figure out how to
deal with the fact that?

472
00:40:28,430 --> 00:40:31,507
these alarms will be used during the interviewing cycle.

473
00:40:31,507 --> 00:40:37,411
in my experience, and I and at my last job, I did do a lot of also interviewing to hire.

474
00:40:37,411 --> 00:40:41,105
And it's definitely a thing.

475
00:40:41,105 --> 00:40:42,025
People use them.

476
00:40:42,025 --> 00:40:44,268
um I did not use them.

477
00:40:44,268 --> 00:40:46,269
It was not part of my interview process.

478
00:40:46,269 --> 00:40:48,291
But I do think.

479
00:40:48,433 --> 00:40:55,967
As long as you're setting the expectation of how and why you're using a tool, uh then it
makes more sense.

480
00:40:55,967 --> 00:41:01,690
I don't think that anyone should be pretending they know something when they don't.

481
00:41:01,690 --> 00:41:05,195
destroyed the whole industry there, just with that one statement.

482
00:41:07,015 --> 00:41:11,638
I think just from a practical perspective, we use tools in our day jobs.

483
00:41:11,638 --> 00:41:25,766
And so if I can't use cursor or cloud code to write code and show how I would do my actual
job, then it's really hard for an interviewer to understand how you're going to do your

484
00:41:25,766 --> 00:41:26,303
job.

485
00:41:26,303 --> 00:41:34,638
interesting personal opinion because, and I think this is a pattern for a lot of our
guests that come on this show that previously you were working at Amazon.

486
00:41:34,638 --> 00:41:38,620
ah I don't know if it was AWS specifically.

487
00:41:39,481 --> 00:41:45,105
So I don't know what it is with our podcast and like people like leave AWS after being
there for like four years or so.

488
00:41:45,105 --> 00:41:46,675
I didn't actually look to see how long you were there.

489
00:41:46,675 --> 00:41:50,577
And then then our new job for a couple of years and uh

490
00:41:50,577 --> 00:41:54,077
then come on this podcast and have some really interesting things to say.

491
00:41:54,538 --> 00:41:56,473
Uh, no spilling of the details.

492
00:41:56,473 --> 00:41:59,328
They're just under five years.

493
00:41:59,789 --> 00:42:01,634
a question, around the interviewing.

494
00:42:01,634 --> 00:42:05,836
ah Did you see things already starting to be rolled out at AWS

495
00:42:19,409 --> 00:42:13,575
During my almost five years I was interviewing people and it's a very rigorous, like you
go through training to learn how to interview and you've probably seen some of the

496
00:42:13,575 --> 00:42:15,336
questions that we ask.

497
00:42:15,336 --> 00:42:24,914
while I was there, there was no conversation, like I didn't have any conversations with my
leadership about when or how LLMs are allowed to be used.

498
00:42:24,914 --> 00:42:26,557
during the process.

499
00:42:26,557 --> 00:42:28,509
don't want to say it was early enough where it wasn't happening.

500
00:42:28,509 --> 00:42:33,953
I'm sure it was happening, but it was not a part of my experience there during the
interviewing.

501
00:42:35,478 --> 00:42:37,431
I don't want to know, think is the, uh, the joke I told.

502
00:42:37,431 --> 00:42:38,260
I'll say this.

503
00:42:38,260 --> 00:42:46,788
if you get through the interview round with us you were using an LLM and then you continue
to use an LLM in your day job for however long you're here until the day you leave and no

504
00:42:46,788 --> 00:42:48,453
one finds out, was there any harm?

505
00:42:48,453 --> 00:42:49,610
Um,

506
00:42:49,610 --> 00:42:56,846
for me, it's been like using an LLM as part of my job has been embedded since she at GPT
came out.

507
00:42:56,846 --> 00:43:04,222
at Amazon, not just AWS, but at Amazon, like we were tasked with figure out how to use
this.

508
00:43:04,222 --> 00:43:12,428
Um, and it same thing at Pencon, like we're, we're tasked with figuring out how to use
these tools because everyone else is using them.

509
00:43:12,428 --> 00:43:15,855
And if they're trying to use our products or they're trying to do stuff with

510
00:43:15,855 --> 00:43:18,576
pine cone, then we need to know their experiences.

511
00:43:18,576 --> 00:43:23,339
We need to know what the friction is, what's working, what's not working, all that kind of
stuff.

512
00:43:32,404 --> 00:43:32,738
when I say I think it is okay as long as to use it during the interview process, as long
as the expectations are the same on both sides, it's because I use them every day.

513
00:43:32,738 --> 00:43:35,518
I fully recognize like I'm in a special situation.

514
00:43:35,518 --> 00:43:37,750
I work for a company that deals with this stuff, right?

515
00:43:37,750 --> 00:43:39,600
And builds this stuff.

516
00:43:39,600 --> 00:43:47,796
so this is actually why I like asking the question specifically from people that are
working at a company that's in and around the space where they're building tools or

517
00:43:47,796 --> 00:43:55,535
support for things like it'd be a weird twist of fate where you're like, well, we don't
know how to deal with these these interviewer like these interviews where the candidate

518
00:43:55,535 --> 00:44:03,031
comes in is using an LLM and like the product that they're using is using some sort of rag
that is has a vector database that's running on pine cone.

519
00:44:03,031 --> 00:44:05,202
you know, that's a sort of like weird circle there.

520
00:44:05,202 --> 00:44:15,113
where like probably something you should have thought about, it's good to realize, okay,
no, actually not only are our employees, but also are the developers that are utilizing

521
00:44:15,113 --> 00:44:17,525
the product to embed in their own applications.

522
00:44:17,525 --> 00:44:22,830
They're utilizing these tools as an important part of the flow, like expectedly.

523
00:44:22,931 --> 00:44:27,688
So understanding how they're utilizing them is important for us to even design a better
application.

524
00:44:27,688 --> 00:44:29,425
Yeah, exactly.

525
00:44:32,141 --> 00:44:35,687
maybe I'll give you the moment in case there's anything that we left out here that you're
just like, I really need to share about this thing.

526
00:44:35,687 --> 00:44:38,531
had a uh major pine cone release.

527
00:44:38,531 --> 00:44:43,196
The best feature that has ever come out is about to drop one week from now.

528
00:44:43,196 --> 00:44:45,919
I don't know what that is, but feel free to plug that

529
00:44:47,973 --> 00:44:48,363
ah I don't have the next feature that's coming out.

530
00:44:48,363 --> 00:44:50,747
And if I did, I probably couldn't say, so.

531
00:45:11,411 --> 00:44:52,235
it's a cool technology.

532
00:44:52,235 --> 00:45:05,083
So if you are a person who likes to uh understand like how things work and we have a lot
of really good content on the Pinecone blog about just kind of

533
00:45:05,083 --> 00:45:09,896
search in general and like algorithms in general and if you're if that's the thing you're
into and

534
00:45:09,896 --> 00:45:15,723
whether LLMs are here to stay or not, I do think that there is a unique innovation that's
happened here.

535
00:45:15,723 --> 00:45:18,102
And outside of that, it's definitely worth learning.

536
00:45:18,102 --> 00:45:28,273
So if this weekend you are going to spend some time rewriting whatever your favorite
JavaScript runtime is in a new language like Rust or Zig, just because you can.

537
00:45:28,273 --> 00:45:37,597
Or an operator for Kubernetes, sounds like a better use of your time would be potentially
to uh invest in learning vector databases and why not use pine cone for that.

538
00:45:38,108 --> 00:45:38,484
I agree with that.

539
00:45:39,366 --> 00:45:40,181
I think upskilling in the long run.

540
00:45:48,227 --> 00:45:43,047
So with that, can switch over to doing picks.

541
00:45:43,047 --> 00:45:45,925
so my pick for this week is, do I put this?

542
00:45:45,925 --> 00:45:51,267
I found that I often plug things into my computers and unplug them over and over again.

543
00:45:51,267 --> 00:45:58,450
And I frequently get concerned about the reliability of the USB-C port, everything I have
is USB-C, especially like my YubiKey.

544
00:45:58,450 --> 00:46:05,033
So I'm a big physical passkey user, I think too much about security, probably more than I
should.

545
00:46:05,462 --> 00:46:06,898
so I...

546
00:46:06,898 --> 00:46:11,599
spent a ton of money, more than I probably should, on buying magnetic USB-C connectors.

547
00:46:11,599 --> 00:46:16,631
So if you're watching the video, this piece here is just magnetic, which just goes
together.

548
00:46:16,631 --> 00:46:18,581
And you can just walk around with this.

549
00:46:18,581 --> 00:46:21,322
Here's a UV key, my micro one.

550
00:46:21,322 --> 00:46:22,663
And honestly, it just made it better.

551
00:46:22,663 --> 00:46:30,025
plug in all my laptops, my cell phone, all my connectors that are sitting out have a
magnetic thing on it.

552
00:46:30,025 --> 00:46:32,586
And it's just been great for the last few months.

553
00:46:32,586 --> 00:46:34,555
I can't believe I waited so long to do this.

554
00:46:34,555 --> 00:46:40,148
Is this magnetic piece like a protector so it's a cover when you're not using it or I
don't I don't understand.

555
00:46:40,148 --> 00:46:43,228
so the piece is just a USB-C to USB-C connector.

556
00:46:43,228 --> 00:46:47,748
So it's just USB-C and this is also USB-C and this part is two pieces.

557
00:46:47,748 --> 00:46:49,225
So this gets connected.

558
00:46:49,225 --> 00:46:57,625
So realistically, I walk around with this, just this side, which is not USB-C, it's just
like a magnetic piece and it just, they just snap together and they're interchangeable.

559
00:46:57,625 --> 00:47:05,205
So like the same one I use for my cell phone, like I could, don't know no one's going to
be able to see this, but if I hold up my cell phone with the piece and I take my YubiKey,

560
00:47:05,205 --> 00:47:06,761
like it will snap onto here.

561
00:47:06,761 --> 00:47:12,325
ah But it's the same thing that also like if I want to charge my phone it's the same
connector.

562
00:47:12,365 --> 00:47:20,471
And it's just been great honestly because like I don't have to worry about where on the
actual connectors anymore because they never pulled in or pushed out.

563
00:47:20,471 --> 00:47:24,514
I had this problem like every single time I'm in the airplane I have those one of those
really annoying.

564
00:47:24,514 --> 00:47:34,241
uh Mini phone three point five millimeter like splitters in the plane just for my
headphones and I like always break them on the plane like I will bash into them whatever

565
00:47:34,241 --> 00:47:35,081
because.

566
00:47:35,252 --> 00:47:45,941
you know the airplanes uh famous for giving you lots of room to move around in I I ruined
them and like this I just it would just come off honestly and it's

567
00:47:45,941 --> 00:47:50,166
it reminds me of the old Mac connectors.

568
00:47:50,166 --> 00:47:59,489
It's not USB-C, it's not lightning, but it was like, if you toggled it just a little bit,
it would come off as opposed to breaking off in the connector, right?

569
00:47:59,489 --> 00:48:00,930
Do you remember that?

570
00:48:02,150 --> 00:48:02,991
Okay then.

571
00:48:02,991 --> 00:48:11,211
was somewhat jealous of people with the AC adapter connector, I think it was MagSafe or
something like that, to connect.

572
00:48:11,291 --> 00:48:15,891
Yeah, and I don't know, my Apple got rid of it and they brought it back.

573
00:48:15,891 --> 00:48:17,211
It seemed like they were onto something.

574
00:48:17,211 --> 00:48:24,891
I do get that it falls off, but for me, I'm not using it for walking around with.

575
00:48:24,891 --> 00:48:26,725
So it's like my USB key.

576
00:48:26,725 --> 00:48:30,558
I'm on, I'm traveling somewhere, it's really annoying to stick it into my laptop and pull
it back out again.

577
00:48:30,558 --> 00:48:32,679
This is like really easy swap.

578
00:48:34,180 --> 00:48:34,043
I don't know what it was that I just like clicked for me.

579
00:48:34,043 --> 00:48:35,284
Like I could actually do this.

580
00:48:35,284 --> 00:48:40,618
I guess I never wanted to do it with like USB-A because it just seemed like such a legacy
technology all the time.

581
00:48:40,618 --> 00:48:46,689
ah But now that everything's USB-C, like this has been absolutely fantastic.

582
00:48:46,689 --> 00:48:48,214
I'm going to have to check it out.

583
00:48:48,996 --> 00:48:55,069
Yeah, be prepared to uh I think this is like by Hansen or something like that.

584
00:48:55,069 --> 00:48:56,740
I don't know some random brand on Amazon.

585
00:48:56,740 --> 00:49:00,823
I'm sure it's ripped off from like another company that makes really good ones.

586
00:49:00,823 --> 00:49:02,654
These are actually not cheap though.

587
00:49:02,654 --> 00:49:03,154
So.

588
00:49:03,154 --> 00:49:05,351
the same one, just a different logo.

589
00:49:05,351 --> 00:49:09,931
Oh yeah, that's the thing is there's no even, I don't know if there's a logo on it
actually.

590
00:49:09,931 --> 00:49:12,631
So like that's the most suspicious part.

591
00:49:12,811 --> 00:49:14,891
So that's gonna be my pick.

592
00:49:15,060 --> 00:49:16,500
You can't just buy one though.

593
00:49:16,500 --> 00:49:17,680
You have to like go full in.

594
00:49:17,680 --> 00:49:20,440
Cause like all your power adapters and everything have to be connected.

595
00:49:20,440 --> 00:49:24,180
Otherwise you're like, well I have to pull out the plug in order to switch it.

596
00:49:24,360 --> 00:49:27,980
So be prepared if that's gonna be your future.

597
00:49:29,280 --> 00:49:31,135
Okay, Jenna, what did you bring for us?

598
00:49:31,135 --> 00:49:33,277
My pick is...

599
00:49:33,277 --> 00:49:36,058
I'll give you a little backstory first.

600
00:49:36,058 --> 00:49:40,861
I have never really understood the allure of mechanical keyboards.

601
00:49:40,861 --> 00:49:53,100
uh Back before microphones actually had like noise canceling on them, my mind's right
here, so back before microphones had noise canceling on them, uh they were very loud and

602
00:49:53,100 --> 00:49:57,592
clicky and clackety and I just didn't understand them.

603
00:49:57,853 --> 00:50:01,119
Anyways, a couple years ago I bought one and fell in love and...

604
00:50:01,119 --> 00:50:02,800
uh I will show it to you.

605
00:50:02,800 --> 00:50:03,481
It's right here.

606
00:50:03,481 --> 00:50:14,991
uh This is not my, this one specifically is not my pick, but uh a couple of weeks ago I
bought another one uh because, well, first of all, I occasionally have wrist pain, so I

607
00:50:14,991 --> 00:50:20,435
wanted a different layout and I also wanted a project.

608
00:50:21,057 --> 00:50:23,098
And so I bought one that you actually had to put together.

609
00:50:23,098 --> 00:50:25,981
And so I'm only part way through this,

610
00:50:25,981 --> 00:50:28,103
Like I said, I wanted, I needed a project.

611
00:50:28,103 --> 00:50:29,933
Like I spent enough time in front of my screen.

612
00:50:29,933 --> 00:50:34,107
Like I could actually build something with my physical, like physical hands.

613
00:50:42,455 --> 00:50:45,125
But it's, kind of nice to just have something, something else um that is not writing code
or being on a computer, but will still support my computer years.

614
00:50:45,382 --> 00:50:47,154
So I got a different layout.

615
00:50:47,154 --> 00:50:48,746
I got the keycaps.

616
00:50:48,746 --> 00:50:53,301
I got different switches that I think they're a little bit different than the ones I have
here.

617
00:50:53,522 --> 00:50:59,074
So I'm excited to kind of try it out figure out if I like this new one as much as I like
this one.

618
00:50:59,164 --> 00:51:04,452
I thought for sure you were going to say that you bought it and you still don't understand
why people like mechanical keyboards.

619
00:51:04,452 --> 00:51:10,029
Oh no, I mean, I'm definitely not like, a fanatic about it.

620
00:51:10,029 --> 00:51:12,084
Like, people are obsessed.

621
00:51:12,084 --> 00:51:16,023
be careful if you say that, we're gonna lose some viewers if uh you come out on the-

622
00:51:16,023 --> 00:51:24,085
judgment, no When I was at Amazon, I wrote a blog post uh about mechanical keyboards.

623
00:51:24,085 --> 00:51:29,387
I just kind of wanted to understand who uses them, who likes them, and which one do you
have.

624
00:51:29,387 --> 00:51:33,270
uh Got a lot of opinions on there.

625
00:51:33,270 --> 00:51:41,554
One of my friends commented, uh coworkers commented, and he's like, as soon as you start
building your own, then you've gone too far.

626
00:51:41,602 --> 00:51:43,887
So I've gone too far.

627
00:51:44,617 --> 00:51:47,546
mean, I get the ergonomics for

628
00:51:47,546 --> 00:51:54,308
if you have some sort of physical pain, like you know that there's something wrong with
what you're doing or if you, I mean, it took me a lot of years to realize, wait, my whole

629
00:51:54,308 --> 00:51:56,198
job revolves around my keyboard.

630
00:51:56,198 --> 00:52:05,388
I probably should have one that fits me as best as I can and that includes, for me it was
a keyboard layout, I get totally the physical result, like actually having to push the

631
00:52:05,388 --> 00:52:07,098
keys and whether or not that's problematic.

632
00:52:07,098 --> 00:52:14,240
I have a thing for sound though and so I did some research on getting the quietest
keyboard possible and

633
00:52:14,240 --> 00:52:18,263
A bunch of people said, yo, they make really quiet mechanical keyboards.

634
00:52:18,263 --> 00:52:20,445
And I'm like, okay, sure.

635
00:52:20,445 --> 00:52:29,161
So I probably spent like a whole bunch of hours going around to different shops, pushing
the keys on different mechanical keyboards in multiple countries and like going online and

636
00:52:29,161 --> 00:52:32,834
like listening to audio clips of the mechanical keyboards.

637
00:52:32,834 --> 00:52:37,147
And after that, all I conclude is those people have no idea what they're talking about.

638
00:52:37,147 --> 00:52:40,059
Mechanical keyboards are not quiet in any way.

639
00:52:41,100 --> 00:52:44,142
The quieter switches, like they're not, they're not quiet.

640
00:52:44,142 --> 00:52:51,166
ah And I actually, my pick actually in a previous episode was my keyboard, which is uh
dedicated to be a silent keyboard.

641
00:52:51,166 --> 00:52:57,290
So people can hate me all they want for that, but I still will not get mechanical
keyboards unless you have a physical element.

642
00:52:57,290 --> 00:53:00,182
ah Cause then, you know, I get it.

643
00:53:00,182 --> 00:53:01,172
totally do.

644
00:53:01,912 --> 00:53:06,194
ah Yeah, I also went down both times.

645
00:53:06,194 --> 00:53:09,206
I went down a rabbit hole trying to figure out which one should I get.

646
00:53:09,206 --> 00:53:10,796
I didn't go quite that far.

647
00:53:10,796 --> 00:53:14,048
I didn't actually go to physical stores.

648
00:53:14,148 --> 00:53:19,051
There's not many here that sell what I was looking for here, but you're right.

649
00:53:19,051 --> 00:53:24,774
They're still not quiet, even though I think I got one of the quieter sets of switches.

650
00:53:24,842 --> 00:53:32,889
And the smoother ones that don't make as much of the clickety clackety, um there's still
some sound there.

651
00:53:33,027 --> 00:53:38,222
I have to ask you, ah so what's the keyboard brand or model that you purchased?

652
00:53:38,426 --> 00:53:44,026
So the one that I showed you is a Keychron Q1 Pro Wireless.

653
00:53:44,026 --> 00:53:47,746
just is, I think it's like the 75%.

654
00:53:47,746 --> 00:53:50,107
I'm holding it up for the people who are just Yeah.

655
00:53:50,107 --> 00:53:52,827
It doesn't have like the number pad on the side.

656
00:53:52,827 --> 00:53:55,887
So it's the 75 % layout, I think.

657
00:53:56,087 --> 00:53:57,616
The new one that I got is an Alice layout.

658
00:53:57,616 --> 00:54:02,849
So it's a little bit more rounded, that, where your hands are

659
00:54:02,849 --> 00:54:11,233
I'm rotating my hands, Where your hands are a little bit in a more natural layout as
opposed to straight up and down like you would on kind of a regular keyboard.

660
00:54:11,233 --> 00:54:13,075
And it's a little bit bigger too.

661
00:54:13,075 --> 00:54:17,188
One thing that like I've noticed with this one here is it is a it's narrower.

662
00:54:17,188 --> 00:54:19,170
It's 75 percent, so it's narrower.

663
00:54:19,170 --> 00:54:21,166
And so like my hands are closer together.

664
00:54:21,166 --> 00:54:26,218
the one other piece that I like about this brand specifically are this this these.

665
00:54:26,218 --> 00:54:28,469
is that ah it's pretty hefty.

666
00:54:28,469 --> 00:54:30,321
Like, this is several pounds.

667
00:54:30,321 --> 00:54:33,631
ah And so it's solid.

668
00:54:33,631 --> 00:54:41,798
I don't know, I just, I like the feel of Yeah, yeah, yeah, it's not gonna go anywhere if I
get like carried away.

669
00:54:42,812 --> 00:54:48,674
I think we have probably like one guest per year on the show who calls out their
mechanical keyboard is there is their pick.

670
00:54:48,674 --> 00:54:50,955
So you're in good company.

671
00:54:51,275 --> 00:54:52,245
Yeah.

672
00:54:52,245 --> 00:54:55,777
So with that, I think we'll call it the end of the episode there.

673
00:54:55,777 --> 00:55:03,166
Thank you so much, Jenna, for coming on and being our target practice for today's episode.

674
00:55:03,166 --> 00:55:05,159
Thank you so much for having me.

675
00:55:05,159 --> 00:55:06,579
been a good conversation.

676
00:55:06,579 --> 00:55:08,040
It has been, I've enjoyed it.

677
00:55:08,040 --> 00:55:11,604
And I just want to thank personally Attribute one last time for sponsoring today's
episode.

678
00:55:11,604 --> 00:55:15,306
And I hope to see all of our viewers and listeners next.

